{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47d36ee",
   "metadata": {
    "id": "LJkGIzvfwx1o"
   },
   "source": [
    "# Koopman decomposition, a toy case: Duffing oscillator\n",
    "\n",
    "\n",
    "The aim of this notebook is to describe the dynamics of a non-linear dynamical system by means of the Koopman theory.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We consider a quantity $x$ (a vector) which evolves with time, following a dynamical system. Think for example of the joint location of the planets in our solar system, which follows the law of gravitation.\n",
    "\n",
    "Formally, given an initial state $x(t=0) \\in \\mathbb{R}^n$ at time $t=0$, the time evolution of $x$ is governed by the following dynamical system:\n",
    "$$\n",
    "\\dot{x(t)} = f(x(t)) \\quad \\text(1)\n",
    "$$\n",
    "\n",
    "where $\\dot{x(t)} := {dx}/{dt}$ is the temporal derivative, and $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ is a given map describing the dynamics.\n",
    "\n",
    "For a given $f$, it is not always possible to solve the differential equation (1) analytically. For this reason, instead, numerical schemes are usually employed, to integrate in time $t$ (and space $\\Omega$ if needed) the eq (1), so as to propagate the initial condition $x(0)$ up to a desired time $T$; think of $x(T) = x(0) + \\int_{t=0}^{T} f(x(t)) dt$. The discretization in time of eq (1) or of the integral introduces numerical approximations, and yields estimates of $x(T)$ of various quality depending on the discretization scheme.\n",
    "\n",
    "In the field of numerical simulations, discretization schemes have been studied for a long time, and numerical solvers already exist to provide good estimates of integrals (far better than with the naive discretization $x_{t+dt} = x_t + dt\\,f(x_t)$, which induces a $O(dt^2)$ error at each time step).\n",
    "\n",
    "The goal of this practical session is to make use of such numerical solvers to improve the learning of dynamical systems with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240baae",
   "metadata": {
    "id": "bde86020"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305bcbbf",
   "metadata": {
    "id": "mDyFzROl3b-N"
   },
   "source": [
    "## Duffing oscillator\n",
    "\n",
    "As a toy example, we consider the Duffing oscillator, where the state $x = (x_1, x_2) \\in \\mathbb{R}^2$ follows the dynamical system described by the following ODEs:\n",
    "\n",
    "$$\n",
    "\\dot{x}_1 = x_2\\\\\n",
    "\\dot{x}_2 = x_1 - x_1^3\n",
    "$$\n",
    "\n",
    "To integrate in time the ODEs, a 4th order Runge-Kutta scheme can be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cf79b",
   "metadata": {
    "id": "9c76fa87"
   },
   "outputs": [],
   "source": [
    "def duffing(t, x : np.ndarray) -> np.ndarray:\n",
    "    dx = np.zeros(x.shape)\n",
    "    dx[0] = x[1]\n",
    "    dx[1] = x[0] -x[0]**3 \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293b25e",
   "metadata": {
    "id": "8140314e"
   },
   "outputs": [],
   "source": [
    "tmax = 500    #time-horizon integration\n",
    "niter = 5000  #number of time steps integration\n",
    "Ninit = 60    #number of initial conditions\n",
    "\n",
    "X0 = (np.random.rand(Ninit,2)-0.5)*4\n",
    "t = np.linspace(0, tmax, niter)\n",
    "Xt = np.zeros((X0.shape[0],X0.shape[1],niter))\n",
    "for i in tqdm(range(X0.shape[0])):\n",
    "    f = solve_ivp(duffing, [0,tmax], X0[i], method='RK45',t_eval=t)\n",
    "    Xt[i,:] = f.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f4a62",
   "metadata": {},
   "source": [
    "The following plot shows trajectories for different initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c7cc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "ea6b6ae1",
    "outputId": "e30d6069-609b-4faf-8db4-3f891cf2599b"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "ax = fig.add_subplot(131)\n",
    "cm = plt.get_cmap(\"tab10\")\n",
    "print(cm)\n",
    "for i in range(10):\n",
    "    ax.plot(Xt[i,0,:], Xt[i,1,:],lw=0.5, color=cm(i))\n",
    "    ax.plot(Xt[i,0,0], Xt[i,1,0],'o',lw=1.5, color=cm(i)) #initial condition\n",
    "ax.set_xlabel('$x_1$', fontsize = 20)\n",
    "ax.set_ylabel('$x_2$', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ce8a0",
   "metadata": {
    "id": "oSzWSnIR6_WD"
   },
   "source": [
    "## The Koopman operator\n",
    "### Discontinuous in time case\n",
    "Given the discrete non-linear dynamical system\n",
    "\n",
    "$$\n",
    "x_{t+1} = \\mathbf{F}(x_t)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{F}$ might be the flow map of the continuous dynamical system in eq (1) and $X = \\{x_t | t = 1 \\ldots N \\}$ the time series of the system state.\n",
    "\n",
    "$$\n",
    "\\mathbf{F}(x(t_0)) = x(t_0) + \\int_{t_0}^{t_0+ t} f(x(\\tau))d\\tau \n",
    "$$\n",
    "\n",
    "the Koopman theory states that there exists an infinite-dimensional linear operator $\\mathcal{K}$ that advances in time all observable functions $g_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
    "\n",
    "$$\n",
    "\\mathcal{K} g_i(x) = g_i \\circ \\mathbf{F}(x).\n",
    "$$\n",
    "\n",
    "This way, the non-linear dynamics of $x$, described by $\\mathbf{F}$, can be turned into a **linear** dynamical system, described by $\\mathcal{K}$, acting on another representation space, formed by the observable quantities $g_i(x)$.\n",
    "\n",
    "It is then sufficient to find a function $g: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ with $m \\gg n$ that embeds the state $x$ into a \"larger enough\" dimensional space $m$ such that the linear operator $\\mathcal{K}$ can be inferred by a matrix $\\mathbf{K} \\in \\mathbb{R}^{m \\times m}$.\n",
    "\n",
    "To project back the dynamics from the Koopman space ($\\mathbb{R}^m$, where $g(x)$ lives) to the phase space ($\\mathbb{R}^n$, where $x$ lives), a supplementary function $\\varphi: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ is needed. Going from $x$ to the Koopman space and back yields $\\varphi \\text{ o  } g = $ Id.\n",
    "\n",
    "Under this condition, the functions $g$, $\\varphi$ and $\\mathbf{K}$ can be parametrized $g_{\\theta}$, $\\varphi_{\\rho}$ and $\\mathbf{K}_{\\phi}$, and the parameters $\\theta$, $\\rho$ and $\\phi$ can be learned minimizing suitable loss functions. \n",
    "\n",
    "For this purpose, given a time series $X = \\{x_t | t = 1 \\ldots N \\}$, the following conditions hold:\n",
    "\n",
    "\n",
    "1.   Reconstruction error\n",
    "     $$\n",
    "     \\Vert \\varphi_\\rho (g_\\theta(x_t)) - x_t  \\Vert = 0\n",
    "     $$\n",
    "2.   Prediction error in Koopman space\n",
    "     $$\n",
    "     \\Vert \\mathbf{K_{\\phi}} g_{\\theta} ( x_t ) - g_{\\theta} (x_{t+1})  \\Vert = 0\n",
    "     $$\n",
    "3.   Prediction error in the phase space\n",
    "     $$\n",
    "     \\Vert \\varphi_{\\rho} \\left( \\mathbf{K_{\\phi}} g_{\\theta} ( x_t )\\right) - x_{t+1} \\Vert = 0\n",
    "     $$\n",
    "\n",
    "The last three errors can be used as loss functions to train three different neural networks. These different neural networks compose our architecture that can be summarized as in the following sketch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445975b",
   "metadata": {},
   "source": [
    "![architecture](./architecture.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18c4cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Km6f2WruJPf7",
    "outputId": "47f394f2-8e94-478a-f938-304119baa739"
   },
   "outputs": [],
   "source": [
    "# arrange the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = np.transpose(np.concatenate([Xt[i,:,:-1] for i in range(Xt.shape[0])], axis=1))\n",
    "Y = np.transpose(np.concatenate([Xt[i,:,1:] for i in range(Xt.shape[0])], axis=1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141401a",
   "metadata": {
    "id": "kipqu2hvJ2cB"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "batch_size = 2000     # data per batch\n",
    "\n",
    "training_data = TensorDataset(torch.from_numpy(X_train),torch.from_numpy(Y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test),torch.from_numpy(Y_test))\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create the models\n",
    "feature_dim = 2       # dimension of the Duffing oscillator\n",
    "hidden_layer = 5      # number of hidden layers in g (ENCODER) and \\varphi (DECODER) \n",
    "output_dim = 30       # dimension in Koopman space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896fae9",
   "metadata": {
    "id": "nhSxlEg-KNwu"
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, layer_dim):\n",
    "        super(encoder, self).__init__()\n",
    "        self.layer_dim = layer_dim\n",
    "        self.list_FC = nn.ModuleList()\n",
    "        for i in range(len(self.layer_dim)-1):\n",
    "            self.list_FC.append(nn.Linear(self.layer_dim[i], self.layer_dim[i+1]))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for i in range(len(self.layer_dim)-2):\n",
    "            X = F.elu(self.list_FC[i](X))\n",
    "        return self.list_FC[-1](X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445d508",
   "metadata": {
    "id": "RjcLaquyKNxB"
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, layer_dim):\n",
    "        super(decoder, self).__init__()\n",
    "        self.layer_dim = layer_dim\n",
    "        self.list_FC = nn.ModuleList()\n",
    "        for i in range(len(self.layer_dim)-1,0,-1):\n",
    "            self.list_FC.append(nn.Linear(self.layer_dim[i], self.layer_dim[i-1]))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for i in range(len(self.layer_dim)-2):\n",
    "            X = F.elu(self.list_FC[i](X))\n",
    "        return self.list_FC[-1](X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae33ef",
   "metadata": {
    "id": "qUOovEfQKNxB"
   },
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,feature_dim, hidden_layer, output_dim):\n",
    "        super(autoencoder, self).__init__()\n",
    "        layer_dim = [output_dim if i == hidden_layer else feature_dim+i*(output_dim-feature_dim)//hidden_layer for i in range(hidden_layer+1)]\n",
    "        self.encoder = encoder(layer_dim)\n",
    "        self.decoder = decoder(layer_dim)\n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        return self.decoder(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411c4de",
   "metadata": {
    "id": "lOpkBElrKek6"
   },
   "source": [
    "The Koopman operator $\\mathbf{K}$ (which is linear, and thus a matrix) must have a spectral radius $\\rho(\\mathbf{K})\\le 1$. Such condition will provide a stable -or at least a marginally stable- Koopman operator. To fulfill this requirement, we might leverage on the Perron-Frobenius theorem. \n",
    "\n",
    "The Perron-Frobenius th. states: if $\\mathbf{K}$ is a $m \\times m$ positive matrix i.e. $k_{ij} > 0$ for $1 \\le i,j \\le m$, then the following inequality holds:\n",
    "\n",
    "$$\n",
    "\\min_i \\sum_j k_{ij} \\le \\rho(\\mathbf{K}) \\le \\max_i \\sum_j k_{ij}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c703a",
   "metadata": {},
   "source": [
    "**Question 1.** : Complete the `KoopmanModule` class to enforce $\\rho(\\mathbf{K})\\le 1$, making use of the Perron-Frobenius theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b7f24",
   "metadata": {
    "id": "HlkD2_wiM3jz"
   },
   "outputs": [],
   "source": [
    "class KoopmanModule(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.weight = torch.randn(output_dim, output_dim)\n",
    "        ## To Be Implemented\n",
    "\n",
    "    def forward(self, X):\n",
    "        x, y = X.shape\n",
    "        if y != self.output_dim:\n",
    "            sys.exit(f'Wrong Input Features. Please use tensor with {self.output_dim} Input Features')\n",
    "        ## To Be Implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd97667",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vcO4V8XUSyE",
    "outputId": "37d998c0-81cc-4a68-d072-a7e085f321c0"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e60ea1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaRampsGUThz",
    "outputId": "5f200617-955e-4f4d-a818-fb8081d91ee9"
   },
   "outputs": [],
   "source": [
    "AUTOENCODER = autoencoder(feature_dim, hidden_layer, output_dim).to(device)\n",
    "KPM = KoopmanModule(output_dim).to(device)\n",
    "print(AUTOENCODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e536dd2e",
   "metadata": {
    "id": "63eQ5P9wU0fp"
   },
   "outputs": [],
   "source": [
    "opt_aut = torch.optim.Adam(AUTOENCODER.parameters(), lr=0.0001)\n",
    "opt_kpm = torch.optim.Adam(KPM.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4e417",
   "metadata": {},
   "source": [
    "**Question 2.** : Define a function to compute the loss to be minimized. It should at least include the 3 terms listed above:\n",
    "- Reconstruction error\n",
    "- Prediction error in the Koopman space\n",
    "- Prediction error in the phase space\n",
    "\n",
    "Because the different objectives outlined by these losses may compete, the training can be difficult. You may try different variations on these losses and comment your findings. In order to improve the training process, one can for instance:\n",
    "- Add a multiplicative factor in front of each loss component, to balance their importance;\n",
    "- We can refine the loss acting upon the latent space, by using a variational autoencoder approach. This is similar to the Gaussian likelihood used in the first practical (TD1). We want the prediction in the latent space (i.e. the Koopman space) to be a normal distribution $\\mathcal{N}(0, 1)$ . Add a corresponding loss for the latent space. Difference to 0 mean and 1 standard deviation must be thus included in the loss;\n",
    "- Freeze the gradients of one part of the network, for instance the encoder, for one specific objective, using the [`requires_grad`](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html) property. For instance:\n",
    "```python\n",
    "criterion = nn.MSELoss()\n",
    "...\n",
    "# Compute one part loss_l of the total loss\n",
    "# First deactivate gradient computation for irrelevant parts of the architecture\n",
    "for p in AUTOENCODER.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "loss_l = criterion(pred, target)\n",
    "# Restore the gradient computation\n",
    "for p in AUTOENCODER.encoder.parameters():\n",
    "    p.requires_grad = True\n",
    "...\n",
    "total_loss = loss_1 + ... + loss_l + ...\n",
    "return total_loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca16de4",
   "metadata": {
    "id": "IuqRdy4gVH8n"
   },
   "outputs": [],
   "source": [
    "def LOSS(X_, Y_, X_recon, gX_, gY_, gY_pred, Y_pred):\n",
    "    # To Be Implemented\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965cb53",
   "metadata": {},
   "source": [
    "The following cell executes the training loop. You can modify it in order to display the different intermediate losses computed in the function `LOSS` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9916e0",
   "metadata": {
    "id": "kys9nMNJVplk"
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    AUTOENCODER.train() \n",
    "    KPM.train()\n",
    "    total_train_loss = 0\n",
    "    total_loss1, total_loss2, total_loss3, total_loss4 = 0, 0, 0, 0\n",
    "    for X_, Y_ in train_dataloader:\n",
    "        X_, Y_ = X_.to(device), Y_.to(device)\n",
    "        \n",
    "        opt_aut.zero_grad()\n",
    "        opt_kpm.zero_grad()\n",
    "        \n",
    "               \n",
    "        # gX_ = observable(X_), gY_ = observable(Y_)\n",
    "        gX_ = AUTOENCODER.encoder(X_)\n",
    "        gY_ = AUTOENCODER.encoder(Y_)\n",
    "        \n",
    "        # X_recon = in_observable(observable(X_))\n",
    "        X_recon = AUTOENCODER.decoder(gY_)\n",
    "         \n",
    "        # gY_pred = KPM*observable(X_)\n",
    "        gY_pred, Koop = KPM(gX_)\n",
    "        \n",
    "        # Y_pred = inv_observable(KPM*observable(X_))\n",
    "        Y_pred = AUTOENCODER.decoder(gY_pred)   \n",
    "            \n",
    "        output = LOSS(X_, Y_, X_recon, gX_, gY_, gY_pred, Y_pred)\n",
    "        output.backward()\n",
    "        opt_aut.step()\n",
    "        opt_kpm.step()\n",
    "    \n",
    "        if epoch%1 == 0:\n",
    "            total_train_loss += output.item()/X_.size(0)/len(train_dataloader)\n",
    "            \n",
    "    if epoch%1 == 0:        \n",
    "        print(epoch, total_train_loss)\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        AUTOENCODER.eval() \n",
    "        KPM.eval()\n",
    "        with torch.no_grad():\n",
    "            total_test_loss = 0  \n",
    "            for X_, Y_ in test_dataloader:\n",
    "                X_, Y_ = X_.to(device), Y_.to(device)\n",
    "    \n",
    "                gX_ = AUTOENCODER.encoder(X_)\n",
    "                gY_ = AUTOENCODER.encoder(Y_)\n",
    "                X_recon = AUTOENCODER.decoder(gY_)\n",
    "                gY_pred, Koop = KPM(gX_)\n",
    "                Y_pred = AUTOENCODER.decoder(gY_pred)   \n",
    "                    \n",
    "                output = LOSS(X_, Y_, X_recon, gX_, gY_, gY_pred, Y_pred)\n",
    "            total_test_loss += output.item()/X_.size(0)/len(test_dataloader)\n",
    "            print('-'*50, 'TEST', '-'*50)\n",
    "            print(epoch, total_test_loss)\n",
    "            print('-'*106)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d9477",
   "metadata": {},
   "source": [
    "#### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a56074",
   "metadata": {},
   "source": [
    "**Question 3.** : We want to ensure the Koopman operator is stable. This can be verified by checking whether its spectral radius $\\rho(\\mathbf{K})\\le 1$. Plot the eigenvalues of the Koopman operator in order to verify the bound on its spectral radius. You can use the [`numpy.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html) function to retrieve the eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146315e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed\n",
    "#check Koopman stability\n",
    "# Plot the eigen values of the Koopman operator against the unit circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997a4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "x = np.linspace(-2, 2, N)\n",
    "y = np.linspace(-2, 2, N)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "Flow = np.zeros((N,N,2))\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        Flow[i,j,:] = duffing(0,np.array([xv[i,j],yv[i,j]]))\n",
    "\n",
    "AUTOENCODER.eval() \n",
    "KPM.eval()\n",
    "Flow_pred = np.zeros((N,N,2))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        Flow_pred[i,j,:] = (AUTOENCODER.decoder(\n",
    "                            torch.matmul(\n",
    "                            AUTOENCODER.encoder(torch.from_numpy(np.array([[xv[i,j],yv[i,j]]])).to(device)), Koop.weight.t())\n",
    "                            ).detach().cpu().numpy() - [[xv[i,j],yv[i,j]]]).ravel()/(tmax/niter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.quiver(xv,yv, Flow[:,:,0], Flow[:,:,1], scale=10)\n",
    "ax.set_title('True')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.quiver(xv,yv, Flow_pred[:,:,0], Flow_pred[:,:,1], scale=10)\n",
    "ax.set_title('Prediction')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "cp = ax.contourf(xv, yv, np.log(np.linalg.norm(Flow - Flow_pred, axis=2)))\n",
    "fig.colorbar(cp)\n",
    "ax.set_title('Error in log scale')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f73a0c",
   "metadata": {
    "id": "Z_OP_wUKJOCM"
   },
   "source": [
    "### Continuous in time case\n",
    "\n",
    "Considering $x_r$ as the observation of a state at time $t$, and $x_{r+1}$ the state at time $t+ \\Delta t$, for $\\Delta t \\rightarrow 0$  it is also possible to define the continuous-time infinitesimal generator of the Koopman operator family as\n",
    "\n",
    "$$\n",
    "\\mathcal{L} g (x_t)  = \\lim_{\\Delta t \\rightarrow 0} \\frac{\\mathcal{K}g(x_t)- g(x_{t})}{\\Delta t} = \\frac{g \\circ\\mathbf{F} (x_t) -x_t}{\\Delta t}\n",
    "$$\n",
    "\n",
    "The pevious expression defines the Lie derivative, and for this reason $\\mathcal{L}$ is known as the Lie operator. $\\mathcal{L}$ describes the continuous dynamics of the observables in the Koopman space:\n",
    "\n",
    "$$\n",
    "\\dot{g} (x) = \\mathcal{L} g(x).\n",
    "$$\n",
    "\n",
    "The latter can be further expressed as:\n",
    "\n",
    "$$\n",
    "\\dot{g} (x(t)) = \\frac{dg(x)}{dt} = \\nabla_x g \\frac{dx}{dt} = \\nabla_x g \\cdot f(x) =\\mathcal{L} g(x).\n",
    "$$\n",
    "\n",
    "Given $g_{\\theta}$, $\\varphi_{\\rho}$ and $\\mathbf{L}_{\\phi}$ three parameterized functions, the following conditions hold:\n",
    "\n",
    "1.   Reconstruction error\n",
    "     $$\n",
    "     \\Vert \\varphi_\\rho (g_\\theta(x)) - x  \\Vert = 0\n",
    "     $$\n",
    "2.   Prediction error in Koopman space\n",
    "     $$\n",
    "     \\Vert \\mathbf{L_{\\phi}} g_{\\theta} ( x ) - \\nabla g_{\\theta} \\cdot f(x)  \\Vert = 0\n",
    "     $$\n",
    "3.   Prediction error in the phase space\n",
    "     $$\n",
    "     \\Vert \\varphi_{\\rho} \\left( \\mathbf{L_{\\phi}} g_{\\theta} ( x )\\right) - f(x) \\Vert = 0\n",
    "     $$\n",
    "\n",
    "As long as the system $f(x)$ is known, the three errors can be computed without data belonging to trajectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for continuous Koopman \n",
    "# with the same amount of points of the Discontinuous Koopman case\n",
    "X0 = (np.random.rand(Ninit*(niter-1),2)-0.5)*4 \n",
    "dX =np.zeros(X0.shape)\n",
    "for i in tqdm(range(X0.shape[0])):\n",
    "    dX[i,:] = duffing(0,X0[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fffaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.quiver(X0[::50,0],X0[::50,1], dX[::50,0]*0.2, dX[::50,1]*0.2, scale=10)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the models\n",
    "feature_dim = 2       # dimension of the Duffing oscillator\n",
    "hidden_layer = 5      # number of hidden layers in g (ENCODER) and \\varphi (DECODER) \n",
    "output_dim = 30       # dimension in Koopman space\n",
    "batch_size = 2000     # data per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test =  train_test_split(X0, dX, test_size=0.2)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "training_data = TensorDataset(torch.from_numpy(X_train),torch.from_numpy(Y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test),torch.from_numpy(Y_test))\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b0bc2",
   "metadata": {
    "id": "4254fe4c"
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, layer_dim):\n",
    "        super(encoder, self).__init__()\n",
    "        self.layer_dim = layer_dim\n",
    "        self.list_FC = nn.ModuleList()\n",
    "        for i in range(len(self.layer_dim)-1):\n",
    "            self.list_FC.append(nn.Linear(self.layer_dim[i], self.layer_dim[i+1]))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for i in range(len(self.layer_dim)-2):\n",
    "            X = F.elu(self.list_FC[i](X))\n",
    "        return self.list_FC[-1](X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd041b4",
   "metadata": {
    "id": "c65379b3"
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, layer_dim):\n",
    "        super(decoder, self).__init__()\n",
    "        self.layer_dim = layer_dim\n",
    "        self.list_FC = nn.ModuleList()\n",
    "        for i in range(len(self.layer_dim)-1,0,-1):\n",
    "            self.list_FC.append(nn.Linear(self.layer_dim[i], self.layer_dim[i-1]))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for i in range(len(self.layer_dim)-2):\n",
    "            X = F.elu(self.list_FC[i](X))\n",
    "        return self.list_FC[-1](X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1e9e1",
   "metadata": {
    "id": "9ba2bab2"
   },
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,feature_dim, hidden_layer, output_dim):\n",
    "        super(autoencoder, self).__init__()\n",
    "        layer_dim = [output_dim if i == hidden_layer else feature_dim+i*(output_dim-feature_dim)//hidden_layer for i in range(hidden_layer+1)]\n",
    "        self.encoder = encoder(layer_dim)\n",
    "        self.decoder = decoder(layer_dim)\n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        return self.decoder(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62728501",
   "metadata": {
    "id": "Idmf_0fQEEst"
   },
   "source": [
    "The Lie operator must be defined such that it will be always stable by construction.\n",
    "To do that, we consider a matrix of parameters $\\Psi \\in \\mathbb{R}^{m \\times m}$ and a vector of parameters $\\Gamma \\in \\mathbb{R}^m$. The resulting Lie operator will be of the form:\n",
    "\n",
    "$$\n",
    "\\mathbf{L} = (\\Psi - \\Psi^T) - \\text{diag}(\\vert \\Gamma \\vert)\n",
    "$$\n",
    "\n",
    "with eigenvalues whose real part $\\Re(\\lambda) \\leq 0$ .\n",
    "See https://math.stackexchange.com/questions/952233/eigenvalues-of-the-sum-of-a-diagonal-matrix-and-a-skew-symmetric-matrix for the mathematical proof. Moreover if $\\lambda \\in \\mathbb{C}$ is an eigenvalue of $\\mathbf{L}$, it turns out that its real part $\\Re(\\lambda) \\propto \\Vert \\Gamma \\Vert$, i.e. it only depends on $\\Gamma$. Minimizing the $\\Vert \\cdot \\Vert_{\\infty}$ norm, we can force just a few modes to be on the $\\Re(\\lambda) =0$ axes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab0ca85",
   "metadata": {},
   "source": [
    "**Question 4.** : As you did for the discrete case, you now have to implement the `LieModule` module. It should have the form indicated above to guarantee $\\Re(\\lambda) \\leq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334387d3",
   "metadata": {
    "id": "ba97f760"
   },
   "outputs": [],
   "source": [
    "# custom nn.Linear to recover the Lie operator\n",
    "class LieModule(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_dim, output_dim))\n",
    "        self.diagonal = # To be Implemented\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=np.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        x, y = X.shape\n",
    "        if x != self.output_dim and y != self.output_dim:\n",
    "            sys.exit(f'Wrong Input Features. Please use tensor with {self.output_dim} Input Features')\n",
    "        ## To be Implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7d230",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e4c55f8",
    "outputId": "0bff3020-50d0-4118-da2b-9b25523a3a68"
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4bebf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53d38a89",
    "outputId": "125c7078-30ba-4359-aee4-a32ce2983ba0"
   },
   "outputs": [],
   "source": [
    "AUTOENCODER = autoencoder(feature_dim, hidden_layer, output_dim).to(device)\n",
    "LIE = LieModule(output_dim).to(device)\n",
    "print(AUTOENCODER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cffcc4",
   "metadata": {
    "id": "dZDn5-VsiLBk"
   },
   "source": [
    "Some tricks are needed to train. If the autoencoder and the Lie model are learned at the same speed, the training turns out to be highly unstable since the three loss functions have moving targets. For this reason, the Lie learning rate has been chosen smaller than the autoencoder one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c090750f",
   "metadata": {
    "id": "ee4be8b4"
   },
   "outputs": [],
   "source": [
    "opt_aut = torch.optim.Adam(AUTOENCODER.parameters(), lr=0.0001, weight_decay=1e-3)\n",
    "opt_lie = torch.optim.Adam(LIE.parameters(), lr=0.00001, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597ea9e",
   "metadata": {
    "id": "Wj5k6s5QizG8"
   },
   "source": [
    "A further loss is considered to stabilize the learning stage. The state $x$ belongs to a compact set, since it is the solution of a dissipative dynamical system. This is not true for $g(x)$ (we need to choose appropriate activation functions to have appropriate Liptchitz guarantees). To avoid discrepancies in magnitudes of $g_i(x)$, a regularization loss is added:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum_m g_i(x) = 0  \\quad \\text{and} \\quad  \\sigma = \\left( \\frac{1}{m}\\sum_m(g_i(x)-\\mu)^2 \\right)^{1/2} = 1\n",
    "$$\n",
    "\n",
    "inspired by VAE.\n",
    "\n",
    "For the training to be smooth, the encoder parameters are not affected by the **prediction loss in phase space**. This is based on an empirical observation and is motivated by the fact that the encoder appears in the three losses and plays a competitive role against the decoder and the Lie model. This should not affect the results since the encoder remains coupled with the decoder in the **reconstruction loss** and with the Lie operator in the **prediction loss in Koopman space**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a2f83",
   "metadata": {},
   "source": [
    "**Question 5.** : Implement the loss function similarly to what you did for the **Question 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c62655",
   "metadata": {
    "id": "e2d9d778"
   },
   "outputs": [],
   "source": [
    "def LOSS(X_, dX, gX, dgX, X_recon, dX_recon, jvp):\n",
    "    # To be Implemented\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e24a1",
   "metadata": {
    "id": "QEiVJGHWmFa9"
   },
   "source": [
    "Since trajectories are not needed, random states can be sampled from the system manifold $x_1 \\in [-2, 2]$, $x_2 \\in [-2, 2]$ (see previous figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e9654",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a4d1ff9",
    "outputId": "da2793c0-f28d-4cad-9435-302d9fe4ed50"
   },
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    AUTOENCODER.train() \n",
    "    LIE.train()\n",
    "    total_train_loss = 0\n",
    "    total_loss1, total_loss2, total_loss3, total_loss4 = 0, 0, 0, 0\n",
    "    for X_, dX_ in train_dataloader:\n",
    "        X_, dX_ = X_.to(device), dX_.to(device)\n",
    "\n",
    "        opt_aut.zero_grad()\n",
    "        opt_lie.zero_grad()\n",
    "         \n",
    "        # dgX = LIE*gX \n",
    "        # jvp = \\nabla g * f(X_)\n",
    "        (gX, jvp) = autograd.functional.jvp(AUTOENCODER.encoder,X_,dX_, create_graph=True)\n",
    "        X_recon = AUTOENCODER.decoder(gX)\n",
    "         \n",
    "        dgX, L = LIE(gX)\n",
    "        dX_recon = AUTOENCODER.decoder(dgX)\n",
    "            \n",
    "        output = LOSS(X_, dX_, gX, dgX, X_recon, dX_recon, jvp)\n",
    "        output.backward()\n",
    "        opt_aut.step()\n",
    "        opt_lie.step()\n",
    "        \n",
    "        if epoch%1 == 0:\n",
    "            total_train_loss += output.item()/X_.size(0)/len(train_dataloader)\n",
    "    if epoch%1 == 0:    \n",
    "        \n",
    "        print(epoch, total_train_loss)\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        AUTOENCODER.eval() \n",
    "        LIE.eval()\n",
    "        with torch.no_grad():\n",
    "            total_test_loss = 0\n",
    "            total_loss1, total_loss2, total_loss3, total_loss4 = 0, 0, 0, 0   \n",
    "            for X_, dX_ in test_dataloader:\n",
    "                X_, dX_ = X_.to(device), dX_.to(device)\n",
    "    \n",
    "                (gX, jvp) = autograd.functional.jvp(AUTOENCODER.encoder,X_,dX_, create_graph=True)\n",
    "                X_recon = AUTOENCODER.decoder(gX)\n",
    "         \n",
    "                dgX, L = LIE(gX)\n",
    "                dX_recon = AUTOENCODER.decoder(dgX)\n",
    "            \n",
    "                output = LOSS(X_, dX_, gX, dgX, X_recon, dX_recon, jvp)\n",
    "                total_test_loss += output.item()/X_.size(0)/len(test_dataloader)\n",
    "            print('-'*50, 'TEST', '-'*50)\n",
    "            print(epoch, total_test_loss)\n",
    "            print('-'*106)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c48dbc",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74a3fd",
   "metadata": {},
   "source": [
    "**Question 6.** : As in the **Question 3.** we want to ensure the Lie operator is stable. This can be verified by checking that the real part of the eigenvalues is negative. Plot the relevant eigenvalues of the Lie operator. You can use the [`numpy.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html) function to retrieve the eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4b924",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "c4wXhwvT6XZ7",
    "outputId": "43286014-e151-49b0-9f00-be3aaaabf7de"
   },
   "outputs": [],
   "source": [
    "#check LIE stability\n",
    "# To Be Implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc7d7f",
   "metadata": {
    "id": "jK7ZfTrBNILp"
   },
   "outputs": [],
   "source": [
    "def DUFFING(x : np.ndarray) -> np.ndarray:\n",
    "    dx = np.zeros(x.shape)\n",
    "    dx[0] = x[1]\n",
    "    dx[1] = x[0] -x[0]**3 \n",
    "    return dx\n",
    "\n",
    "N = 30\n",
    "x = np.linspace(-2, 2, N)\n",
    "y = np.linspace(-2, 2, N)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "Flow = np.zeros((N,N,2))\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        Flow[i,j,:] = DUFFING(np.array([xv[i,j],yv[i,j]]))\n",
    "\n",
    "AUTOENCODER.eval() \n",
    "LIE.eval()\n",
    "Flow_pred = np.zeros((N,N,2))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        Flow_pred[i,j,:] =  AUTOENCODER.decoder(\n",
    "                            torch.matmul(\n",
    "                            AUTOENCODER.encoder(torch.from_numpy(np.array([[xv[i,j],yv[i,j]]])).to(device)), L.t())\n",
    "                            ).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e3c6b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "DjQyXlTnQ4Pl",
    "outputId": "d40ba8f8-81c3-4007-be9e-805ef7655920"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.quiver(xv,yv, Flow[:,:,0], Flow[:,:,1], scale=10)\n",
    "ax.set_title('True')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.quiver(xv,yv, Flow_pred[:,:,0], Flow_pred[:,:,1], scale=10)\n",
    "ax.set_title('Prediction')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "cp = ax.contourf(xv, yv, np.log(np.linalg.norm(Flow - Flow_pred, axis=2)))\n",
    "fig.colorbar(cp)\n",
    "ax.set_title('Error in log scale')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d4a8b",
   "metadata": {},
   "source": [
    "**Question 7.** : Compare and comment below the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188be98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "eigen_penalty_stable.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
