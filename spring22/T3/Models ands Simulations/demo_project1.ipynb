{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: reading and formatting dataset \n",
    "\n",
    "# Q1.1: read German credit dataset \n",
    "dataset = pd.read_csv('GermanCredit.txt', delim_whitespace = True, header = None)\n",
    "dataset.shape\n",
    "\n",
    "# Q1.2: credit risk data \n",
    "m = 800 \n",
    "n = 200\n",
    "ytrain = np.array(dataset.iloc[0:m,-1]) - 1\n",
    "ytest = np.array(dataset.iloc[m:(m+n),-1]) - 1\n",
    "\n",
    "# Q1.3: center and scale features \n",
    "xdata = np.array(dataset.iloc[:, 0:24])\n",
    "xdata = (xdata - np.mean(xdata, axis = 0)) / np.std(xdata, axis = 0)\n",
    "xtrain = xdata[0:m, :]\n",
    "xtest = xdata[m:(m+n), :]\n",
    "\n",
    "# Q1.4 add a column of ones \n",
    "xtrain = np.concatenate((np.ones([m, 1]), xtrain), axis = 1)\n",
    "xtest = np.concatenate((np.ones([n, 1]), xtest), axis = 1)\n",
    "\n",
    "# dimension\n",
    "D = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install jax package\n",
    "import sys\n",
    "!{sys.executable} -m pip install jax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 write log-likelihood function in numpy \n",
    "def loglikelihood_numpy(beta):\n",
    "    x_beta = np.matmul(xtrain, beta)\n",
    "    output = np.sum(ytrain * x_beta - np.log(1 + np.exp(x_beta)))\n",
    "    return output\n",
    "\n",
    "# simulate regression coeff\n",
    "beta = np.random.randn(D)  \n",
    "print(loglikelihood_numpy(beta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "# write log-likelihood function in a JAX-compatiable way\n",
    "@jit\n",
    "def loglikelihood_jax(beta):\n",
    "    x_beta = jnp.matmul(xtrain, beta)\n",
    "    output = jnp.sum(ytrain * x_beta - jnp.log(1 + jnp.exp(x_beta)))\n",
    "    return output\n",
    "\n",
    "# check that we get the same result as the numpy implementation\n",
    "print(loglikelihood_jax(beta))\n",
    "\n",
    "# there is no point using JIT for the log-likelihood\n",
    "jit_loglikelihood_jax = jit(loglikelihood_jax)\n",
    "\n",
    "import time\n",
    "%timeit jit_loglikelihood_jax(beta) # timing with JIT \n",
    "%timeit loglikelihood_jax(beta) # timing without JIT\n",
    "\n",
    "# use the implementation without JIT\n",
    "loglikelihood = loglikelihood_jax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: auto-diff to get the gradient of the log-likelihood\n",
    "from jax import grad\n",
    "gradloglikelihood1 = grad(loglikelihood_jax) # without JIT\n",
    "gradloglikelihood2 = jit(grad(loglikelihood_jax)) # with JIT \n",
    "\n",
    "# for the gradient, there is a signfiicant speedup with JIT\n",
    "%timeit gradloglikelihood1(beta)\n",
    "%timeit gradloglikelihood2(beta)\n",
    "\n",
    "# use the faster implementation\n",
    "gradloglikelihood = jit(grad(loglikelihood_jax))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92682f8464e9044ba142483ddeefd87e1deb5806fbb1da5a3f470e5bd39935be"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
