{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NeImLrWIlK2",
        "outputId": "a7efc371-a550-4362-ff31-9fef1ecd7503"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 112 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 122 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 163 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 174 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 4.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=53dbf3532530adb62f73674f836181271a0aece92f55c7c2d8823e18c7509b9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T11:35:12.193318Z",
          "start_time": "2020-12-23T11:35:12.134663Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLjiDrcdH2Ws",
        "outputId": "d5e4f6d8-0c45-4203-aa70-df19570335cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
          ]
        }
      ],
      "source": [
        "# Base and Cleaning \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "#Natural Language Processing (NLP)\n",
        "import spacy  \n",
        "from spacy.tokenizer import Tokenizer\n",
        "import gensim\n",
        "from gensim.corpora import Dictionary \n",
        "from gensim import models\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
        "from wordcloud import STOPWORDS\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "#Visualizations\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# settings for easier coding and inspection of data  \n",
        "%matplotlib notebook\n",
        "pd.set_option('display.max_colwidth', -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T11:35:12.438152Z",
          "start_time": "2020-12-23T11:35:12.435193Z"
        },
        "id": "m2MH60wDH2Wu"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')#\"error\", \"ignore\", \"always\", \"default\", \"module\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMy-nVNRIRv0",
        "outputId": "021d088c-f05c-4347-fbb6-95ca7ee308fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T11:36:24.772103Z",
          "start_time": "2020-12-23T11:36:24.393669Z"
        },
        "id": "6ztQo9YnH2Wu"
      },
      "outputs": [],
      "source": [
        "df_import = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/EL/cyberbullying_tweets.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T11:36:26.351682Z",
          "start_time": "2020-12-23T11:36:26.330016Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "oOnvOW_3H2Wv",
        "outputId": "1736ae3e-0d10-4e67-837a-7ac917eee94b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ac55d861-615f-4307-9c97-3278a68a07f7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was crapilicious! #mkr</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red velvet cupcakes?</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, but not too concerned about another angry dude on twitter.</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretending to be a Kurdish account.  Like Islam, it is all lies.</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47687</th>\n",
              "      <td>Black ppl aren't expected to do anything, depended on for anything. Yet free to participate, work, enjoy the freedom of humans all across this globe. If you waste your energy on the past you will be wasting it for building the future.</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47688</th>\n",
              "      <td>Turner did not withhold his disappointment. Turner called the court an “abominable conclave of negro hating demons” (with one exception) who “issued another decree that colored men and women must be driven into Jim Crow cars whenever it suits the whim of any white community.”</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47689</th>\n",
              "      <td>I swear to God. This dumb nigger bitch. I have got to bleach my hair reeeeeal fuckin' soon. D:&amp;lt; FUCK.</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47690</th>\n",
              "      <td>Yea fuck you RT @therealexel: IF YOURE A NIGGER FUCKING UNFOLLOW ME, FUCKING DUMB NIGGERS.</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47691</th>\n",
              "      <td>Bro. U gotta chill RT @CHILLShrammy: Dog FUCK KP that dumb nigger bitch lmao</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47692 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac55d861-615f-4307-9c97-3278a68a07f7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac55d861-615f-4307-9c97-3278a68a07f7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac55d861-615f-4307-9c97-3278a68a07f7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                 tweet_text  \\\n",
              "0      In other words #katandandre, your food was crapilicious! #mkr                                                                                                                                                                                                                          \n",
              "1      Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc                                                                                                                                                                    \n",
              "2      @XochitlSuckkks a classy whore? Or more red velvet cupcakes?                                                                                                                                                                                                                           \n",
              "3      @Jason_Gio meh. :P  thanks for the heads up, but not too concerned about another angry dude on twitter.                                                                                                                                                                                \n",
              "4      @RudhoeEnglish This is an ISIS account pretending to be a Kurdish account.  Like Islam, it is all lies.                                                                                                                                                                                \n",
              "...                                                                                                        ...                                                                                                                                                                                \n",
              "47687  Black ppl aren't expected to do anything, depended on for anything. Yet free to participate, work, enjoy the freedom of humans all across this globe. If you waste your energy on the past you will be wasting it for building the future.                                             \n",
              "47688  Turner did not withhold his disappointment. Turner called the court an “abominable conclave of negro hating demons” (with one exception) who “issued another decree that colored men and women must be driven into Jim Crow cars whenever it suits the whim of any white community.”   \n",
              "47689  I swear to God. This dumb nigger bitch. I have got to bleach my hair reeeeeal fuckin' soon. D:&lt; FUCK.                                                                                                                                                                               \n",
              "47690  Yea fuck you RT @therealexel: IF YOURE A NIGGER FUCKING UNFOLLOW ME, FUCKING DUMB NIGGERS.                                                                                                                                                                                             \n",
              "47691  Bro. U gotta chill RT @CHILLShrammy: Dog FUCK KP that dumb nigger bitch lmao                                                                                                                                                                                                           \n",
              "\n",
              "      cyberbullying_type  \n",
              "0      not_cyberbullying  \n",
              "1      not_cyberbullying  \n",
              "2      not_cyberbullying  \n",
              "3      not_cyberbullying  \n",
              "4      not_cyberbullying  \n",
              "...                  ...  \n",
              "47687  ethnicity          \n",
              "47688  ethnicity          \n",
              "47689  ethnicity          \n",
              "47690  ethnicity          \n",
              "47691  ethnicity          \n",
              "\n",
              "[47692 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df_import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T11:36:58.964271Z",
          "start_time": "2020-12-23T11:36:58.953375Z"
        },
        "id": "z9v4eRjhH2Wv"
      },
      "outputs": [],
      "source": [
        "temp = df_import.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:45:08.954992Z",
          "start_time": "2020-12-23T07:45:08.951820Z"
        },
        "id": "t6hzuSe2H2Ww"
      },
      "outputs": [],
      "source": [
        "import emoji  # needs to be installed with pip\n",
        "import regex # needs to be installed with pip\n",
        "import re\n",
        "import string\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:45:08.964839Z",
          "start_time": "2020-12-23T07:45:08.958334Z"
        },
        "id": "hIRlPG7TH2Ww"
      },
      "outputs": [],
      "source": [
        "def give_emoji_free_text(text):\n",
        "    \"\"\"\n",
        "    Removes emoji's from tweets\n",
        "    Accepts:\n",
        "        Text (tweets)\n",
        "    Returns:\n",
        "        Text (emoji free tweets)\n",
        "    \"\"\"\n",
        "    emoji_list = [c for c in text if c in emoji.UNICODE_EMOJI]\n",
        "    if not emoji_list==[] :\n",
        "        print(\"\",emoji_list,end=\"\")\n",
        "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
        "    return clean_text\n",
        "\n",
        "def url_free_text(text):   \n",
        "    '''\n",
        "    Cleans text from urls\n",
        "    '''\n",
        "    text, _ = re.subn(r'http\\S+', '', text)\n",
        "    return text \n",
        "\n",
        "def url_free_text_s(text):  # as some https were missed out on by the previous function\n",
        "    '''\n",
        "    Cleans text from urls\n",
        "    '''\n",
        "    text, _ = re.subn(r'https\\S+', '', text)\n",
        "    return text  \n",
        "\n",
        "\n",
        "# Apply the function above and get tweets free of emoji's\n",
        "call_emoji_free = lambda x: give_emoji_free_text(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:45:09.833564Z",
          "start_time": "2020-12-23T07:45:08.966928Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMIOBj49H2Wx",
        "outputId": "ba1a2cf0-21e9-4766-de2a-9c017bb51195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not_cyberbullying 's tweets included the following emojis :\n",
            "\n",
            "\n",
            "gender 's tweets included the following emojis :\n",
            "\n",
            "\n",
            "religion 's tweets included the following emojis :\n",
            "\n",
            "\n",
            "other_cyberbullying 's tweets included the following emojis :\n",
            "\n",
            "\n",
            "age 's tweets included the following emojis :\n",
            "\n",
            "\n",
            "ethnicity 's tweets included the following emojis :\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply `call_emoji_free` which calls the function to remove all emoji's\n",
        "# Apply 'url_free_text'which calls the function to remove all links\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn' turn off one the warning ... \n",
        "\n",
        "for c_b in df_import.cyberbullying_type.unique():\n",
        "    temp_df = df_import[df_import.cyberbullying_type ==c_b]\n",
        "    print(c_b,\"'s tweets included the following emojis :\")\n",
        "    temp_df.tweet_text = temp_df.tweet_text.apply(call_emoji_free)\n",
        "    print(\"\\n\")\n",
        "    temp_df.tweet_text =temp_df.tweet_text.apply(url_free_text) \n",
        "    temp_df.tweet_text =temp_df.tweet_text.apply(url_free_text_s)\n",
        "    df_import[df_import.cyberbullying_type ==c_b] = temp_df\n",
        "    \n",
        "df_prep=df_import # df_prep = result of preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:45:10.524113Z",
          "start_time": "2020-12-23T07:45:09.835638Z"
        },
        "id": "8558KfPlH2Wy"
      },
      "outputs": [],
      "source": [
        "# Load spacy\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "#nlp = spacy.load('en_core_web_sm')\n",
        "nlp = en_core_web_sm.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:45:10.537408Z",
          "start_time": "2020-12-23T07:45:10.526625Z"
        },
        "id": "15CQs1GFH2Wy"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "# Custom stopwords\n",
        "custom_stopwords = ['hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \n",
        "                    \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
        "\n",
        "\n",
        "# Customize stop words by adding to the default list\n",
        "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
        "\n",
        "# ALL_STOP_WORDS = spacy + gensim + wordcloud \n",
        "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:45:10.545915Z",
          "start_time": "2020-12-23T07:45:10.541687Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuMie-wKH2Wy",
        "outputId": "6ec7db52-6615-4ba4-aefb-c741041a2fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These are 10 of our stopwords:\n",
            "\n",
            "did\n",
            "his\n",
            "above\n",
            "beside\n",
            "themselves\n",
            "we'll\n",
            "detail\n",
            "who's\n",
            "everything\n",
            "top\n"
          ]
        }
      ],
      "source": [
        "n = 10\n",
        "print(\"These are\",n,\"of our stopwords:\\n\")\n",
        "for i in list(ALL_STOP_WORDS)[:n]:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:45:14.635860Z",
          "start_time": "2020-12-23T07:45:10.549599Z"
        },
        "id": "OASRaSMDH2Wz"
      },
      "outputs": [],
      "source": [
        "tokens = []\n",
        "\n",
        "for doc in tokenizer.pipe(df_prep.tweet_text,batch_size=500):\n",
        "    doc_tokens = []    \n",
        "    for token in doc: \n",
        "        if token.text.lower() not in ALL_STOP_WORDS:  \n",
        "            doc_tokens.append(token.text.lower())   \n",
        "    tokens.append(doc_tokens)\n",
        "    \n",
        "# Makes tokens column\n",
        "df_prep['tokens'] = tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:45:14.653862Z",
          "start_time": "2020-12-23T07:45:14.638095Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "lss5LW5iH2Wz",
        "outputId": "59476cd0-f385-49fb-96a6-0a3235de8e07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a948dfa0-df61-4ebe-9c55-eb43283bcb37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>45149</th>\n",
              "      <td>dumb bitches ! Go drink some bleach RT @tayyoung_: FUCK OBAMA, dumb ass nigger</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>[dumb, bitches, !, drink, bleach, rt, @tayyoung_:, fuck, obama,, dumb, ass, nigger]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44924</th>\n",
              "      <td>Youre a nigger and you hate your life hmm, really? Lmfao Briana is a dumb cunt. Fuck you bitcchh.</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>[youre, nigger, hate, life, hmm,, really?, lmfao, briana, dumb, cunt., fuck, bitcchh.]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a948dfa0-df61-4ebe-9c55-eb43283bcb37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a948dfa0-df61-4ebe-9c55-eb43283bcb37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a948dfa0-df61-4ebe-9c55-eb43283bcb37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                              tweet_text  \\\n",
              "45149  dumb bitches ! Go drink some bleach RT @tayyoung_: FUCK OBAMA, dumb ass nigger                      \n",
              "44924  Youre a nigger and you hate your life hmm, really? Lmfao Briana is a dumb cunt. Fuck you bitcchh.   \n",
              "\n",
              "      cyberbullying_type  \\\n",
              "45149  ethnicity           \n",
              "44924  ethnicity           \n",
              "\n",
              "                                                                                       tokens  \n",
              "45149  [dumb, bitches, !, drink, bleach, rt, @tayyoung_:, fuck, obama,, dumb, ass, nigger]     \n",
              "44924  [youre, nigger, hate, life, hmm,, really?, lmfao, briana, dumb, cunt., fuck, bitcchh.]  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df_prep.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:45:14.718347Z",
          "start_time": "2020-12-23T07:45:14.655781Z"
        },
        "id": "ymJOqEDJH2W0"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_prep['tokens_back_to_text'] = [' '.join(map(str, l)) for l in df_prep['tokens']]\n",
        "\n",
        "\n",
        "# to remove weird characters and valueless words (in sense of our analysis),\n",
        "# we want to remove words with len<=2\n",
        "# to keep interesting words with this length, we define the following list\n",
        "\n",
        "list_useful_2char_word = [\"uk\",\"us\",\"ad\"] \n",
        "\n",
        "\n",
        "def get_lemmas(text):\n",
        "    '''Used to lemmatize the processed tweets'''\n",
        "    lemmas = []\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    for token in doc: \n",
        "        if ((token.is_stop == False) and (token.is_punct == False) and \n",
        "                (token.pos_!= 'PRON') and (token.is_digit== False) and \n",
        "                ((len(token)>2) or (token.text in list_useful_2char_word))): \n",
        "            lemmas.append(token.lemma_)\n",
        "    \n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:48:17.168672Z",
          "start_time": "2020-12-23T07:45:14.720317Z"
        },
        "id": "qh4jpdf5H2W0"
      },
      "outputs": [],
      "source": [
        "# appllication of get_lemmas\n",
        "df_prep['lemmas'] = df_prep['tokens_back_to_text'].apply(get_lemmas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:48:17.193068Z",
          "start_time": "2020-12-23T07:48:17.170925Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "PLMW14IXH2W0",
        "outputId": "08bf8395-bc95-4c60-f4e3-cb8e85b299b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3af73dc3-a336-4f6e-84ec-a9e24e8f82be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tokens_back_to_text</th>\n",
              "      <th>lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20654</th>\n",
              "      <td>#StandwithSajid Indian muslims are supporting every anti nationals and they think we are idiots you gonna suffer your karma</td>\n",
              "      <td>religion</td>\n",
              "      <td>[#standwithsajid, indian, muslims, supporting, anti, nationals, think, idiots, gonna, suffer, karma]</td>\n",
              "      <td>#standwithsajid indian muslims supporting anti nationals think idiots gonna suffer karma</td>\n",
              "      <td>[standwithsajid, indian, muslims, support, anti, national, think, idiot, go, suffer, karma]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3788</th>\n",
              "      <td>@mykitchenrules where is this place? It's beautiful! #MKR</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>[@mykitchenrules, place?, beautiful!, #mkr]</td>\n",
              "      <td>@mykitchenrules place? beautiful! #mkr</td>\n",
              "      <td>[@mykitchenrule, place, beautiful, mkr]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27519</th>\n",
              "      <td>ok time to write code bbl.</td>\n",
              "      <td>other_cyberbullying</td>\n",
              "      <td>[ok, time, write, code, bbl.]</td>\n",
              "      <td>ok time write code bbl.</td>\n",
              "      <td>[time, write, code, bbl]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29592</th>\n",
              "      <td>So, I guess it's really happened. The #mkr competition is running for eternity.</td>\n",
              "      <td>other_cyberbullying</td>\n",
              "      <td>[so,, guess, happened., #mkr, competition, running, eternity.]</td>\n",
              "      <td>so, guess happened. #mkr competition running eternity.</td>\n",
              "      <td>[guess, happen, mkr, competition, run, eternity]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3af73dc3-a336-4f6e-84ec-a9e24e8f82be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3af73dc3-a336-4f6e-84ec-a9e24e8f82be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3af73dc3-a336-4f6e-84ec-a9e24e8f82be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                                        tweet_text  \\\n",
              "20654  #StandwithSajid Indian muslims are supporting every anti nationals and they think we are idiots you gonna suffer your karma   \n",
              "3788   @mykitchenrules where is this place? It's beautiful! #MKR                                                                     \n",
              "27519  ok time to write code bbl.                                                                                                    \n",
              "29592  So, I guess it's really happened. The #mkr competition is running for eternity.                                               \n",
              "\n",
              "        cyberbullying_type  \\\n",
              "20654  religion              \n",
              "3788   not_cyberbullying     \n",
              "27519  other_cyberbullying   \n",
              "29592  other_cyberbullying   \n",
              "\n",
              "                                                                                                     tokens  \\\n",
              "20654  [#standwithsajid, indian, muslims, supporting, anti, nationals, think, idiots, gonna, suffer, karma]   \n",
              "3788   [@mykitchenrules, place?, beautiful!, #mkr]                                                            \n",
              "27519  [ok, time, write, code, bbl.]                                                                          \n",
              "29592  [so,, guess, happened., #mkr, competition, running, eternity.]                                         \n",
              "\n",
              "                                                                            tokens_back_to_text  \\\n",
              "20654  #standwithsajid indian muslims supporting anti nationals think idiots gonna suffer karma   \n",
              "3788   @mykitchenrules place? beautiful! #mkr                                                     \n",
              "27519  ok time write code bbl.                                                                    \n",
              "29592  so, guess happened. #mkr competition running eternity.                                     \n",
              "\n",
              "                                                                                            lemmas  \n",
              "20654  [standwithsajid, indian, muslims, support, anti, national, think, idiot, go, suffer, karma]  \n",
              "3788   [@mykitchenrule, place, beautiful, mkr]                                                      \n",
              "27519  [time, write, code, bbl]                                                                     \n",
              "29592  [guess, happen, mkr, competition, run, eternity]                                             "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "df_prep.sample(4) # visual inspection of lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:48:17.271522Z",
          "start_time": "2020-12-23T07:48:17.195246Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjeTus4pH2W0",
        "outputId": "ab91e311-0ba6-494b-9412-a5a9457a4d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are still 2445 tweets with lemmas of length <=2 that are not allowed. \n",
            "These too short lemmas are:\n",
            "\n",
            "go,go,go,go,go,go,go,go,go,go,go,ff,go,go,ff,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,lb,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,10,go,go,go,go,ff,go,up,go,go,do,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,op,go,go,go,go,go,ff,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,do,go,go,go,go,go,go,go,go,do,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,ht,go,go,go,go,do,go,go,go,go,go,go,go,go,go,do,go,go,go,go,go,go,pc,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,do,go,go,go,go,go,go,go,go,go,go,go,go,no,go,go,go,go,go,go,go,go,go,go,go,go,go,go,re,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,aw,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,10,go,go,rt,go,go,go,go,go,go,go,go,go,go,go,go,go,go,ff,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,80,go,go,go,go,go,go,te,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,yr,go,80,go,go,go,go,go,go,go,go,go,go,go,go,go,go,80,go,go,go,go,go,up,go,go,go,go,go,go,go,go,go,70,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,90,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,de,go,go,20,go,go,go,go,go,go,go,80,go,go,go,go,go,lb,go,go,go,go,ge,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,do,go,go,go,go,go,go,go,up,go,go,go,go,go,go,go,go,f,in,go,go,go,go,go,go,go,go,go,go,go,go,go,go,wp,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,80,38,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,90,go,go,te,te,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,rs,go,go,go,go,go,go,go,go,go,ld,go,rs,go,go,go,go,go,go,go,go,go,go,go,go,go,e,go,go,go,go,ur,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,70,80,go,go,go,go,go,go,go,go,rs,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,rs,go,go,go,go,go,go,go,go,th,go,go,go,go,go,go,go,go,go,go,go,80,80,go,go,go,go,up,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,e,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,ff,go,go,go,go,pe,go,go,go,go,go,go,go,go,35,go,go,go,go,go,go,go,go,go,go,go,go,go,ff,ff,go,go,go,go,go,go,go,go,mp,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,e,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,re,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,f,go,go,go,go,go,go,go,go,go,yr,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,pc,go,go,do,go,go,go,go,go,go,go,go,go,go,go,go,go,go,90,go,go,go,rt,do,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,30,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,do,go,go,go,go,go,go,go,go,go,go,go,hr,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,do,no,go,go,go,go,go,go,go,go,go,go,go,go,ip,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,yr,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,as,go,go,lb,go,go,go,go,go,go,go,go,go,ht,go,aw,go,go,go,go,go,go,go,go,bg,go,go,go,no,go,go,on,go,go,go,go,go,go,go,go,go,go,in,go,ff,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,re,go,20,go,go,ff,do,go,go,ip,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,id,no,go,go,op,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,80,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,de,da,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,80,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,lb,go,go,go,go,go,go,go,up,go,go,go,go,go,go,go,go,50,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,90,go,go,go,go,go,go,70,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,up,go,80,go,go,go,go,go,go,go,go,go,go,go,20,go,go,go,go,go,go,go,go,go,go,80,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,ex,30,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,ge,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,dm,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,90,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,al,go,go,go,go,go,do,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,we,go,go,go,go,hr,go,go,go,go,go,go,go,go,go,go,go,40,go,go,go,go,80,80,go,go,go,go,go,go,go,go,go,go,80,go,go,up,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,90,go,go,go,pc,up,go,go,go,75,go,go,go,go,go,lb,go,go,go,go,20,30,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,90,go,go,go,go,go,go,go,do,go,go,go,go,go,go,go,go,go,go,go,80,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,up,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,up,go,go,go,go,40,go,go,go,go,go,00,go,go,go,go,go,go,go,go,go,do,go,go,go,go,go,yr,go,go,go,go,go,go,go,80,go,go,go,go,30,go,go,go,go,go,go,go,30,go,go,go,80,90,go,go,go,go,go,go,go,go,go,go,00,go,go,go,go,go,go,go,go,go,go,go,go,go,go,ff,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,ge,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,do,70,80,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,lb,go,go,go,go,go,go,20,go,go,go,go,go,go,up,go,go,go,go,go,go,do,80,go,go,go,go,go,go,as,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,do,go,go,go,go,ff,60,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,do,go,go,pc,go,up,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,di,go,go,ff,go,go,go,go,go,go,go,go,ur,go,go,go,go,go,do,go,go,go,go,go,go,60,go,go,up,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,up,go,50,60,ff,go,go,do,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,re,go,go,go,go,go,go,go,go,go,q',go,we,go,go,go,go,di,go,go,go,go,go,go,go,go,go,18,go,go,up,go,go,go,go,go,go,go,do,go,go,go,go,go,go,go,go,go,go,go,go,in,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,as,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,60,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,do,go,go,go,go,go,do,go,do,go,go,go,go,go,go,go,go,do,go,go,go,go,go,go,go,go,go,do,go,go,go,go,go,go,go,go,go,op,go,op,go,go,de,go,do,up,go,go,go,go,go,go,go,go,go,go,go,go,as,go,go,er,go,go,go,go,go,ur,go,go,go,go,go,go,cd,go,go,go,go,go,go,go,go,40,go,go,go,go,go,go,go,go,go,go,go,go,go,as,go,go,go,go,go,go,go,go,go,go,go,do,do,go,go,go,go,go,do,go,go,go,go,go,go,go,go,op,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,op,[None None None ... None None None]\n"
          ]
        }
      ],
      "source": [
        "## check if we still got tokens with length <= 2 that are not specified in out list\n",
        "def too_short_lemmas(x):\n",
        "    for single_lemma in x:\n",
        "        if (len(single_lemma)<=2) and (single_lemma not in list_useful_2char_word):\n",
        "                print(single_lemma,end=\",\")\n",
        "                \n",
        "\n",
        "def check_for_min_length_lemma(x):\n",
        "    too_short=False\n",
        "    for single_lemma in x:\n",
        "        if (len(single_lemma)<=2) and (single_lemma not in list_useful_2char_word):\n",
        "            too_short = True\n",
        "    return too_short\n",
        "\n",
        "\n",
        "print(\"There are still\",\n",
        "      len(df_prep[df_prep.lemmas.apply(lambda x:check_for_min_length_lemma(x))]),\n",
        "         \"tweets with lemmas of length <=2 that are not allowed.\",\n",
        "     \"\\nThese too short lemmas are:\\n\")\n",
        "\n",
        "# check if these lemmas can be removed or if they include important information\n",
        "print(df_prep.lemmas.apply(lambda x:too_short_lemmas(x)).values) \n",
        "\n",
        "\n",
        "# this happened because the token was for example going --> len(going)>2 but lemma = go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:48:17.335994Z",
          "start_time": "2020-12-23T07:48:17.273698Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCnce2RBH2W1",
        "outputId": "fae410ca-fb88-479f-83ea-7c72edf4d853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are still 6 tweets with lemmas of length <=2 that are not allowed.\n"
          ]
        }
      ],
      "source": [
        "def remove_too_short(x):\n",
        "    for single_lemma in x:\n",
        "        if (len(single_lemma)<=2) and (single_lemma not in list_useful_2char_word):\n",
        "                x.remove(single_lemma)\n",
        "    return x\n",
        "\n",
        "df_prep.lemmas = df_prep.lemmas.apply(lambda x:remove_too_short(x))\n",
        "\n",
        "print(\"There are still\",\n",
        "      len(df_prep[df_prep.lemmas.apply(lambda x:check_for_min_length_lemma(x))]),\n",
        "         \"tweets with lemmas of length <=2 that are not allowed.\")\n",
        "# because of the small amount the effort to catch these slips was rated to high"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prep['lemmas_back_to_text'] = [' '.join(map(str,l)) for l in df_prep['lemmas']]"
      ],
      "metadata": {
        "id": "ylKj3KzVL2CJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:48:17.610419Z",
          "start_time": "2020-12-23T07:48:17.604301Z"
        },
        "id": "lVcN-HjFH2W2"
      },
      "outputs": [],
      "source": [
        "lemma_tokens_all = []\n",
        "# Tokenizer function\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Parses a string into a list of semantic units (words)\n",
        "    Args:\n",
        "        text (str): The string that the function will tokenize.\n",
        "    Returns:\n",
        "        list: tokens parsed out\n",
        "    \"\"\"\n",
        "    # first of the following does not seem to work - or is this bcs of string digits\n",
        "    tokens = re.sub(r'[^a-zA-Z 0-9]', '', text) \n",
        "    tokens = re.sub(r'[%s]' % re.escape(string.punctuation), '', text) # Remove punctuation\n",
        "    tokens = re.sub(r'\\w*\\d\\w*', '', text) # Remove words containing numbers\n",
        "    tokens = re.sub(r'@*!*\\$*', '', text) # Remove @ ! $\n",
        "    tokens = tokens.strip(',') \n",
        "    tokens = tokens.strip('?') \n",
        "    tokens = tokens.strip('!') \n",
        "    tokens = tokens.strip(\"'\") \n",
        "    tokens = tokens.strip(\".\") \n",
        "\n",
        "\n",
        "    tokens = tokens.lower().split() # Make text lowercase and split it\n",
        "                \n",
        "    lemma_tokens_all.extend(tokens)\n",
        "    return tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:48:18.839247Z",
          "start_time": "2020-12-23T07:48:17.612466Z"
        },
        "id": "wM7rw8M_H2W2"
      },
      "outputs": [],
      "source": [
        "# Apply tokenizer to df_all_orgas_cut_off and parallel create a list for every cyberbullying_type\n",
        "# witl all lemma tokens\n",
        "\n",
        "temp_series = pd.Series()\n",
        "for i in df_prep.cyberbullying_type.unique():\n",
        "    temp_df_part= df_prep[df_prep.cyberbullying_type==i]\n",
        "    exec(f'list_{i}=[]') # create list to store all cyberbullying_type specific lemma tokens\n",
        "    temp_df_part['lemma_tokens'] = temp_df_part['lemmas_back_to_text'].apply(tokenize)\n",
        "    exec(f'list_{i}.extend(lemma_tokens_all)')\n",
        "    lemma_tokens_all=[]\n",
        "    temp_series= temp_series.append(temp_df_part['lemma_tokens'])\n",
        "\n",
        "df_prep['lemma_tokens'] =temp_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:48:18.857257Z",
          "start_time": "2020-12-23T07:48:18.841859Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "qHUA8lmrH2W2",
        "outputId": "37ef6548-b4d0-4562-bbb9-e35e74ed5b86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d07e02e6-9900-4e79-9e34-31f35683be00\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tokens_back_to_text</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>lemmas_back_to_text</th>\n",
              "      <th>lemma_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was crapilicious! #mkr</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>[words, #katandandre,, food, crapilicious!, #mkr]</td>\n",
              "      <td>words #katandandre, food crapilicious! #mkr</td>\n",
              "      <td>[word, katandandre, food, crapilicious, mkr]</td>\n",
              "      <td>word katandandre food crapilicious mkr</td>\n",
              "      <td>[word, katandandre, food, crapilicious, mkr]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>[#aussietv, white?, #mkr, #theblock, #imacelebrityau, #today, #sunrise, #studio10, #neighbours, #wonderlandten, #etc]</td>\n",
              "      <td>#aussietv white? #mkr #theblock #imacelebrityau #today #sunrise #studio10 #neighbours #wonderlandten #etc</td>\n",
              "      <td>[aussietv, white, mkr, theblock, imacelebrityau, today, sunrise, studio10, neighbour, wonderlandten, etc]</td>\n",
              "      <td>aussietv white mkr theblock imacelebrityau today sunrise studio10 neighbour wonderlandten etc</td>\n",
              "      <td>[aussietv, white, mkr, theblock, imacelebrityau, today, sunrise, studio10, neighbour, wonderlandten, etc]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d07e02e6-9900-4e79-9e34-31f35683be00')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d07e02e6-9900-4e79-9e34-31f35683be00 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d07e02e6-9900-4e79-9e34-31f35683be00');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                            tweet_text  \\\n",
              "0  In other words #katandandre, your food was crapilicious! #mkr                                                         \n",
              "1  Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc   \n",
              "\n",
              "  cyberbullying_type  \\\n",
              "0  not_cyberbullying   \n",
              "1  not_cyberbullying   \n",
              "\n",
              "                                                                                                                  tokens  \\\n",
              "0  [words, #katandandre,, food, crapilicious!, #mkr]                                                                       \n",
              "1  [#aussietv, white?, #mkr, #theblock, #imacelebrityau, #today, #sunrise, #studio10, #neighbours, #wonderlandten, #etc]   \n",
              "\n",
              "                                                                                         tokens_back_to_text  \\\n",
              "0  words #katandandre, food crapilicious! #mkr                                                                 \n",
              "1  #aussietv white? #mkr #theblock #imacelebrityau #today #sunrise #studio10 #neighbours #wonderlandten #etc   \n",
              "\n",
              "                                                                                                      lemmas  \\\n",
              "0  [word, katandandre, food, crapilicious, mkr]                                                                \n",
              "1  [aussietv, white, mkr, theblock, imacelebrityau, today, sunrise, studio10, neighbour, wonderlandten, etc]   \n",
              "\n",
              "                                                                             lemmas_back_to_text  \\\n",
              "0  word katandandre food crapilicious mkr                                                          \n",
              "1  aussietv white mkr theblock imacelebrityau today sunrise studio10 neighbour wonderlandten etc   \n",
              "\n",
              "                                                                                                lemma_tokens  \n",
              "0  [word, katandandre, food, crapilicious, mkr]                                                               \n",
              "1  [aussietv, white, mkr, theblock, imacelebrityau, today, sunrise, studio10, neighbour, wonderlandten, etc]  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df_prep.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:49:46.837192Z",
          "start_time": "2020-12-23T07:49:46.759196Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46C4t3kUH2W2",
        "outputId": "cb493bb6-a8b5-43bb-b62a-2d62031c926d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1950 is of type <class 'str'> and found in: ['clippertea', 'target', 'demographic', '1950', 'businessman', 'apparently', 'fun', 'fact', 'draper', 'fucking', 'love', 'tea'] \n",
            "\n",
            "me is of type <class 'str'> and found in: ['verywhiteguy', 'me', 'twitter', 'search', 'look', 'block', 'manually', 'write'] \n",
            "\n",
            "na is of type <class 'str'> and found in: ['rgay', 'conversation', 'interest', 'have', 'write', 'possible', 'not', 'write', 'na'] \n",
            "\n",
            "th is of type <class 'str'> and found in: ['guiltyx', 'jillpantozzi', 'report', 'abuse', 'twitter', 'api', 'privacy', 'hack', 'support', 'freebsdgirl', 'th'] \n",
            "\n",
            "br is of type <class 'str'> and found in: ['br', 'yes', 'yes', 'money'] \n",
            "\n",
            "de is of type <class 'str'> and found in: ['zeynep', 'ugh', 'twitter', 'well', 'report', 'spam', 'block', 'single', 'menu', 'recombine', 'page', 'de'] \n",
            "\n",
            "vp is of type <class 'str'> and found in: ['mumtazceltik', 'whitehouse', 'vp', 'kobane', 'jesuischarlie', 'turkish', 'military', 'say', 'mit', 'ship', 'weapon', \"qaeda'fehimtastekin\"] \n",
            "\n",
            "go is of type <class 'str'> and found in: ['read', 'fucking', 'stupid', 'boy', 'blow', 'think', 'go', 'djdk'] \n",
            "\n",
            "ap is of type <class 'str'> and found in: ['cnn', 'abc', 'nytime', 'washingtonpost', 'msnbc', 'cbsnews', 'foxnews', 'ap', 'bbcworld', 'reuter', 'folk', 'look', 'mother', 'eye', 'deeply', 'feel', 'hear', 'woman', 'f**king', 'bitch', 'friggin', 'female', 'dog', 'heat', 'include', 'mother', 'female'] \n",
            "\n",
            "1600 is of type <class 'str'> and found in: ['word', 'use', 'come', '1600', 'bother', 'look', 'origin', 'let', 'man', 'man', 'literally', 'represent', 'rape', 'culture', 'thank', 'prove', 'sexist', 'homophobic', 'gay', 'mention', 'sexuality'] \n",
            "\n",
            "1970 is of type <class 'str'> and found in: ['sub', '1970', 'gay', 'joke', 'rape', 'joke', 'itv', 'raise', 'bar', 'new', 'sitcom', 'vicious'] \n",
            "\n",
            "2000 is of type <class 'str'> and found in: ['good', 'millennial', 'pick', 'humor', '90-', 'early', '2000', 'joke', 'pedophilia', 'gay', 'rape', 'racist', 'shit', 'imagine', 'gen', 'z”a', 'culture', 'center', 'humor', 'scoot', 'mother', 'vageen', 'wait', 'till', 'hear', 'real', 'joke', 'come'] \n",
            "\n",
            "1970 is of type <class 'str'> and found in: ['holy', 'shit', 'special', 'brand', '1970', 'racism', 'homophobia', 'sexism', 'character', 'name', 'jemima', 'brown', 'super', 'gay', 'dude', 'rape', 'joke', 'man', 'hard', 'watch'] \n",
            "\n",
            "go is of type <class 'str'> and found in: ['thing', 'think', 'maher', 'try', 'joke', 'rape', 'joke', 'gay', 'people', 'sex', 'habits”-', 'think', 'say', 'cohen', 'go', 'aspect', 'prison', 'life', 'stereotypically', 'depict', 'mass', 'medium', 'deserve'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['fatf', 'terrorist', 'nation', 'pakistan', 'start', 'recruit', 'kid', 'wage', 'jihad', 'india', 'high', 'time', 'fatfshouldblacklistpakistan', 'save', 'world', 'save', 'humanity', 'fatfnews', 'un', 'fatf', 'fatfblacklist'] \n",
            "\n",
            "1000 is of type <class 'str'> and found in: ['maratha', 'come', 'british', 'get', 'rule', 'muslims', 'yrs', 'idiot', 'ppl', 'pakistan', 'belong', 'land', '1000', 'yrs', 'embrace', 'islam', 'land', 'ownership', 'change', 'chnage', 'religion'] \n",
            "\n",
            "vp is of type <class 'str'> and found in: ['buhari', 'prison', 'potus', 'realdonaldtrump', 'crime', 'humanity', 'past', 'present', 'vp', 'mike_pence', 'foundation', 'president', 'kill', 'christian', 'biafra', 'whitehouse', 'pentagonpressec', 'buhari', 'jihad', 'terrorist', 'save', 'child', 'god'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['modi', 'support', 'terrorist', 'organisation', 'take', 'practical', 'step', 'muslim', 'genocide', 'india', 'target', 'muslims', 'christians', 'dalit', 'turn', 'india', 'radical', 'hindutva', 'terrorist', 'state', 'isis', 'un', 'saudi_gazette', 'bahrain', 'uaegov'] \n",
            "\n",
            "1930 is of type <class 'str'> and found in: ['ironic', 'smart', 'funny', 'idiot', 'know', 'apostate', 'italy', 'scream', 'banners', '1930', 'guénonian', 'muslims', 'italy', 'start', 'julius', 'evola', 'philosopher', 'fascism'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['know', 'supporter', 'jihad', 'activist', 'isis', 'form', 'jantakareporter', 'shri', 'narendramodi', 'messenger', 'love', 'peace', 'humanity', 'underestimate', 'spare', 'jihadi', 'terrorist', 'republic', 'un', 'cnn', 'bjp4india', 'zeenews'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['rights4israel', 'gaza', 'militant', 'islamic', 'jihad', 'aka', 'crime', 'humanity', 'terrorist', 'action', 'ignore', 'forever', 'un', 'israel'] \n",
            "\n",
            "vp is of type <class 'str'> and found in: ['hey', 'vp', 'know', 'mother', 'people', 'lie', 'right', 'secondlady', 'lie', 'think', 'good', 'christian', 'woman'] \n",
            "\n",
            "80 is of type <class 'str'> and found in: ['know', 'pawankhera', 'incindia', 'rule', 'gujrat', '80', 'idiot', 'chintan', 'patel', 'amar', 'chaudhry', 'madhav', 'solanki', 'form', 'k.h.a.m', 'kshatriya', 'harijan', 'adivasi', 'muslim', 'play', 'caste', 'politic', 'finish', 'guj', 'challange', 'congress', 'dead', 'guj', 'amitmalviya'] \n",
            "\n",
            "as is of type <class 'str'> and found in: ['as', 'republican', 'christian', 'woman', 'paulawhite', 'scary', 'opportunist', 'good', 'example', 'right', 'wing', 'extremism', 'exploit', 'christianity', 'bible.its', 'sole', 'purpose', 'radicalize', 'mobilize', 'violent', 'follower', 'criminality', 'terrorism', 'religion'] \n",
            "\n",
            "100 is of type <class 'str'> and found in: ['lovernova', 'muslim', 'sympathy', 'killer', 'regardless', 'religion', 'believe', 'blaim', 'muslims', 'look', 'decade', 'long', 'terorism', 'muslims', 'kill', '100', 'thousand', 'muslims', 'illitrate', 'idiot', 'religion'] \n",
            "\n",
            "ap is of type <class 'str'> and found in: ['kenisaacs1', 'raymondarroyo', 'davidlimbaugh', 'ap', 'yes', 'sit', 'butts', 'let', 'murder', 'enslave', 'ezidis', 'christians'] \n",
            "\n",
            "vp is of type <class 'str'> and found in: ['democraps', 'domestic', 'terrorist', 'leftism', 'cult', 'attack', 'christians', 'jews', 'support', 'islamic', 'terrorism', 'domestic', 'terrorist', 'group', 'openly', 'attack', 'white', 'christians', 'conservative', 'radical', 'islam', 'radical', 'democraps', 'whitehouse', 'presssec', 'vp'] \n",
            "\n",
            "is is of type <class 'str'> and found in: ['toroloconc', 'zainiboy915', 'slave', 'political', 'correctness', 'fall', 'islam', 'sharia', 'is'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['un', 'unhumanrights', 'potus', 'murderer', 'pakistani', 'minority', 'becoz', 'isolate', 'pakistan', 'radical', 'islamic', 'terrorism', 'shame'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['15augustblackday', 'tomorrow', 'test', 'unsecuritycouncil', 'weather', 'stand', 'principle', 'humanity', 'terrorist', 'humanrightsviolation', 'option', 'leave', 'jihad', 'un', 'world', 'responsible', 'consequence'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['isham_alassad', 'un', 'fool', 'photoshop', 'job', 'goat', 'fucker'] \n",
            "\n",
            "vp is of type <class 'str'> and found in: ['ditto', 'support', 'radical', 'islam', 'kill', 'christians', 'world', 'big', 'huge', 'problem', 'saudis', 'arabia', 'include', 'need', 'quit', 'fund', 'radical', 'jihadist', 'realdonaldtrump', 'vp', 'secpompeo', 'patrickbuchanan', 'tuckercarlson', 'asavagenation', 'newswar'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['majorpoonia', 'rubikaliyaquat', 'narendramodi', 'un', 'world', 'need', 'support', 'india', 'great', 'revolutions', 'protect', 'humanity', 'world', 'come', 'jealousy', 'discrimination', 'policy', 'terrorist', 'country', 'pakistan', 'kill', 'humanity', 'jihad'] \n",
            "\n",
            "vp is of type <class 'str'> and found in: ['facebook', 'muslims', 'come', 'country', 'kill', 'people', 'think', 'good', 'thing', 'try', 'warn', 'people', 'ban', 'show', 'total', 'idiot', 'potus', 'aclj', 'jaysekulow', 'jordansekulow', 'judicialwatch', 'judgejeanine', 'brettkavanaugh', 'vp', 'mike_pence'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['un', 'need', 'accountability', 'islamicterrorism', 'peddle', 'platform', 'terrorist', 'nation', 'muslim', 'terrorist', 'jihad', 'hate', 'world', 'humanity', 'jihadwatchrs', 'norbertghofer', 'tarekfatah'] \n",
            "\n",
            "1000 is of type <class 'str'> and found in: ['forefather', 'worst', 'creature', 'history', 'bring', '1000', 'excuse', 'accept', 'fact', 'century', 'old', 'book', 'muslims', 'proudly', 'say', 'demolish', 'kafir', 'butkhana', 'idiot', 'trust'] \n",
            "\n",
            "vp is of type <class 'str'> and found in: ['say', 'vp', 'blatantly', 'lie', 'support', 'radical', 'right', 'fake', 'christian', 'agenda'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['amazonin', 'wtf', 'civilized', 'world', 'war', 'humanity', 'threaten', 'corona', 'roll', 'jihad', 'apologist', 'potential', 'terrorist', 'threaten', 'lead', 'indian', 'medium', 'house', 'republic', 'platform', 'phew', 'interpol_cyber', 'interpol', 'un', 'amitshah', 'pmoindia'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['apologize', 'make', 'realize', 'responsibility', 'hope', 'solve', 'problem', 'muslim', 'world', 'muslim', 'ummah', 'live', 'paradise', 'idiot', 'year', 'found', 'un', '1/4'] \n",
            "\n",
            "un is of type <class 'str'> and found in: ['isham_alassad', 'un', 'say', 'goat', 'fucker'] \n",
            "\n",
            "br is of type <class 'str'> and found in: ['nickiminaj', 'wutkinda', 'rate', 'mkr', 'final', 'december', 'mkr', 'ethelmonster', 'sadsuspenders', 'haha', 'true', 'bellathorne143', 'noticed', 'tweet', 'bully', 'redconversation', 'thing', 'bug', 'people', 'misquote', 'happen', 'think', 'prowlegacy', 'tennesseehoney', 'count', 'idiotð\\x9f\\x92¯', 'windows', 'people', 'free', 'app', 'puppet', 'allow', 'define', 'app', 'install', 'new', 'kylephilip', 'leave', 'legend', 'themichaelowen', 'bully', 'jeremiahfelt', 'twitter', 'email', 'evernote', 'web', 'hearthstone', 'br', 'yes', 'yes', 'money', 'monte_lin', 'twitter', 'bug', 'time', 'surprising', 'bug', 'willing', 'bet', 'replicatable', 'solri', 'huh', 'realize', 'consider', 'heroic', 'funkyrefresh', 'superspacedad', 'target', 'delusion', 'yeah', 'thing', 'behave', 'queenof_pluto', 'yea', 'right', 'lemme', 'stop', 'bully', 'lol', 'person', 'bully', 'jessica', 'talk', 'jessica', 'follow', 'twitter', 'jessicahate', 'send', 'follow', 'request', 'know', 'likely', 'approve', 'know', 'meh', 'maybe', 'wish', 'bloody', 'hell', 'shanley', 'defend', 'point', 'technique', 'apply', 'supporter', 'sure', 'stop', 'insight', 'indie', 'game', 'community', 'tonight', 'peek', 'enlightening', 'immortaldixon', 'holy', 'fuck', 'jist', 'throw', 'michonne', 'kantrn', 'person', 'usually', 'chat', 'ranjib', 'ughhhh', 'aiiane', 'aquarianfool', 'feel', 'fight', 'power', 'see', 'law', 'enforcement', 'social', 'provider', 'respond', 'regal', 'elegant', 'kristinaa_egger', 'kat', 'deserve', 'whack', 'mkr', 'fatnoobtm', 'fucking', 'win', 'anildash', 'omg', 'anil', 'die', 'slain', 'ericahuurnandez', 'makinn', 'fun', 'day', 'license', 'call', 'karma', 'bully', 'lt;3', 'cage_rusty', 'blameonenotall', 'walk', 'home', 'drunk', 'naked', 'pictures', 'ask', 'trou', 'see', 'lot', 'story', 'gamer', 'act', 'horribly', 'dev', 'like', 'info', 'post', 'oapi', 'soon', 'lot', 'question', 'hire', 'glad', 'people', 'excited', 'eaten', 'army', 'food', 'tin', 'look', 'better', 'dick', 'mkr', 'feel', 'bad', 'greyson', 'bully', 'school', 'good', 'thing', 'bully', 'idea', 'work', 'sound', 'wizardry', 't__lourdes', 'aww', 'hugss', 'loool', 'bullying', 'long', 'ago', 'kevin', 'hart', 'stupid', 'hell', 'think', 'man', 'allehegen', 'romney', 'jeb', 'huge', 'mistake', 'cheeseplus', 'obfuscurity', 'wsp', 'sigh', 'docker', 'aws', 'grafana', 'image', 'fuck', 'configure', 'crap', 'mastersparkle', 'genehack', 'yeah', 'stuck', 'fidi', 'different', 'kind', 'area', 'hate', 'video', 'game', '2bithacker', 'hang', 'xixohs', 'get', 'retweet', 'play', 'exodia', 'mage', 'idiot', 'win', 'game', 'rank', 'toc', 'vienna', 'day', 'till', 'deck', 'sâ\\x80', 'whafe', 'gutsful', 'tolerant', 'tolerate', 'bully', 'haranguing', 'darle', 'mras', 'make', 'jabs', 'appearance', 'work', 'provide', 'tool', 'shut', 'lt;3', 'atla', 'pick', 'toy', 'night', 'game', 'coon', 'cat', '5monthsold', 'mainecoone', 'mainecoon_id', 'cute', 'benpobjie', 'whittle', 'short', 'month', 'mkr', 'knife', 'fight', 'car', 'park', 'well', 'jaysonelliot', 'walk', 'distance', 'apartment', 'head', 'ddlovato', 'yooooo', 'song', 'fucking', 'lit', 'repeatttttt', 'davidfeild', 'thelmasleaze', 'the_author', 'statement', 'white', 'woman'] \n",
            "\n",
            "601136508169711 is of type <class 'str'> and found in: ['_glitterjizz', '601136508169711', 'exactly', 'bully'] \n",
            "\n",
            "th is of type <class 'str'> and found in: ['guiltyx', 'jillpantozzi', 'report', 'abuse', 'twitter', 'api', 'privacy', 'hack', 'support', 'freebsdgirl', 'th'] \n",
            "\n",
            "ow is of type <class 'str'> and found in: ['edzitron', 'funranium', 'jonrussell', 'ow', 'literally'] \n",
            "\n",
            "lx is of type <class 'str'> and found in: ['lx', 'hope', 'pedal', 'artisanal', 'perl'] \n",
            "\n",
            "me is of type <class 'str'> and found in: ['verywhiteguy', 'me', 'twitter', 'search', 'look', 'block', 'manually', 'write'] \n",
            "\n",
            "ow is of type <class 'str'> and found in: ['edzitron', 'funranium', 'jonrussell', 'ow', 'general', 'life', 'advice', 'random', 'thing', 'eye', 'good', 'idea'] \n",
            "\n",
            "kf is of type <class 'str'> and found in: ['feminazi', 'actual', 'word', 'denot', 'nasharchy', 'job', 'mean', 'protect', 'people', 'people', 'agree', 'like', 'barely', 'cook', 'entrée', 'mkr', 'kf', 'like', 'community', 'fostering', 'jerks', 'prevent', 'user', 'adopt', 'community', 'code', 'technical', 'problem', 'time', 'jamesgweenwood', 'shock', 'bully', 'lionlioneateat', 'yeah', 'dme', 'screenshot', 'meh', 'let', 'idea', 'egregious', 'case', 'harassment', 'definitely', 'need', 'able', 'respond', 'appropriately', 'toxicity', 'happen', 'nibelsnarfabarf', 'srhbutts', 'grimachu', 'funny', 'assumption', 'work', 'wrong', 'lack', 'self', 'awareness', 'wadhwa', 'right', 'staggering', 'hilarious', 'stopwadhwa2015', 'hahahaha', 'well', 'finale', 'meet', 'mother', 'way', 'trump', 'evil', 'cabal', 'criminal', 'catch', 'lie', 'ass', 'impeach', 'moment', 'soon', 'saintneko', 'love', 'rock', 'band', 'dedicate', 'thread', 'swiftonsecurity', 'srhbutts', 'woman', 'gdc', 'share', 'heartbreaking', 'story', 'gamergate', 'choose', 'respond', 'slainv_fr', 'try', 'mistake', 'hard', 'think', 'president', 'need', 'congressional', 'approval', 'idiot', 'ð\\x9f\\x98\\x84', 'empresssudol', 'freebsdgirl', 'book', 'reject', 'draft', 'kill', 'mockingbird', 'befor', 'lie', 'think', 'try', 'pomegranate', 'cous', 'cous', 'salad', 'sound', 'yum', 'mkr', \"gt;'feminazi\", 'brittany_blade', 'feminazi_front', 'isis', 'claim', 'muslim', 'belief', '1/2', 'harpcast', 'zajice', 'fucking', 'know', 'large', 'spider', 'conlan_webster', 'pretty', 'rad', 'non', 'standard', 'get', 'randi', 'harpers', 'world', 'poor', 'thing', 'people', 'advantage', 'generosity', 'bad', 'offer', 'cheetohs', 'greedy', 'bastard', 'hate', 'person', 'assume', 'shyt', 'come', 'fucking', 'ask', 'clarify', 'lie', 'dumbass', 'ð\\x9f\\x96\\x95ð\\x9f\\x8f¾', 'missclarolyn', 'deconstruct', 'lemon', 'tart', 'item', 'menu', 'mkr', 'selena', 'hate', 'account', 'think', 'nolan', 'cyber', 'bullying', 'tht', 'make', 'lot', 'sense', 'titanfall', 'saltball', 'grapple', 'ronin', 'music', 'iniquity', 'youtube', 'shit', 'sick', 'dude', 'iniqu1ty', 'charlottesph', 'girl', 'bully', 'zachs', 'mom', 'kind', 'rude', 'know', 'better', 'katelinreed', 'big', 'ole', 'mean', 'bully', 'sister', 'dicetechjobs', 'itshella_dom', 'maira_benjamin', 'jenrpetersen', 'squldz', 'bitch', 'wanna', 'penguin', 'bad', 'get', 'read', 'blue', 'apron', 'recipe', 'realize', 'use', 'oxford', 'comma', 'sayymiaa', 'lmao', 'naw', 'like', 'thank', 'nice', 'try', 'bully', 'ppl', 'mcmahoniel', 'lot', 'sarcasm', 'doublespeak', 'group', 'especially', 'appropriation', 'term', 'text', 'filter', 'work', 'atonal440', 'freebsdgirl', 'comment', 'article', 'remind', 'stilget', 'dos', 'mansplainers', 'nintendo', '3ds', 'mcmahoniel', 'unfortunately', 'text', 'filter', 'high', 'risk', 'cath_mayoevan', 'julesrisk63', 'virtual', 'bully', 'make', 'hit', 'lol', 'tthacker15', 'frazer_cobb', 'stop', 'bully', 'thankyou', '.wavinator', 'chobitcoin', 'love', 'spin', 'gallery', 'tablet', 'price', 'right', 'wheel', 'use', 'come', 'matt_cooke86', 'nutrition', 'key', 'closing', '.#aboriginal', 'life', 'expectancy', 'gap', 'croakeyblog', 'check', 'guest', 'post', 'zusterschap', 'blog', 'response', 'blameonenotall', 'thread', 'darxtorm', 'sigh', 'finally', 'meal', 'dracko', 'piping', 'mkr', 'plumsofdoom', 'unfairly', 'antagonize', 'bully', 'barton', 'want', 'team', 'bastad', 'lol', 'a_man_in_black', 'freebsdgirl', 'person', 'twitter', 'classify', 'wielding', 'mob', 'indirect', 'bullying', 'sure', 'kelly', 'attack', 'brie', 'attack', 'attack', 'unprovoked', 'big', 'bully', 'diva', 'destruction', 'leyna76', 'case', 'evidently', 'yes', 'stibbon', 'use', 'slavenout', 'bilic', 'fucking', 'hate', 'watch', 'lose', 'swansea', 'relegation', 'imminent', 'm_d3mps3y', 'debt', 'fucking', 'hole', 'pathetic', 'ass', 'boyfriend', 'putâ\\x80'] \n",
            "\n",
            "-e is of type <class 'str'> and found in: ['xor', 'meg', 'ryan', 'indie', 'bookseller', 'tom', 'hanks', 'amazon', 'zillionaire', 'keep', '-e', 'twitter', 'come', 'soon', 'male'] \n",
            "\n",
            "100 is of type <class 'str'> and found in: ['shanley', 'create', 'isolation', 'need', 'long', 'term', 'campaign', '100', 'threatening', 'abusive', 'harass', 'message', 'hour'] \n",
            "\n",
            "de is of type <class 'str'> and found in: ['zeynep', 'ugh', 'twitter', 'well', 'report', 'spam', 'block', 'single', 'menu', 'recombine', 'page', 'de'] \n",
            "\n",
            "a is of type <class 'str'> and found in: ['sooo', 'callmehyours', 'jus', 'say', 'mean', 'bully', 'call', 'a', 'hole', 'mean'] \n",
            "\n",
            "in is of type <class 'str'> and found in: ['tumblr', 'girl', 'vsco', 'girl', 'ready', 'twitt', 'girl', 'qualification', 'avi', 'mirror', 'pic', 'underwear', 'lingerie', 'bikini', 'emoji', 'username', 'def', 'bully', 'high', 'school', 'premium', 'snapchat', 'venmo', 'paypal', 'in', 'bio', 'relatable'] \n",
            "\n",
            "my is of type <class 'str'> and found in: ['my', 'middle', 'school', 'bully', 'deny', 'bully', 'belittling', 'trauma', 'prove', 'good', 'person', 'way', 'racist'] \n",
            "\n",
            "‘s is of type <class 'str'> and found in: ['people', 'tiktok', 'literally', 'super', 'nice', 'see', 'video', 'say', 'share', 'ready', 'school', 'comment', 'prepare', 'hate', 'school', 'bully', 'bully', 'person', 'ask', '‘s'] \n",
            "\n",
            "me is of type <class 'str'> and found in: ['sorry', 'dream', 'give', 'high', 'school', 'bully', 'vibe', 'bully', 'nerd', 'homo', 'people', 'secretly', 'gay', 'me'] \n",
            "\n",
            "my is of type <class 'str'> and found in: ['my', 'secondary', 'school', 'bully', 'praise', 'embrace', 'feminity', 'wear', 'skirt', 'lgbt', 'ally', 'man', 'bully', 'friend', 'queer'] \n",
            "\n",
            "go is of type <class 'str'> and found in: ['matter', 'hand', 'daughter', 'get', 'bully', 'try', 'school', 'community', 'police', 'go', 'girl', 'door', 'say', 'dad', 'everytime', 'happen', 'come', 'stop', 'day', 'hate', 'bully'] \n",
            "\n",
            "at is of type <class 'str'> and found in: ['grandson', 'angry', 'gender', 'free', 'crap', 'primary', 'at', 'high', 'school', 't.he', 'old', 'asd', 'bully', 'girl', 'sen', 'base', 'step', 'teacher', 'busy', 'phone', 'play', 'game', 'wee', 'lass', 'run', 'loo', 'unisex'] \n",
            "\n",
            "ap is of type <class 'str'> and found in: ['pathetic', 'parenting', 'portland', 'porker', 'child', 'abuse', 'stem', 'public', 'school', 'education', 'san', 'moral', 'compass', 'u.s', 'good', 'reallocated', 'fund', 'send', 'low', 'live', 'bully', 'matter', 'rock', 'mrandyngo', 'abc', 'ap', 'cnn', 'oregonian'] \n",
            "\n",
            "it is of type <class 'str'> and found in: ['trump', 'bully', 'public', 'school', 'bec', 'know', 'funding', 'totally', 'dependent', 'government', 'threat', 'defund', 'school', 'open', 'apply', 'private', 'school', 'exclude', 'threat', 'son', 'school', 'opening', 'elitism', 'it', 'best'] \n",
            "\n",
            "30 is of type <class 'str'> and found in: ['thing', 'bully', 'school', 'young', 'dumb', 'uneducated', 'scary', 'see', 'people', '30', 'act', 'kid', 'miniscule', 'stuff', 'easy', 'simple', 'twitter', 'search', 'grow', 'ass', 'adult', 'send', 'death', 'threat', 'abuse', 'wtf'] \n",
            "\n",
            "501 is of type <class 'str'> and found in: ['thin', 'thighe', 'girl', 'tease', 'high', 'school', 'muscle', 'run', 'track', 'bully', 'wear', 'man', '501', 'guess', 'carry', 'water', 'bottle', 'knee', 'rollin', 'wheelchair', 'break', 'ankle', 'bitch', 'teamthickthigh'] \n",
            "\n",
            "11 is of type <class 'str'> and found in: ['bunch', 'high', 'school', 'bully', '11'] \n",
            "\n",
            "a is of type <class 'str'> and found in: ['bully', 'high', 'school', 'girl', 'fat', 'karma', 'come', 'bite', 'a'] \n",
            "\n",
            "1950 is of type <class 'str'> and found in: ['rmbr', 'kid', 'exposing', 'racism', 'book', 'say', 'colored', 'people', 'live', '1950'] \n",
            "\n",
            "1970 is of type <class 'str'> and found in: ['thank', 'sense', 'black', 'people', 'call', 'colored', 'negro', '1970', 'call', 'people', 'color', 'foh', 'white', 'person', 'definitely', 'come', 'term', 'give', 'right', 'define'] \n",
            "\n",
            "100 is of type <class 'str'> and found in: ['makgowa', 'say', 'nigger', 'islam', 'threaten', 'american', 'freedom', \"we've\", 'goat', 'herder', 'live', 'cave', 'afghanistan', 'want', 'kill', 'join', 'war', 'nigger', 'join', '100', 'thousand', \"ma'nigga\", 'dumb', 'fuck', 'sie'] \n",
            "\n",
            "1940 is of type <class 'str'> and found in: ['clarke', 'niger', 'sheriff', 'think', 'give', 'fuck', 'dumb', '1940', 'cowboy', 'hat', 'wear', 'watch', 'country', 'music', 'video'] \n",
            "\n",
            "1940 is of type <class 'str'> and found in: ['fund', 'create', 'hbcus', 'name', '1940', 'historical', 'reason', 'call', 'negro', 'fund', 'old', 'white', 'man', 'dismiss', 'educate', 'black', 'man', 'argument', 'call', 'negro', 'racist', 'difference'] \n",
            "\n",
            "60 is of type <class 'str'> and found in: ['nah', 'racism', 'black', 'people', 'colored', 'people', 'think', 'living', '60', 'ignorance'] \n",
            "\n",
            "1900 is of type <class 'str'> and found in: ['racism', 'manifest', 'differently', 'society', '1900', 'mean', 'racism', 'dead', 'stop', 'racist', 'blaming', 'folk', 'offend', 'white', 'folks-', 'stop', 'compare', 'colored', 'folk', 'experience', 'assume', 'pleasant', 'experience'] \n",
            "\n",
            "is is of type <class 'str'> and found in: ['cruelsummerlive', 'is', 'gt;&gt;&gt;&gt;&gt', 'fave', 'taylor', 'stan', 'frjend'] \n",
            "\n",
            "26 is of type <class 'str'> and found in: ['dear', 'faisal', 'iqbal', 'hasan', 'raza', 'average', 'class', 'cricket', 'era', 'average', 'class', 'play', 'test', 'match', '26', 'hasan', 'play', 'test', 'lucky', 'cricketer'] \n",
            "\n",
            "de is of type <class 'str'> and found in: ['racism', 'select', 'white', 'clothe', 'wash', 'black', 'colored', 'one', 'racism', 'wash', 'de', 'time'] \n",
            "\n",
            "me is of type <class 'str'> and found in: ['gay', 'trendsettashane', 'me', 'shut', 'fuck', 'dumb', 'nigger', 'give', 'fuck', 'think', 'tryin', 'feel', 'good', 'iownyou'] \n",
            "\n",
            "947 is of type <class 'str'> and found in: ['947', '947olympics', 'people', 'diss', 'come', 'happen', 'past', 'black', 'white', 'speak', 'feel', 'bear', 'color', 'make', 'person', 'eminem', 'teameminem'] \n",
            "\n",
            "me is of type <class 'str'> and found in: ['yea', 'fuck', 'bitch', 'ass', 'nigger', 'lil', 'tunechi', 'nuisance', 'stupid', 'dumb', 'like', '~lil', 'wayne', 'look', 'me'] \n",
            "\n",
            "me is of type <class 'str'> and found in: ['human', 'color', 'ancestor', 'piece', 'bread', 'toast', 'look', 'determine', 'hot', 'country', 'bread', 'sun', 'toaster', 'racism', 'me'] \n",
            "\n",
            "ur is of type <class 'str'> and found in: ['possible', 'dance', 'song', 'ur', 'future', 'wedding', 'option', 'estoy', 'perdido', 'los', 'tre'] \n",
            "\n",
            "1970 is of type <class 'str'> and found in: ['old', 'man', 'stick', 'past', 'recommend', '1970', 'light', 'attic', 'spooky', 'old', 'house', 'variety', 'say', 'try', 'burn', 'offering', 'star', 'oliver', 'reed', 'karen', 'black', 'bette', 'davis'] \n",
            "\n",
            "me is of type <class 'str'> and found in: ['me', 'ignorant', 'distasteful', 'offensive', 'nigger', 'call', 'dumb', 'ass', 'cracka', 'fuck', 'cousin', 'wrong', 'get', 'attack', 'choose', 'ignorant'] \n",
            "\n",
            "it is of type <class 'str'> and found in: ['ignorance', 'it', 'fine', 'kristian_elise', '1st', 'tayyoung', 'fuck', 'obama', 'dumb', 'ass', 'nigger'] \n",
            "\n",
            "There are only 83 lemma tokens left, that do not comply to our length rule\n"
          ]
        }
      ],
      "source": [
        "# check how many tokens are left that do not comply to our length rule (if the number is small we keep them)\n",
        "counter_01=0\n",
        "for i in df_prep['lemma_tokens']:\n",
        "    for k in i: \n",
        "        if (k.isnumeric()) or (len(k)<3) and (k  not in list_useful_2char_word):\n",
        "            counter_01 +=1\n",
        "            a=k\n",
        "            print(k,\"is of type\",type(k),\"and found in:\",i,\"\\n\")\n",
        "print(\"There are only\",counter_01,\"lemma tokens left, that do not comply to our length rule\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:50.620509Z",
          "start_time": "2020-12-23T07:57:50.093215Z"
        },
        "id": "24AetiQ1H2W3"
      },
      "outputs": [],
      "source": [
        "# Create a gensim dictionary out of ALL lemma tokens\n",
        "gensim_dict = Dictionary(df_prep['lemma_tokens'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:50.633030Z",
          "start_time": "2020-12-23T07:57:50.623035Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqyaYlqhH2W3",
        "outputId": "93bd5a0c-ee9c-499b-ee02-8aaa954721b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dict was created by lookint at 47692 tweets \n",
            "It contains 53255 different lemma tokens \n",
            "\n",
            "Here are the first 60 tweets:\n",
            "crapilicious, food, katandandre, mkr, word, aussietv, etc, imacelebrityau, neighbour, studio10, sunrise, theblock, today, white, wonderlandten, classy, cupcake, red, velvet, whore, xochitlsuckkks, angry, concerned, dude, head, jason_gio, meh, thank, twitter, account, isis, islam, kurdish, lie, pretend, rudhoeenglish, bad, existence, god, good, indifferent, prove, quickieleak, raja5aab, test, weird, yes, bukan, bully, itu, jauh, kaya, neraka, sekolah, tempat, bite, butt, hope, karma, kat, "
          ]
        }
      ],
      "source": [
        "def some_stats_on_gen_dict(gd):\n",
        "    n=60\n",
        "    #This module implements the concept of a Dictionary \n",
        "    # – a mapping between words and their integer ids.\n",
        "    print(\"The dict was created by lookint at\",gd.num_docs,\n",
        "          \"tweets \\nIt contains\",len(gd),\n",
        "          \"different lemma tokens \\n\\nHere are the first\",n,\"tweets:\")\n",
        "    for i in range(n):\n",
        "        print (gd.get(i),end=\", \")\n",
        "\n",
        "some_stats_on_gen_dict(gensim_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:51.087542Z",
          "start_time": "2020-12-23T07:57:50.636206Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDILyRCsH2W3",
        "outputId": "e89d915c-7ebd-44f0-a910-896641f8b934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "# create a corpus with all final lemma tokens for each cyberbullying_type\n",
        "lemma_tokens_all = []\n",
        "for sublist in df_prep.cyberbullying_type.unique():\n",
        "    exec(f'lemma_tokens_all.append(list_{sublist})')\n",
        "\n",
        "print(len(lemma_tokens_all)) \n",
        "\n",
        "corpus_all = [gensim_dict.doc2bow(d) for d in lemma_tokens_all]\n",
        "\n",
        "# create work frequency list for all cyberbullying_type\n",
        "word_frequencies = [[(gensim_dict[id], frequence) for id, \n",
        "                     frequence in couple] for couple in corpus_all]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:51.108489Z",
          "start_time": "2020-12-23T07:57:51.092091Z"
        },
        "id": "GrdOq30tH2W4"
      },
      "outputs": [],
      "source": [
        "word_frequencies_sorted = []\n",
        "for tuples in word_frequencies:\n",
        "    word_frequencies_sorted.append(sorted(tuples, key=lambda tup: tup[1], reverse = True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:51.186204Z",
          "start_time": "2020-12-23T07:57:51.111746Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZMelojZH2W4",
        "outputId": "594805e0-8580-458f-c043-f5be7b75953d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[65, 7, 21, 59, 3, 5]\n"
          ]
        }
      ],
      "source": [
        "k = []\n",
        "for word_frequencies in word_frequencies_sorted:\n",
        "    cumu_count = [0]\n",
        "    counter = 1\n",
        "    for i in word_frequencies:\n",
        "        cumu_count.append(cumu_count[counter-1]+i[1])\n",
        "        counter+=1\n",
        "\n",
        "    num_element = 0\n",
        "    \n",
        "    for i in cumu_count:\n",
        "        if i > 0.2 * max(cumu_count):\n",
        "            k.append(num_element)\n",
        "            break\n",
        "        num_element += 1\n",
        "\n",
        "print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:51.193956Z",
          "start_time": "2020-12-23T07:57:51.188945Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u-okbq4H2W4",
        "outputId": "fd1702df-e4a0-4901-daf7-394f53f108f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words across all cyberbullying_type: 160\n",
            "Number of unique words across all cyberbullying_type: 101\n"
          ]
        }
      ],
      "source": [
        "c_b = 0\n",
        "feature_list = []\n",
        "for j in k:\n",
        "    for i in range(0,j):\n",
        "        feature_list.append(word_frequencies_sorted[c_b][i][0])\n",
        "    c_b +=1\n",
        "    \n",
        "\n",
        "unique_feature_list = list(set(feature_list))\n",
        "print(\"Number of words across all cyberbullying_type:\",len(feature_list))\n",
        "print(\"Number of unique words across all cyberbullying_type:\",len(unique_feature_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:51.203539Z",
          "start_time": "2020-12-23T07:57:51.196748Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PApUl0e_H2W4",
        "outputId": "4537de01-a6b4-4365-dcf6-e2d5ca632266"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['guy',\n",
              " 'team',\n",
              " 'muslims',\n",
              " 'kat',\n",
              " 'round',\n",
              " 'school',\n",
              " 'nigger',\n",
              " 'female',\n",
              " 'see',\n",
              " 'mean',\n",
              " 'daesh',\n",
              " 'make',\n",
              " 'stupid',\n",
              " 'bullying',\n",
              " 'cook',\n",
              " 'lot',\n",
              " 'right',\n",
              " 'try',\n",
              " 'rape',\n",
              " 'start',\n",
              " 'people',\n",
              " 'twitter',\n",
              " 'freebsdgirl',\n",
              " 'woman',\n",
              " 'man',\n",
              " 'shit',\n",
              " 'joke',\n",
              " 'year',\n",
              " 'love',\n",
              " 'get',\n",
              " 'know',\n",
              " 'need',\n",
              " 'feel',\n",
              " 'dumb',\n",
              " 'colin',\n",
              " 'home',\n",
              " 'use',\n",
              " 'que',\n",
              " 'tell',\n",
              " 'mkr',\n",
              " 'thing',\n",
              " 'gay',\n",
              " 'islamic',\n",
              " 'terrorism',\n",
              " 'kill',\n",
              " 'terrorist',\n",
              " 'time',\n",
              " 'country',\n",
              " 'isis',\n",
              " 'fucking',\n",
              " 'work',\n",
              " 'ass',\n",
              " 'quran',\n",
              " 'new',\n",
              " 'harassment',\n",
              " 'hope',\n",
              " 'class',\n",
              " 'watch',\n",
              " 'bully',\n",
              " 'yeah',\n",
              " 'think',\n",
              " 'black',\n",
              " 'talk',\n",
              " 'christian',\n",
              " 'fuck',\n",
              " 'hate',\n",
              " 'stop',\n",
              " 'way',\n",
              " 'yes',\n",
              " 'feminazi',\n",
              " 'pretty',\n",
              " 'gamergate',\n",
              " 'religion',\n",
              " 'tweet',\n",
              " 'say',\n",
              " 'person',\n",
              " 'idiot',\n",
              " 'bad',\n",
              " 'thank',\n",
              " 'read',\n",
              " 'look',\n",
              " 'sure',\n",
              " 'blameonenotall',\n",
              " 'islam',\n",
              " 'day',\n",
              " 'high',\n",
              " 'happen',\n",
              " 'today',\n",
              " 'good',\n",
              " 'girl',\n",
              " 'call',\n",
              " 'andre',\n",
              " 'radical',\n",
              " 'mkr2015',\n",
              " 'help',\n",
              " 'muslim',\n",
              " 'support',\n",
              " 'bitch',\n",
              " 'come',\n",
              " 'dude',\n",
              " 'lol']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "unique_feature_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:51.216400Z",
          "start_time": "2020-12-23T07:57:51.207782Z"
        },
        "id": "4iicoeXdH2W5"
      },
      "outputs": [],
      "source": [
        "dictOfWords = dict.fromkeys(unique_feature_list)\n",
        "key_names = []\n",
        "counter = 0\n",
        "for key in dictOfWords:\n",
        "    key_names.append(key)\n",
        "    dictOfWords[key] = counter \n",
        "    counter+=1\n",
        "\n",
        "lemma_list = list(df_prep.lemma_tokens)\n",
        "\n",
        "len(lemma_list)\n",
        "key_names.append('Not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:51.403662Z",
          "start_time": "2020-12-23T07:57:51.219791Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY8aXHgRH2W5",
        "outputId": "9830535f-7fc8-41a6-db78-770b596fb741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102\n",
            "47692\n"
          ]
        }
      ],
      "source": [
        "feature_matrix = []\n",
        "feature_size = len(unique_feature_list) + 1\n",
        "\n",
        "lemma_list\n",
        "\n",
        "for array in lemma_list:\n",
        "    temp = [0]*feature_size\n",
        "    for k in array:\n",
        "        index = dictOfWords.get(k,feature_size-1)\n",
        "        temp[index] = 1\n",
        "    \n",
        "    feature_matrix.append(temp)\n",
        "\n",
        "print(feature_size)\n",
        "print(len(feature_matrix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:52.438809Z",
          "start_time": "2020-12-23T07:57:51.406236Z"
        },
        "id": "2DZVukqXH2W5"
      },
      "outputs": [],
      "source": [
        "feature_df = pd.DataFrame(feature_matrix, index = df_prep.index, columns = key_names)\n",
        "feature_df = feature_df.drop(['Not found'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:52.455720Z",
          "start_time": "2020-12-23T07:57:52.440892Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "yKDZc-7SH2W5",
        "outputId": "f05b7f49-7946-4112-e00c-decb523ab090"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3b18abc5-3a26-483e-932e-061daf02ca19\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>guy</th>\n",
              "      <th>team</th>\n",
              "      <th>muslims</th>\n",
              "      <th>kat</th>\n",
              "      <th>round</th>\n",
              "      <th>school</th>\n",
              "      <th>nigger</th>\n",
              "      <th>female</th>\n",
              "      <th>see</th>\n",
              "      <th>mean</th>\n",
              "      <th>...</th>\n",
              "      <th>andre</th>\n",
              "      <th>radical</th>\n",
              "      <th>mkr2015</th>\n",
              "      <th>help</th>\n",
              "      <th>muslim</th>\n",
              "      <th>support</th>\n",
              "      <th>bitch</th>\n",
              "      <th>come</th>\n",
              "      <th>dude</th>\n",
              "      <th>lol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 101 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b18abc5-3a26-483e-932e-061daf02ca19')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b18abc5-3a26-483e-932e-061daf02ca19 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b18abc5-3a26-483e-932e-061daf02ca19');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   guy  team  muslims  kat  round  school  nigger  female  see  mean  ...  \\\n",
              "0  0    0     0        0    0      0       0       0       0    0     ...   \n",
              "1  0    0     0        0    0      0       0       0       0    0     ...   \n",
              "\n",
              "   andre  radical  mkr2015  help  muslim  support  bitch  come  dude  lol  \n",
              "0  0      0        0        0     0       0        0      0     0     0    \n",
              "1  0      0        0        0     0       0        0      0     0     0    \n",
              "\n",
              "[2 rows x 101 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "feature_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:52.489405Z",
          "start_time": "2020-12-23T07:57:52.457934Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "JKFMxKE4H2W5",
        "outputId": "625f2e75-a5cd-4634-c313-de986ea6236f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0db7bd8b-1d5e-4cb2-a413-259c07d3c6ac\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tokens_back_to_text</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>lemmas_back_to_text</th>\n",
              "      <th>lemma_tokens</th>\n",
              "      <th>guy</th>\n",
              "      <th>team</th>\n",
              "      <th>muslims</th>\n",
              "      <th>...</th>\n",
              "      <th>andre</th>\n",
              "      <th>radical</th>\n",
              "      <th>mkr2015</th>\n",
              "      <th>help</th>\n",
              "      <th>muslim</th>\n",
              "      <th>support</th>\n",
              "      <th>bitch</th>\n",
              "      <th>come</th>\n",
              "      <th>dude</th>\n",
              "      <th>lol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was crapilicious! #mkr</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>[words, #katandandre,, food, crapilicious!, #mkr]</td>\n",
              "      <td>words #katandandre, food crapilicious! #mkr</td>\n",
              "      <td>[word, katandandre, food, crapilicious, mkr]</td>\n",
              "      <td>word katandandre food crapilicious mkr</td>\n",
              "      <td>[word, katandandre, food, crapilicious, mkr]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>[#aussietv, white?, #mkr, #theblock, #imacelebrityau, #today, #sunrise, #studio10, #neighbours, #wonderlandten, #etc]</td>\n",
              "      <td>#aussietv white? #mkr #theblock #imacelebrityau #today #sunrise #studio10 #neighbours #wonderlandten #etc</td>\n",
              "      <td>[aussietv, white, mkr, theblock, imacelebrityau, today, sunrise, studio10, neighbour, wonderlandten, etc]</td>\n",
              "      <td>aussietv white mkr theblock imacelebrityau today sunrise studio10 neighbour wonderlandten etc</td>\n",
              "      <td>[aussietv, white, mkr, theblock, imacelebrityau, today, sunrise, studio10, neighbour, wonderlandten, etc]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 108 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0db7bd8b-1d5e-4cb2-a413-259c07d3c6ac')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0db7bd8b-1d5e-4cb2-a413-259c07d3c6ac button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0db7bd8b-1d5e-4cb2-a413-259c07d3c6ac');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                            tweet_text  \\\n",
              "0  In other words #katandandre, your food was crapilicious! #mkr                                                         \n",
              "1  Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc   \n",
              "\n",
              "  cyberbullying_type  \\\n",
              "0  not_cyberbullying   \n",
              "1  not_cyberbullying   \n",
              "\n",
              "                                                                                                                  tokens  \\\n",
              "0  [words, #katandandre,, food, crapilicious!, #mkr]                                                                       \n",
              "1  [#aussietv, white?, #mkr, #theblock, #imacelebrityau, #today, #sunrise, #studio10, #neighbours, #wonderlandten, #etc]   \n",
              "\n",
              "                                                                                         tokens_back_to_text  \\\n",
              "0  words #katandandre, food crapilicious! #mkr                                                                 \n",
              "1  #aussietv white? #mkr #theblock #imacelebrityau #today #sunrise #studio10 #neighbours #wonderlandten #etc   \n",
              "\n",
              "                                                                                                      lemmas  \\\n",
              "0  [word, katandandre, food, crapilicious, mkr]                                                                \n",
              "1  [aussietv, white, mkr, theblock, imacelebrityau, today, sunrise, studio10, neighbour, wonderlandten, etc]   \n",
              "\n",
              "                                                                             lemmas_back_to_text  \\\n",
              "0  word katandandre food crapilicious mkr                                                          \n",
              "1  aussietv white mkr theblock imacelebrityau today sunrise studio10 neighbour wonderlandten etc   \n",
              "\n",
              "                                                                                                lemma_tokens  \\\n",
              "0  [word, katandandre, food, crapilicious, mkr]                                                                \n",
              "1  [aussietv, white, mkr, theblock, imacelebrityau, today, sunrise, studio10, neighbour, wonderlandten, etc]   \n",
              "\n",
              "   guy  team  muslims  ...  andre  radical  mkr2015  help  muslim  support  \\\n",
              "0  0    0     0        ...  0      0        0        0     0       0         \n",
              "1  0    0     0        ...  0      0        0        0     0       0         \n",
              "\n",
              "   bitch  come  dude  lol  \n",
              "0  0      0     0     0    \n",
              "1  0      0     0     0    \n",
              "\n",
              "[2 rows x 108 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "combined_features = pd.concat([df_prep,feature_df],axis = 1)\n",
        "combined_features.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:52.499819Z",
          "start_time": "2020-12-23T07:57:52.491432Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Df8L-CH2W5",
        "outputId": "1d9bf414-42fe-425d-e7fe-45926a76f044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['halalflaws', 'greenlinerzjm', 'biebervalue', 'problem', 'force', 'wear', 'microbrain']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Series([], Name: 899, dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# test whether the feature creation works\n",
        "## entry number to test\n",
        "test = 899\n",
        "print(combined_features.iloc[test][6])\n",
        "feature_df.iloc[test][feature_df.iloc[test]>0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:52.505450Z",
          "start_time": "2020-12-23T07:57:52.502286Z"
        },
        "id": "a5_VA7F1H2W5"
      },
      "outputs": [],
      "source": [
        "assert len(combined_features) == len(df_import) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-23T07:57:53.890963Z",
          "start_time": "2020-12-23T07:57:52.507318Z"
        },
        "id": "3dGW8yTRH2W6"
      },
      "outputs": [],
      "source": [
        "## save file from data frame to csv\n",
        "combined_features.to_csv(\"bag of words_02.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "pre_processing_02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}