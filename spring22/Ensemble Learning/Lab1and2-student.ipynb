{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"nav_menu":{"height":"279px","width":"309px"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"toc_cell":false,"toc_position":{},"toc_section_display":"block","toc_window_display":false},"colab":{"name":"Lab1and2-student.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ISQmA1UdDS_O"},"source":["<center><img src='https://netacad.centralesupelec.fr/img/cs.jpg' width=200></center>\n","\n","<h6><center>M.Sc. DSBA & AI</center></h6>\n","<h3><center>Ensemble learning from theory to practice</center></h3>\n","\n","\n","<h1>\n","<hr style=\" border:none; height:3px;\">\n","<center>Lab 1 & 2: Decision Trees & Bagging</center>\n","<hr style=\" border:none; height:3px;\">\n","</h1>\n","\n","__Teacher__: Myriam Tami"]},{"cell_type":"markdown","metadata":{"id":"vD9RZ84RDS_Q"},"source":["# Introduction\n"]},{"cell_type":"markdown","metadata":{"id":"xQ_6ta8mDS_S"},"source":["This lab consists of two parts. In the first part, we're going to experiment decision trees approach on both simulated and real data; in the second part, we're going to perform random forest. \n","\n","This lab is free from some of the examples shown in the `Scikit-Learn` [global documentation](https://scikit-learn.org/stable/modules/tree.html) and documentation on [random forests for the regression case](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [random forests for classification problems](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n","\n","\n","The objectives of this practical session is tofold: \n"," + use decision trees for classification and regression problems in python (`sklearn` module);\n"," + use **bagging** and **random forests** approaches for classification and regression problems in python (`sklearn` module).\n"," \n","Useful reference links:\n","+ [`Scikit-learn` website](https://scikit-learn.org/stable/index.html)\n","+ [`NumPy` documentation](https://docs.scipy.org/doc/numpy/user/index.html)\n","+ [`SciPy` documentation](https://docs.scipy.org/doc/scipy/reference/)\n","+ [`MatPlotLib` documentation](https://matplotlib.org/)\n","+ [Python website](https://www.python.org/)\n"]},{"cell_type":"markdown","metadata":{"id":"xLB3tL6GDS_U"},"source":["# 1. Decision trees"]},{"cell_type":"markdown","metadata":{"id":"oNN-NlZcDS_W"},"source":["Decision trees are learning methods used for classification and regression prediction problems. The goal is to create a model that predicts the values of the output variable, based on a set of sequences of decision rules (formaly a set of split points) deduced from the training data. The tree therefore approximates the output by a succession of if-then-else rules (leading to terminal nodes). This paradigm works well for both categorical and numerical data. The more complex the generated tree, the more the model \"explains\"/fits the training data but also the more the chances of overfitting.\n","\n","**Advantages of decision trees**\n","+ They are simple to understand and visualize\n","+ They require little data preparation (standardization, etc.)\n","+ The cost of using trees is logarithmic\n","+ They are able to manage categorical and numerical data\n","+ They are able to deal with multi-class problems\n","+ White box model: the result is easy to conceptualize and visualize\n","\n","**Disadvantages of decision trees**\n","\n","+ Overfitting: sometimes the trees generated are too complex and generalize poorly. Choosing the right values for the maximum depth (`max_depth`) and minimum number of examples per leaf (`min_samples_leaf`) parameters avoids this problem.\n","+ Sometimes the trees generated are not balanced. It is therefore recommended to adjust the database before construction, to avoid one class largely dominating the others (in terms of the number of training examples).\n","\n","#### 1.0. Python dependencies and setup\n","\n","You will need to install some packages allowing to visualize some results of this lab. Run the following command.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"inlxz19pDS_X"},"source":["pip install graphviz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3h4AvGg_DS_g"},"source":["pip install pydot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWblqN_9DS_m"},"source":["pip install pydotplus "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTKbDc3ZDS_q"},"source":["# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","\n","import numpy as np\n","import pandas as pd\n","\n","\n","# Matplotlib is used to plot graphs\n","%matplotlib inline \n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","# Style options for plots.\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rNKMCC0WDS_w"},"source":["#### 1.1. Classification Decision Trees\n","\n","In Scikit-learn, the class [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) allows to make a multi-class classification on a database. This class needs as input a matrix X of size `[n_samples, n_features]` containing the data and a vector Y of size `[n_samples]` with the values of the target (output) class.\n","\n","We start by importing the right modules and building the tree object:\n"]},{"cell_type":"code","metadata":{"id":"a0qJiizDDS_x"},"source":["from sklearn import tree\n","clf = tree.DecisionTreeClassifier()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LSeq2ZtaDS_2"},"source":["The data:"]},{"cell_type":"code","metadata":{"id":"FFwgGxGRDS_3"},"source":["X = [[0, 0], [1, 1]]\n","Y = [0, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6uvQMWHKDS_8"},"source":["Model construction (model will be nammed clf):\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"QquAhjTqDS_8"},"source":["clf = clf.fit(X, Y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-WSJoBEDTAA"},"source":["Prediction on new samples:\n"]},{"cell_type":"code","metadata":{"id":"Lpxe2ENJDTAB"},"source":["clf.predict([[2., 2.]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lieg0H4QDTAF"},"source":["We can also predict the probability of each class for a sample (which is calculated as the fraction of training data in each terminal node):"]},{"cell_type":"code","metadata":{"id":"q4CxNU9BDTAH"},"source":["clf.predict_proba([[2., 2.]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fXbLZZdvDTAK"},"source":["#### Iris data classification\n","\n","DecisionTreeClassifier is able to handle classification problems with several classes (for example, with labels 0, 1,… K-1). In this example we will work with the [Iris](https://archive.ics.uci.edu/ml/datasets/Iris) database, easily accessible in sklearn. This database contains 3 classes of 150 observations, each class referring to a variety of iris (plant). One of the classes is linearly separable from the other two, but the other two are not separable one from the other. The variable to predict is the variety of iris.\n","\n","Attributes:\n","+ sepal length in cm \n","+ sepal width in cm \n","+ petal length in cm \n","+ petal width in cm \n","+ class: Iris Setosa, Iris Versicolour, Iris Virginica\n","\n","One sample from the database: 4.9,3.6,1.4,0.1, “Iris-setosa”"]},{"cell_type":"code","metadata":{"id":"w4Y8VjbfDTAL"},"source":["from sklearn.datasets import load_iris\n","from sklearn import tree\n","iris = load_iris()\n","clf = tree.DecisionTreeClassifier(max_depth=5)\n","clf = clf.fit(iris.data, iris.target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2XdaCknDTAP"},"source":["Once learning is complete we can view the tree created using the `graphviz` tool. To display it, we create a function `create_tree_graph_png()`:"]},{"cell_type":"code","metadata":{"id":"S8TyE2ChDTAP"},"source":["from sklearn.tree import export_graphviz\n","from IPython.display import Image \n","import pydotplus\n","\n","# Function to create a tree diagram\n","def create_tree_graph_png(tree, feature_names):\n","    tree_str = export_graphviz(tree, feature_names=feature_names, filled=True, out_file=None)\n","    graph = pydotplus.graph_from_dot_data(tree_str)  \n","    graph.write_png('tree.png')\n","    return Image(graph.create_png())\n","\n","# Display the tree\n","create_tree_graph_png(clf, feature_names= iris.feature_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHYooYK6DTAV"},"source":["The generated image looks like this: \n","\n","<figure>\n","<center><img src='https://github.com/myriamtami/ML-Labs/blob/master/Figures/Iris_Tree.png?raw=true' width=1000></center>\n","</figure>\n"," \n"]},{"cell_type":"markdown","metadata":{"id":"88bd_ImYDTAV"},"source":["After its construction, the model can be used for prediction:"]},{"cell_type":"code","metadata":{"id":"yJOsJcSUDTAW"},"source":["print(clf.predict(iris.data[:1, :]))\n","print(clf.predict_proba(iris.data[:1, :]))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ipo5AaoY-bNC"},"source":["iris.data[:10,]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwrqycqiDTAa"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 1.** Change the parameter values `max depth` and `min_samples_leaf`. What do you observe?\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"tvH43MXIDTAb"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 2.** Split randomly the database into training/testing sets (70% training, 30% testing) and calculate the rate of misclassified items on the test set. Vary the values of the `max_depth` and `min_samples_leaf` parameters to see their impact on this score.\n","\n","</div>\n"]},{"cell_type":"code","metadata":{"id":"M-HJp_7ODTAc","collapsed":true},"source":["# Write the code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WGQVImvEDTAf"},"source":["To search for good values for the parameters you can use cross validation with GridSearchCV:"]},{"cell_type":"code","metadata":{"id":"prS7Qd0xDTAg"},"source":["from sklearn import model_selection\n","\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target,\n","    test_size=0.30, random_state=0)\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","pgrid = {\"max_depth\": [1, 2, 3, 4, 5, 6, 7],\n","      \"min_samples_split\": [2, 3, 5, 10, 15, 20]}\n","\n","grid_search = GridSearchCV(tree.DecisionTreeClassifier(), param_grid=pgrid, cv=10)\n","grid_search.fit(X_train, y_train)\n","print(grid_search.best_params_)\n","print(grid_search.best_estimator_.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ns_umVzrDTAj"},"source":["**Display of the decision surface**\n","\n","For a pair of attributes we can visualize the decision surface in 2 dimensions. First we discretize the two-dimensional domain with a constant step and then we evaluate the model on each point of the grid."]},{"cell_type":"code","metadata":{"id":"tYxx4CgrDTAk"},"source":["%matplotlib inline \n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Parameters\n","n_classes = 3\n","plot_colors = \"bry\" # blue-red-yellow\n","plot_step = 0.02\n","\n","# Load the data\n","iris = load_iris()\n","\n","# Choose the attributes length and width of the petals\n","pair = [2, 3]\n","\n","# Keep only the two attributes\n","X = iris.data[:, pair]\n","y = iris.target\n","\n","# Classification Decision Tree learning\n","clf = DecisionTreeClassifier().fit(X, y)\n","\n","# Display of the decision surface\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))\n","Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n","plt.xlabel(iris.feature_names[pair[0]])\n","plt.ylabel(iris.feature_names[pair[1]])\n","plt.axis(\"tight\")\n","\n","# Display of training points\n","for i, color in zip(range(n_classes), plot_colors):\n","    idx = np.where(y == i)\n","    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i], cmap=plt.cm.Paired)\n","plt.axis(\"tight\")\n","plt.suptitle(\"Decision surface of a decision tree using paired features\")\n","plt.legend()\n","plt.savefig('fig.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_12nr2SpDTAo"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 3.** Redo the display for the other attribute pairs. On which pair is the separation between the classes most marked?\n","\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"18kZBIZhDTAo"},"source":["#### 1.2. Regression Decision Trees\n","\n","For regression with decision trees, Scikit-learn offers the `DecisionTreeRegressor` class. As for the classification, the `fit()` method takes as input the parameter X (attributes of the observations). Please note: the y value are not class labels but real (numerical) values."]},{"cell_type":"code","metadata":{"id":"XdUS9SuXDTAp"},"source":["X = [[0, 0], [2, 2]]\n","y = [0.5, 2.5]\n","clf = tree.DecisionTreeRegressor()\n","clf = clf.fit(X, y)\n","clf.predict([[1, 1]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R2gtiDT5DTAs"},"source":["In the following example we will build a sine wave signal affected by white noise and we will learn a regression tree on this training data."]},{"cell_type":"code","metadata":{"id":"1ju0-LNcDTAu"},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","\n","# Create training data\n","rng = np.random.RandomState(1)\n","X = np.sort(5 * rng.rand(80, 1), axis=0)\n","y = np.sin(X).ravel()\n","y[::5] += 3 * (0.5 - rng.rand(16))\n","\n","# Learn the model\n","regr_1 = DecisionTreeRegressor(max_depth=2)\n","regr_1.fit(X, y)\n","\n","# Prediction\n","X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n","y_1 = regr_1.predict(X_test)\n","\n","# Display the results\n","plt.figure()\n","plt.scatter(X, y, c=\"darkorange\", label=\"data\")\n","plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n","plt.xlabel(\"data\")\n","plt.ylabel(\"target\")\n","plt.title(\"Decision Tree Regression\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66E880GoDTAx"},"source":["print(len(y))\n","print(len(y[::5]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"99lQHNt5DTA0"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 4.** Change the value of the `max_depth` parameter. What happens if we take a too large value? Too small ? Change the rate of elements affected by noise (`y[:: 5]`). When all the elements are affected by noise, should we prefer a high or low value for `max_depth`?\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"0Iqyo_jeDTA1"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 5.** Load the Diabetes database from the `sklearn.datasets` module and make a random partition in the training part and the test part (70% training, 30% test). Build a regression tree model on the training set. Calculate the mean square error on the test set. Do a grid search to find the value of the `max_depth` parameter which minimizes this error.\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"33rg-b0fDTA2"},"source":["from sklearn.metrics import mean_squared_error\n","from sklearn.datasets import load_diabetes\n","\n","diabetes = # Complete the code\n","X_train, X_test, y_train, y_test = # Complete the code\n","\n","\n","\n","pgrid = # Complete the code\n","grid_search = GridSearchCV(dtr, param_grid=pgrid, scoring='neg_mean_squared_error', cv=10)\n","\n","# Complete the code"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"74IiVoCoiacH"},"source":["# 2. Random Forest & Bagging\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nuVumZMXijYS"},"source":["### Ensemble methods\n","\n","The ensemble methods for statistical learning algorithms (ie: ensemble learning) are based on the idea of combining the predictions of several predictors (or classifiers) for better generalization and to compensate for any defects of individual predictors.\n","\n","Usually, there are two families of such methods:\n","\n","   1. Averaging (aggregating is a more appropriate word) methods (such as bagging and random forests) where the principle is to average several predictions while hoping for a better result following the reduction in variance of the average estimator (note that for the classification case, we consider the majority vote).\n","   2. Adaptive methods (boosting) where the parameters are iteratively adapted to produce a better mixture (we will introduce them through the 3rd lecture).\n","\n","In the following we will explore the algorithm classes cited in the first point using `Scikit-learn` and present some comparisons.\n","\n","### Bagging\n","\n","Bagging methods build several instances of an estimator, computed on random samples taken from the learning base (and possibly a part of the attributes, also randomly selected), and then combines the individual predictions by averaging them to reduce the variance of the estimator. Their main advantage lies in the fact that they build an improved version of the basic algorithm, without asking for modification of this algorithm. The price to pay is a higher computation cost. \n","\n","In `Scikit-learn`, the bagging methods are implemented via both the `BaggingClassifier` and `BaggingRegressor` class. The constructors take as parameters a basic estimator and the strategy for selecting points and attributes:\n","\n","+ `base_estimator`: optional (default = None). If None then the estimator is a decision tree.\n","+ `max_samples`: the size of the random sample taken from the learning database.\n","+ `max_features`: the number of attributes drawn at random.\n","+ `bootstrap`: boolean, optional (default = True). Draw data points $(x_i,y_i)$ with replacement or not.\n","+ `bootstrap_features`: boolean, optional (default = False). Attributes drawing with replacement or not.\n","+ `oob_score`: boolean. Estimate or not the OOB (Out-of-Bag) generalization error.\n","\n","The following code builds a set of basic `KNeighborsClassifier` classifiers, each using a 50% sample of learning data points and 50% of the attributes (features):"]},{"cell_type":"code","metadata":{"id":"axLAGIlsisx-"},"source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"giv4-0fMiv76"},"source":["In this example we will use the digits database, which contains 10 classes (images of numbers in handwriting). There are 1797 instances, each instance has 64 attributes."]},{"cell_type":"code","metadata":{"id":"pDkp3UwLizUn"},"source":["from sklearn.datasets import load_digits\n","digits = load_digits()\n","print(digits.data.shape)\n","# Display one picture\n","import matplotlib.pyplot as plt\n","plt.gray()\n","plt.matshow(digits.images[1])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XBp0WEkvi2aK"},"source":["The basic classifier is a decision tree:"]},{"cell_type":"code","metadata":{"id":"hz1zqxl7i5Bj"},"source":["X=digits.data\n","y=digits.target\n","clf = tree.DecisionTreeClassifier()\n","clf.fit(X, y)\n","accuracy=clf.score(X,y)\n","print('accuracy ', accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dUZbikILi7rd"},"source":["On the learning set, accuracy = 1. For more realism, we split the learning set into a train/test set in order to see the behavior of the tree on new data (different from that of training):"]},{"cell_type":"code","metadata":{"id":"aoPyYLyyi-hk"},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n","#clf = tree.DecisionTreeClassifier()\n","clf = tree.DecisionTreeClassifier(max_depth=5)\n","clf.fit(X_train, y_train)\n","Z = clf.predict(X_test)\n","accuracy=clf.score(X_test,y_test)\n","print('accuracy ', accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gKiWOpwQjF-E"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 6.** Compute the mean and the variance of the `accuracy` value on 100 draws for the train/test splitting. Compute then the standard deviation and the confidence interval with 95 $\\%$ probability of containing the true mean parameter. What can we conclude?\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"l2H34yeXjGxw"},"source":["import math\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_digits\n","\n","digits = load_digits()\n","X=digits.data\n","y=digits.target\n","N = 100\n","accuracy = np.zeros(N)\n","\n","for i in range(N):\n","    X_train, X_test, y_train, y_test = # Complete the code\n","    # Complete the code\n","    accuracy[i]=# Complete the code\n","    \n","\n","# Print the mean and standard deviation of accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c383AomMjKzc"},"source":["To compare, we will build a bagging classifier on our data with a basic `DecisionTreeClassifier` classifier (two trees are proposed here; one tree with more random through the option `max_features=0.5`):"]},{"cell_type":"code","metadata":{"id":"AM6bYWwsjOC2"},"source":["clf = BaggingClassifier(tree.DecisionTreeClassifier(), max_samples=0.5, n_estimators=100)\n","clf.fit(X_train, y_train)\n","Z = clf.predict(X_test)\n","accuracy=clf.score(X_test,y_test)\n","print(\"Accuracy of bagging classifier \", accuracy)\n","\n","clf_more_rd = BaggingClassifier(tree.DecisionTreeClassifier(), max_samples=0.5, max_features=0.5, n_estimators=100)\n","clf_more_rd.fit(X_train, y_train)\n","Z_more_rd = clf_more_rd.predict(X_test)\n","accuracy_more_rd=clf_more_rd.score(X_test,y_test)\n","print(\"Accuracy of bagging classifier with more random \", accuracy_more_rd)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SUeh8EPEjRMV"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 7.** Compute the variance of the accuracy value on 100 draws for the train/test splitting. Compare with the variance of the base classifier. What can we conclude?\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"Tb0nzNlFjUdJ"},"source":["# Write the code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8-qtY-fjXfW"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 8.** Construct and plot accuracy vs `n_estimators`. What do you observe?\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"xYydzSEZjb7S"},"source":["# Write the code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxt8S76djg9g"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 9.** By using cross valisation with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV), [tune](https://scikit-learn.org/stable/modules/grid_search.html) (i.e: search for the best values of) both parameters `max_samples` and `max_features` parameters. In other words, search of the values of these parameters allowing to get the best result? For which values do we get this best result? \n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"ts_ODAbhjiJA"},"source":["# Write the code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nWN_3qmajktu"},"source":["### Random Forests\n","\n","The random forest algorithm proposes an aggregation of decision trees. It uses the same principle as bagging, but with an additional step of randomization in the selection of the attributes of the nodes in order to reduce the variance of the obtained estimator. The two Python objects that implement random forests are `RandomForestClassifier` and `RandomForestRegressor`. The most important parameters are:\n","\n","   1. `n_estimators`: integer, optional (default = 10). The number of trees.\n","   2. `max_features`: the number of attributes to consider in each split.\n","   3. `max_samples`: the size of the random sample taken from the learning base.\n","   4. `min_samples_leaf`: the minimum number of elements in a leaf node.\n","   5. `oob_score`: boolean. Estimate or not the OOB (Out-of-Bag) generalization error.\n","\n","Thereafter we will redo the classification based on Digits using a `RandomForestClassifier` classifier:"]},{"cell_type":"code","metadata":{"id":"O6mlr0KGjn7P"},"source":["from sklearn.ensemble import RandomForestClassifier\n","digits = load_digits()\n","X=digits.data\n","y=digits.target\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.90)\n","clf = RandomForestClassifier(n_estimators=200)\n","clf.fit(X_train, y_train)\n","Z = clf.predict(X_test)\n","accuracy=clf.score(X_test,y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PMl1fQ_yjq5Z"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 10.** How does the value of the accuracy variable compare with the bagging case that uses the same number of trees (200 in our case)?\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"9XCfYaX9jtmH"},"source":["# Write the code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hSnwGmljxA6"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 11.** Compute the variance of the accuracy value on 100 draws for the train/test splitting. What can we conclude by comparing with the previous section (bagging)?\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"_toC9FdLjz3Y"},"source":["# Write the code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yY0sfZypj3CY"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 12.** Plot the accuracy vs `n_estimators` graph. What do you notice? From what n_estimators value do we no longer improve?\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"iKQxJHmTj5hi"},"source":["# Write the code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0ydqKjCj8tK"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","**Question 13.** Look in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html) for the [`ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html) and redo the classification with this type of classifier. Compare with `RandomForestClassifier`.\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"SrY6usl0kCxD"},"source":["from sklearn.ensemble import ExtraTreesClassifier\n","\n","# Write the code here"],"execution_count":null,"outputs":[]}]}