{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "For the competition, our goal was to produce as many rich features as possible in order to capture maximal amounts of information from the dataset\n",
    "\n",
    "Below is our approach on preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sklearn as sk \n",
    "import matplotlib.pyplot as plt \n",
    "import xgboost as xgb \n",
    "import seaborn as sns \n",
    "from math import inf\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying TLD and ORG columns \n",
    "copy_train = pd.read_csv(\"C://Users//Sauraj (Work mode)//Desktop//train.csv\")\n",
    "copy_test = pd.read_csv(\"C://Users//Sauraj (Work mode)//Desktop//test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspected that the data must be having duplicated rows since some of the rows seemed to repeat at various locations, so in order to reduce data repetition and minimize the chance of having increased false-positives, our approach was heuristic in nature, by just removing the duplicates that were present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>org</th>\n",
       "      <th>tld</th>\n",
       "      <th>ccs</th>\n",
       "      <th>bcced</th>\n",
       "      <th>mail_type</th>\n",
       "      <th>images</th>\n",
       "      <th>urls</th>\n",
       "      <th>salutations</th>\n",
       "      <th>designation</th>\n",
       "      <th>chars_in_subject</th>\n",
       "      <th>chars_in_body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>232</td>\n",
       "      <td>4 Dec 2013 13:50:13 -0000</td>\n",
       "      <td>phpclasses</td>\n",
       "      <td>org</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>multipart/alternative</td>\n",
       "      <td>21</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>26592</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>434</td>\n",
       "      <td>Thu, 4 Feb 2016 16:00:36 +0530</td>\n",
       "      <td>iiitd</td>\n",
       "      <td>ac.in</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>multipart/related</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1521139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>577</td>\n",
       "      <td>Sat,  5 Mar 2016 12:23:17 +0530 (IST)</td>\n",
       "      <td>sampark</td>\n",
       "      <td>gov.in</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>text/html</td>\n",
       "      <td>52</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>28650</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>985</td>\n",
       "      <td>Tue, 18 Aug 2015 12:14:47 +0530</td>\n",
       "      <td>iiitd</td>\n",
       "      <td>ac.in</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>multipart/alternative</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>9562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>987</td>\n",
       "      <td>Fri, 07 Apr 2017 15:30:01 +0530</td>\n",
       "      <td>nrsc</td>\n",
       "      <td>gov.in</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>multipart/alternative</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80168</th>\n",
       "      <td>80168</td>\n",
       "      <td>Fri, 15 Feb 2019 18:07:55 +0100</td>\n",
       "      <td>asvspoof</td>\n",
       "      <td>org</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>multipart/alternative</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16533</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80170</th>\n",
       "      <td>80170</td>\n",
       "      <td>Tue, 07 Apr 2015 14:12:29 +0000 (UTC)</td>\n",
       "      <td>quora</td>\n",
       "      <td>com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>multipart/alternative</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>80913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80172</th>\n",
       "      <td>80172</td>\n",
       "      <td>Fri,  1 May 2015 11:48:55 +0530 (IST)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>text/html</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>13464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80174</th>\n",
       "      <td>80174</td>\n",
       "      <td>Fri, 14 Dec 2018 09:01:13 +0000 (UTC)</td>\n",
       "      <td>medium</td>\n",
       "      <td>com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>multipart/alternative</td>\n",
       "      <td>32</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>169.0</td>\n",
       "      <td>105276</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80175</th>\n",
       "      <td>80175</td>\n",
       "      <td>Sun, 21 Oct 2012 23:43:49 -0400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>multipart/alternative</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31036 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                   date         org     tld  \\\n",
       "232           232              4 Dec 2013 13:50:13 -0000  phpclasses     org   \n",
       "434           434         Thu, 4 Feb 2016 16:00:36 +0530       iiitd   ac.in   \n",
       "577           577  Sat,  5 Mar 2016 12:23:17 +0530 (IST)     sampark  gov.in   \n",
       "985           985        Tue, 18 Aug 2015 12:14:47 +0530       iiitd   ac.in   \n",
       "987           987        Fri, 07 Apr 2017 15:30:01 +0530        nrsc  gov.in   \n",
       "...           ...                                    ...         ...     ...   \n",
       "80168       80168        Fri, 15 Feb 2019 18:07:55 +0100    asvspoof     org   \n",
       "80170       80170  Tue, 07 Apr 2015 14:12:29 +0000 (UTC)       quora     com   \n",
       "80172       80172  Fri,  1 May 2015 11:48:55 +0530 (IST)         NaN     NaN   \n",
       "80174       80174  Fri, 14 Dec 2018 09:01:13 +0000 (UTC)      medium     com   \n",
       "80175       80175        Sun, 21 Oct 2012 23:43:49 -0400         NaN     NaN   \n",
       "\n",
       "       ccs  bcced              mail_type  images  urls  salutations  \\\n",
       "232      0      0  multipart/alternative      21    55            0   \n",
       "434      2      0      multipart/related       0     0            1   \n",
       "577      0      0              text/html      52   101            0   \n",
       "985      7      0  multipart/alternative       2    13            1   \n",
       "987      0      0  multipart/alternative       0     4            1   \n",
       "...    ...    ...                    ...     ...   ...          ...   \n",
       "80168    0      0  multipart/alternative       0     7            1   \n",
       "80170    0      0  multipart/alternative       0    88            1   \n",
       "80172    0      0              text/html       0     2            1   \n",
       "80174    0      0  multipart/alternative      32   239            1   \n",
       "80175    0      0  multipart/alternative       0     5            0   \n",
       "\n",
       "       designation  chars_in_subject  chars_in_body  label  \n",
       "232              0              50.0          26592      1  \n",
       "434              0              96.0        1521139      1  \n",
       "577              0              75.0          28650      2  \n",
       "985              1              34.0           9562      1  \n",
       "987              0              32.0           1210      1  \n",
       "...            ...               ...            ...    ...  \n",
       "80168            0              29.0          16533      1  \n",
       "80170            1              98.0          80913      1  \n",
       "80172            0              73.0          13464      0  \n",
       "80174            1             169.0         105276      0  \n",
       "80175            0             120.0           2467      0  \n",
       "\n",
       "[31036 rows x 14 columns]"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_check = ['date','mail_type','images','urls','chars_in_subject','chars_in_body','org','tld']\n",
    "copy_train[copy_train.duplicated(subset=cols_to_check, keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is another dataset we made with outliers removed (Our hypothesis was that removing outliers should improve performance )\n",
    "train_data = pd.read_csv(\"C://Users//Sauraj (Work mode)//Desktop//total_train_removed_outlier_with_0.98.csv\")\n",
    "test_data = pd.read_csv(\"C://Users//Sauraj (Work mode)//Desktop//total_test_removed_outlier_with_0.98.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is another dataset we made with our initial Feature engineering notebook (FE_initial.ipynb)\n",
    "train_copy = pd.read_csv(\"C://Users//Sauraj (Work mode)//Desktop//Databases//train_data_processed.csv\")\n",
    "test_copy = pd.read_csv(\"C://Users//Sauraj (Work mode)//Desktop//Databases//test_data_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns which we need to copy \n",
    "cols_to_copy = ['total_mails_by_sender','first_mail_of_sender','sender_freq_total_period','sender_freq_prev_year',\n",
    "                'sender_freq_prev_week','sender_freq_prev_month','sender_freq_prev_six_months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns to copy into the original training and testing sets \n",
    "train_data = pd.concat([train_data, train_copy[cols_to_copy]], axis=1)\n",
    "test_data = pd.concat([test_data, test_copy[cols_to_copy]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing data calculator function to detect the number of missing values in each column \n",
    "def missing_data_calculator(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending = False)\n",
    "    missing_data = pd.concat([total,percent], axis = 1, keys = ['Total', 'Percent'])\n",
    "    return missing_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mail-type is more of a categorical feature, so the code here is to convert it to categorical \n",
    "train_data['mail_type'] = train_data['mail_type'].astype('category')\n",
    "train_data['mail_type'] = train_data['mail_type'].cat.codes\n",
    "#Chars_in_subject had NaN values, so filling them up with 0\n",
    "train_data['chars_in_subject'] = train_data['chars_in_subject'].fillna(0)\n",
    "test_data['chars_in_subject'] = test_data['chars_in_subject'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['label'] = train_copy['label']\n",
    "# train_data['tld'] = copy_train['tld']\n",
    "# train_data['org'] = copy_train['org']\n",
    "\n",
    "# test_data['tld'] = copy_test['tld']\n",
    "# test_data['org'] = copy_test['org']\n",
    "\n",
    "# test_data['date'] = copy_test['date']\n",
    "# train_data['date'] = copy_train['date']\n",
    "\n",
    "\n",
    "# test_data['mail_type'] = copy_test['mail_type']\n",
    "# train_data['mail_type'] = copy_train['mail_type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['date','mail_type','images','urls','chars_in_subject','chars_in_body','org','tld']\n",
    "dropping_index = train_data[train_data.duplicated(subset=cols_to_check, keep='first')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(dropping_index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49140 entries, 0 to 80173\n",
      "Data columns (total 21 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Unnamed: 0                   49140 non-null  int64  \n",
      " 1   hour_offset                  49140 non-null  float64\n",
      " 2   ccs                          49140 non-null  int64  \n",
      " 3   bcced                        49140 non-null  int64  \n",
      " 4   total_mails_by_sender        49140 non-null  int64  \n",
      " 5   sender_freq_total_period     49140 non-null  float64\n",
      " 6   sender_freq_prev_year        49140 non-null  float64\n",
      " 7   sender_freq_prev_week        49140 non-null  float64\n",
      " 8   sender_freq_prev_month       49140 non-null  float64\n",
      " 9   sender_freq_prev_six_months  49140 non-null  float64\n",
      " 10  images                       49140 non-null  int64  \n",
      " 11  urls                         49140 non-null  int64  \n",
      " 12  salutations                  49140 non-null  int64  \n",
      " 13  designation                  49140 non-null  int64  \n",
      " 14  chars_in_subject             49140 non-null  int64  \n",
      " 15  chars_in_body                49140 non-null  int64  \n",
      " 16  label                        49140 non-null  int64  \n",
      " 17  tld                          46997 non-null  object \n",
      " 18  org                          46999 non-null  object \n",
      " 19  date                         49140 non-null  object \n",
      " 20  mail_type                    49026 non-null  object \n",
      "dtypes: float64(6), int64(11), object(4)\n",
      "memory usage: 8.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#Information about the train data after removing duplicates \n",
    "train_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34365 entries, 0 to 34364\n",
      "Data columns (total 20 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Unnamed: 0                   34365 non-null  int64  \n",
      " 1   hour_offset                  34365 non-null  float64\n",
      " 2   ccs                          34365 non-null  int64  \n",
      " 3   bcced                        34365 non-null  int64  \n",
      " 4   total_mails_by_sender        34365 non-null  int64  \n",
      " 5   sender_freq_total_period     34365 non-null  float64\n",
      " 6   sender_freq_prev_year        34365 non-null  float64\n",
      " 7   sender_freq_prev_week        34365 non-null  float64\n",
      " 8   sender_freq_prev_month       34365 non-null  float64\n",
      " 9   sender_freq_prev_six_months  34365 non-null  float64\n",
      " 10  images                       34365 non-null  int64  \n",
      " 11  urls                         34365 non-null  int64  \n",
      " 12  salutations                  34365 non-null  int64  \n",
      " 13  designation                  34365 non-null  int64  \n",
      " 14  chars_in_subject             34365 non-null  int64  \n",
      " 15  chars_in_body                34365 non-null  int64  \n",
      " 16  tld                          32829 non-null  object \n",
      " 17  org                          32829 non-null  object \n",
      " 18  date                         34365 non-null  object \n",
      " 19  mail_type                    34285 non-null  object \n",
      "dtypes: float64(6), int64(10), object(4)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#Information about the test data\n",
    "test_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log-transformation function to alleviate skewness (This was just a prototype function to see if log-transform is needed or not)\n",
    "def log_transformer(data, column):\n",
    "    print(\"Pre-transformation Skewness: {}\".format(scipy.stats.skew(data[column])))\n",
    "    data[column] = np.log1p(data[column])\n",
    "    print(\"Post-transformation Skewness: {}\".format(scipy.stats.skew(data[column])))\n",
    "    return data[column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                      0.388898\n",
       "hour_offset                     1.200528\n",
       "ccs                             3.573887\n",
       "bcced                          17.956983\n",
       "total_mails_by_sender           2.115314\n",
       "sender_freq_total_period        1.393086\n",
       "sender_freq_prev_year           1.780520\n",
       "sender_freq_prev_week           4.238770\n",
       "sender_freq_prev_month          1.695669\n",
       "sender_freq_prev_six_months     1.617779\n",
       "images                          1.760993\n",
       "urls                            1.884381\n",
       "salutations                     0.430141\n",
       "designation                     2.610617\n",
       "chars_in_subject                1.059809\n",
       "chars_in_body                  21.833610\n",
       "label                           2.038638\n",
       "dtype: float64"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the skewness of the features (What the approach was that if skew is more than 1 or less than -1, we would log-transform it)\n",
    "train_data.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most ML algorithms expect normality in their data since the gaussianity in the data helps to extract the optimal parameters that help to maximize the predictive power of the models. \n",
    "\n",
    "We had various approaches for tackling this, with the following used: \n",
    "\n",
    "1. ~StandardScaler~: Was dropped since it is sensitive to outliers and extreme datapoints, and we did not have enough statistical evidence to reject outliers from the data or impute them with a mean, since mean is also sensitive to outliers. \n",
    "\n",
    "2. ~Box-Cox transform~: Helped only for some columns, but came with the drawback that the skewness would often cross +1, leading to shape imbalance. \n",
    "\n",
    "3. ~MinMaxScaler~: Scaled the data between -1 and 1 to make it normalized, but only scaled the data and not change the shape of the distribution. \n",
    "\n",
    "\n",
    "Finally, we came to QuantileTransformer, a preprocessing method that converts the numeric columns into a normal distribution by partitioning them into bins and finding the parameter that helps to turn the columns into a normally distributed shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-92f34f976f8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQuantileTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_quantiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_distribution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_implicit_zeros\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "\n",
    "#These columns are numeric in nature, so normalize them=\n",
    "cols = ['total_mails_by_sender','sender_freq_total_period','sender_freq_prev_year','sender_freq_prev_week',\n",
    "        'sender_freq_prev_month','sender_freq_prev_six_months','images','urls','chars_in_subject','chars_in_body']\n",
    "\n",
    "#For loop (Because we don't do these things manually)\n",
    "for col in cols:\n",
    "    mms = QuantileTransformer(n_quantiles=100, output_distribution='normal', ignore_implicit_zeros=True)\n",
    "    train_data[col] = mms.fit_transform(train_data[col].to_numpy().reshape(-1, 1))\n",
    "    test_data[col] = mms.transform(test_data[col].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing Normality in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log-transformation (Was applied before, but dropped later)\n",
    "train_data['images'] = np.log1p(train_data['images'])\n",
    "train_data['urls'] = np.log1p(train_data['urls'])\n",
    "train_data['chars_in_subject'] = np.log1p(train_data['chars_in_subject'])\n",
    "train_data['chars_in_body'] = np.log1p(train_data['chars_in_body'])\n",
    "\n",
    "test_data['images'] = np.log1p(test_data['images'])\n",
    "test_data['urls'] = np.log1p(test_data['urls'])\n",
    "test_data['chars_in_subject'] = np.log1p(test_data['chars_in_subject'])\n",
    "test_data['chars_in_body'] = np.log1p(test_data['chars_in_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-transformation Skewness: 259.2875244854419\n",
      "Post-transformation Skewness: 0.4784786447007777\n",
      "Pre-transformation Skewness: 20.794114254400494\n",
      "Post-transformation Skewness: -0.8549831830804311\n",
      "Pre-transformation Skewness: 140.9033857557555\n",
      "Post-transformation Skewness: -0.3038343614984591\n",
      "Pre-transformation Skewness: 2.3847461829109706\n",
      "Post-transformation Skewness: -1.3569506502331155\n",
      "Pre-transformation Skewness: 1.9650694911025917\n",
      "Post-transformation Skewness: -0.4306094761764705\n",
      "Pre-transformation Skewness: 1.5765969840800516\n",
      "Post-transformation Skewness: 0.6977705114465274\n",
      "Pre-transformation Skewness: 1.5922024544286655\n",
      "Post-transformation Skewness: 0.8328332957332146\n",
      "Pre-transformation Skewness: 4.743116650178704\n",
      "Post-transformation Skewness: 0.6678900545232977\n",
      "Pre-transformation Skewness: 1.398686194476134\n",
      "Post-transformation Skewness: 0.6164297138092417\n",
      "Pre-transformation Skewness: 1.413013345066901\n",
      "Post-transformation Skewness: 0.7152531580243529\n",
      "Pre-transformation Skewness: 122.57392245738895\n",
      "Post-transformation Skewness: 0.5145791632252409\n",
      "Pre-transformation Skewness: 20.618708974652098\n",
      "Post-transformation Skewness: -0.8140390935917928\n",
      "Pre-transformation Skewness: 111.06983152360473\n",
      "Post-transformation Skewness: -0.2991642958439879\n",
      "Pre-transformation Skewness: 2.4110269229367405\n",
      "Post-transformation Skewness: -1.4132410086228195\n",
      "Pre-transformation Skewness: 1.9698055227915487\n",
      "Post-transformation Skewness: -0.2743936639964486\n",
      "Pre-transformation Skewness: 1.5991135803811676\n",
      "Post-transformation Skewness: 0.7939481259014447\n",
      "Pre-transformation Skewness: 1.605398110246509\n",
      "Post-transformation Skewness: 1.0302189278405063\n",
      "Pre-transformation Skewness: 4.720389725025849\n",
      "Post-transformation Skewness: 0.8837600655901039\n",
      "Pre-transformation Skewness: 1.4233420497293692\n",
      "Post-transformation Skewness: 0.7887208349686018\n",
      "Pre-transformation Skewness: 1.4392449878268039\n",
      "Post-transformation Skewness: 0.8973338758284017\n"
     ]
    }
   ],
   "source": [
    "# Dampening the effect of outliers in the training set \n",
    "\n",
    "# train_data['images'] = log_transformer(train_data, 'images')\n",
    "# train_data['chars_in_body'] = log_transformer(train_data, 'chars_in_body')\n",
    "# train_data['urls'] = log_transformer(train_data, 'urls')\n",
    "# train_data['chars_in_subject'] = log_transformer(train_data, 'chars_in_subject')\n",
    "# train_data['total_mails_by_sender'] = log_transformer(train_data, 'total_mails_by_sender')\n",
    "# train_data['sender_freq_total_period'] = log_transformer(train_data, 'sender_freq_total_period')\n",
    "# train_data['sender_freq_prev_year'] = log_transformer(train_data, 'sender_freq_prev_year')\n",
    "# train_data['sender_freq_prev_week'] = log_transformer(train_data, 'sender_freq_prev_week')\n",
    "# train_data['sender_freq_prev_month'] = log_transformer(train_data, 'sender_freq_prev_month')\n",
    "# train_data['sender_freq_prev_six_months'] = log_transformer(train_data, 'sender_freq_prev_six_months')\n",
    "\n",
    "\n",
    "# Dampening the effect of outliers in the testing set \n",
    "\n",
    "# test_data['images'] = log_transformer(test_data, 'images')\n",
    "# test_data['chars_in_body'] = log_transformer(test_data, 'chars_in_body')\n",
    "# test_data['urls'] = log_transformer(test_data, 'urls')\n",
    "# test_data['chars_in_subject'] = log_transformer(test_data, 'chars_in_subject')\n",
    "# test_data['total_mails_by_sender'] = log_transformer(test_data, 'total_mails_by_sender')\n",
    "# test_data['sender_freq_total_period'] = log_transformer(test_data, 'sender_freq_total_period')\n",
    "# test_data['sender_freq_prev_year'] = log_transformer(test_data, 'sender_freq_prev_year')\n",
    "# test_data['sender_freq_prev_week'] = log_transformer(test_data, 'sender_freq_prev_week')\n",
    "# test_data['sender_freq_prev_month'] = log_transformer(test_data, 'sender_freq_prev_month')\n",
    "# test_data['sender_freq_prev_six_months'] = log_transformer(test_data, 'sender_freq_prev_six_months')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Preprocessing functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "#Function that creates a new column called \"sender\", capturing the details of the sender by appending \n",
    "#the org and tld together. \n",
    "def create_senders(df):\n",
    "    #Senders list\n",
    "    senders = [] \n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isnull(row['tld']):\n",
    "            sender = row['org']\n",
    "        elif pd.isnull(row['org']):\n",
    "            sender = row['tld']\n",
    "        elif row['tld'][0] == '.':\n",
    "            sender = row['org'] + row['tld']\n",
    "        else:\n",
    "            sender = row['org'] + '.' + row['tld']\n",
    "        senders.append(sender)\n",
    "\n",
    "#Insert the \"sender\" feature first\n",
    "    df.insert(8, 'sender', senders)\n",
    "    print(\"Sender information added\")\n",
    "    return df\n",
    "\n",
    "#Date-time extraction feature for calculating the date and time \n",
    "def date_time_extraction(df):\n",
    "    datetimes = []\n",
    "    dates = []\n",
    "    years = []\n",
    "    months = []\n",
    "    days = []\n",
    "    weekdays = []\n",
    "    times = []\n",
    "    offsets = []\n",
    "    for i, row in df.iterrows():\n",
    "        time = parser.parse(row['date'].split('(')[0])\n",
    "        local_time = time.astimezone()\n",
    "        datetimes.append(local_time)\n",
    "        date = local_time.date()\n",
    "        dates.append(date)\n",
    "        years.append(date.year)\n",
    "        months.append(date.month)\n",
    "        days.append(date.day)\n",
    "        weekdays.append(local_time.weekday())\n",
    "        times.append(local_time.time())\n",
    "        try:\n",
    "            offset = time.utcoffset().seconds/3600 - local_time.utcoffset().seconds/3600\n",
    "            offsets.append(offset)\n",
    "        except:\n",
    "            offsets.append(None)\n",
    "            #print(\"Error\")\n",
    "\n",
    "    df.insert(0,'datetime', datetimes)\n",
    "    print('Dates added')\n",
    "    df['date'] = dates\n",
    "    df.insert(2,'year', years)\n",
    "    print('Year added')\n",
    "    df.insert(3,'month', months)\n",
    "    print('Month added')\n",
    "    df.insert(4,'day', days)\n",
    "    print('Day added')\n",
    "    df.insert(5,'weekday', weekdays)\n",
    "    print('Weekday added')\n",
    "    df.insert(6,'time', times)\n",
    "    print('Time added')\n",
    "    df.insert(7,'hour_offset', offsets)\n",
    "    print('Hour-offset added')\n",
    "    \n",
    "    return df \n",
    "\n",
    "# def row_deleter(df):\n",
    "#     df = df.drop('Unnamed: 0', axis=1)\n",
    "#     print('Row deletion complete')\n",
    "#     return df \n",
    "\n",
    "#Function to calculate img-url ratio\n",
    "def img_url_ratio(df):\n",
    "    df['img_url_ratio'] = df['images']/df['urls']\n",
    "    df['img_url_ratio'].fillna(0)\n",
    "    print('Img/url complete')\n",
    "    return df\n",
    "\n",
    "\n",
    "#Function to calculate body-subject ratio\n",
    "def body_subject_ratio(df):\n",
    "    df['body_subj_ratio'] = df['chars_in_body']/df['chars_in_subject']\n",
    "    df['body_subj_ratio'].fillna(0)\n",
    "    df.loc[df[\"body_subj_ratio\"] == inf] = 0 \n",
    "    print('Body/subject ratio added')\n",
    "    return df \n",
    "\n",
    "\n",
    "def ht_hs_binary(df):\n",
    "    \"\"\"\n",
    "    Function for representing in binary whether an email \n",
    "    has a subject/title or not \n",
    "    \"\"\"\n",
    "    df.loc[df[\"chars_in_subject\"] != 0, \"has_subject\"] = 1\n",
    "    df.loc[df[\"chars_in_subject\"] == 0, \"has_subject\"] = 0\n",
    "\n",
    "    df.loc[df[\"chars_in_body\"] != 0, \"has_body\"] = 1\n",
    "    df.loc[df[\"chars_in_body\"] == 0, \"has_body\"] = 0\n",
    "    print('HT-HS-binary encoding complete')\n",
    "    return df\n",
    "\n",
    "\n",
    "def mail_type_preprocessing(df):\n",
    "    \n",
    "    #Preprocessing \"multipart/text\" and \"Multipart/Text\" repetition issue \n",
    "    df['mail_type'] = df['mail_type'].str.lower().unique()\n",
    "    df[['data_type','source_type']] = df['mail_type'].str.split('/', expand=True)\n",
    "    return df \n",
    "\n",
    "def mailtype_dummy_vars(df):\n",
    "    #Preprocessing for mailtype to make them into dummy variables \n",
    "#     df['mail_type'] = df['mail_type'].str.lower().unique()\n",
    "    df['mail_type'] = df['mail_type'].astype('category')\n",
    "    df['mail_type'] = df['mail_type'].cat.codes\n",
    "    \n",
    "    return df \n",
    "\n",
    "def forward_level(df):\n",
    "    #Feature to determine if the mail was cced and bcced together \n",
    "    # If CC & BCC, forward level is type 2\n",
    "    # If either CC or BCC, forward level is type 1\n",
    "    # If none, then type 0 \n",
    "    \n",
    "    df.loc[(df[\"ccs\"] == 0) & (df[\"bcced\"] == 0), \"forward_level\"] = 0 \n",
    "    df.loc[(df[\"ccs\"] >= 1) | (df[\"bcced\"] > 0), \"forward_level\"] = 1 \n",
    "    df.loc[(df[\"ccs\"] == 1) & (df[\"bcced\"] == 1), \"forward_level\"] = 2\n",
    "    print('Forward levels added')\n",
    "    \n",
    "    return df \n",
    "\n",
    "#Function to create dummy values for each column. \n",
    "def dummy_transform(df, column_name=str):\n",
    "    df = pd.get_dummies(df, columns=[column_name])\n",
    "    print(\"Dummy variable transformation for {} complete\".format(column_name))\n",
    "    return df \n",
    "\n",
    "\n",
    "#Function that detects if a certain email had a sender or not \n",
    "def no_sender_reported(df):\n",
    "    \"\"\"\n",
    "    Certain emails are coming without any specified address or top level domain. \n",
    "    Because these emails are anomalous in nature, it is best to classify them as \n",
    "    a 'no-sender-reported' type email\"\n",
    "    \"\"\"\n",
    "    df['No_sender_reported'] = (train_data['org'].isnull()) & (train_data['tld'].isnull())\n",
    "    df['No_sender_reported'] = 1 - (pd.get_dummies(train_data['No_sender_reported']))  # want 1 on NaN values, not the opposite\n",
    "    print(\"No sender reported feature created\")\n",
    "    return df\n",
    "\n",
    "#Function to create columns for organizations wrt to the labels they have \n",
    "def org_set_information(df):\n",
    "    #Find all the unique values in each subset\n",
    "    labelled_update = train_data[train_data['label']==0]['org'].unique()\n",
    "    labelled_personal = train_data[train_data['label']==1]['org'].unique()\n",
    "    labelled_promotions = train_data[train_data['label']==2]['org'].unique()\n",
    "    labelled_forums = train_data[train_data['label']==3]['org'].unique()\n",
    "    labelled_purchases = train_data[train_data['label']==4]['org'].unique()\n",
    "    labelled_travel = train_data[train_data['label']==5]['org'].unique()\n",
    "    labelled_spam = train_data[train_data['label']==6]['org'].unique()\n",
    "    labelled_social = train_data[train_data['label']==7]['org'].unique()\n",
    "\n",
    "\n",
    "    #Creating binary columns for the col \"label\" values\n",
    "    df['update_org'] = df['org'].apply(lambda x : 1 if (x in labelled_update) else 0)\n",
    "    df['personal_org'] = df['org'].apply(lambda x : 1 if (x in labelled_personal) else 0)\n",
    "    df['promotions_org'] = df['org'].apply(lambda x : 1 if (x in labelled_promotions) else 0)\n",
    "    df['forums_org'] = df['org'].apply(lambda x : 1 if (x in labelled_forums) else 0)\n",
    "    df['purchases_org'] = df['org'].apply(lambda x : 1 if (x in labelled_purchases) else 0)\n",
    "    df['travel_org'] = df['org'].apply(lambda x : 1 if (x in labelled_travel) else 0)\n",
    "    df['spam_org'] = df['org'].apply(lambda x : 1 if (x in labelled_spam) else 0)\n",
    "    df['social_org'] = df['org'].apply(lambda x : 1 if (x in labelled_social) else 0)\n",
    "    print(\"Organization set information complete\")\n",
    "\n",
    "    \n",
    "    return df\n",
    "    \n",
    "#Function to create columns for top-level domain wrt to the labels they have \n",
    "def tld_set_information(df):\n",
    "    #Find all the unique values in each subset\n",
    "    labelled_update = train_data[train_data['label']==0]['tld'].unique()\n",
    "    labelled_personal = train_data[train_data['label']==1]['tld'].unique()\n",
    "    labelled_promotions = train_data[train_data['label']==2]['tld'].unique()\n",
    "    labelled_forums = train_data[train_data['label']==3]['tld'].unique()\n",
    "    labelled_purchases = train_data[train_data['label']==4]['tld'].unique()\n",
    "    labelled_travel = train_data[train_data['label']==5]['tld'].unique()\n",
    "    labelled_spam = train_data[train_data['label']==6]['tld'].unique()\n",
    "    labelled_social = train_data[train_data['label']==7]['tld'].unique()\n",
    "\n",
    "\n",
    "    #Creating binary columns for the col \"label\" values\n",
    "    df['update_tld'] = df['tld'].apply(lambda x : 1 if (x in labelled_update) else 0)\n",
    "    df['personal_tld'] = df['tld'].apply(lambda x : 1 if (x in labelled_personal) else 0)\n",
    "    df['promotions_tld'] = df['tld'].apply(lambda x : 1 if (x in labelled_promotions) else 0)\n",
    "    df['forums_tld'] = df['tld'].apply(lambda x : 1 if (x in labelled_forums) else 0)\n",
    "    df['purchases_tld'] = df['tld'].apply(lambda x : 1 if (x in labelled_purchases) else 0)\n",
    "    df['travel_tld'] = df['tld'].apply(lambda x : 1 if (x in labelled_travel) else 0)\n",
    "    df['spam_tld'] = df['tld'].apply(lambda x : 1 if (x in labelled_spam) else 0)\n",
    "    df['social_tld'] = df['tld'].apply(lambda x : 1 if (x in labelled_social) else 0)\n",
    "    \n",
    "    print(\"TLD information set complete\")\n",
    "    return df\n",
    "    \n",
    "    \n",
    "#Function to create columns for sender wrt to the labels they have \n",
    "def sender_set_information(df):\n",
    "    update_set = set(train_data[train_data['label']==0]['sender'].unique())\n",
    "    social_set = set(train_data[train_data['label']==1]['sender'].unique())\n",
    "    forum_set = set(train_data[train_data['label']==2]['sender'].unique())\n",
    "    promo_set = set(train_data[train_data['label']==3]['sender'].unique())\n",
    "    \n",
    "    df['forum_sender'] = df['sender'].apply(lambda x : int(x in forum_set))\n",
    "    df['social_sender'] = df['sender'].apply(lambda x : int(x in social_set))\n",
    "    df['promo_sender'] = df['sender'].apply(lambda x : int(x in promo_set))\n",
    "    df['update_sender'] = df['sender'].apply(lambda x : int(x in update_set))\n",
    "    \n",
    "    df['update_sender_only'] = (df['update_sender'] * (1-df['promo_sender']) \n",
    "                                    * (1- df['social_sender']) * (1-df['forum_sender'])) \n",
    "    df['promo_sender_only'] = (df['promo_sender'] * (1-df['update_sender']) \n",
    "                                    * (1- df['social_sender']) * (1-df['forum_sender']))\n",
    "    \n",
    "    df['forum_sender_only'] = (df['forum_sender'] * (1-df['promo_sender']) \n",
    "                                    * (1- df['social_sender']) * (1-df['update_sender'])) \n",
    "    df['social_sender_only'] = (df['social_sender'] * (1-df['promo_sender']) \n",
    "                                    * (1- df['forum_sender']) * (1-df['update_sender'])) \n",
    "    df['promo_and_update_sender'] = df['promo_sender'] * df['update_sender'] \n",
    "    \n",
    "    #add the sender domain depth by counting the number of \".\" in sender\n",
    "    df['sender_depth'] = df['sender'].apply(lambda s : str(s).count('.'))\n",
    "    \n",
    "    print(\"Sender set information added\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here, we run the preprocessing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sender information added\n",
      "Sender information added\n",
      "Dates added\n",
      "Year added\n",
      "Month added\n",
      "Day added\n",
      "Weekday added\n",
      "Time added\n",
      "Hour-offset added\n",
      "Dates added\n",
      "Year added\n",
      "Month added\n",
      "Day added\n",
      "Weekday added\n",
      "Time added\n",
      "Hour-offset added\n",
      "Row deletion complete\n",
      "Row deletion complete\n",
      "Img/url complete\n",
      "Img/url complete\n",
      "Body/subject ratio added\n",
      "Body/subject ratio added\n",
      "HT-HS-binary encoding complete\n",
      "HT-HS-binary encoding complete\n",
      "Forward levels added\n",
      "Forward levels added\n",
      "No sender reported feature created\n",
      "No sender reported feature created\n",
      "No sender reported feature created\n",
      "No sender reported feature created\n",
      "Organization set information complete\n",
      "Organization set information complete\n",
      "TLD information set complete\n",
      "TLD information set complete\n",
      "Sender set information added\n",
      "Sender set information added\n"
     ]
    }
   ],
   "source": [
    "train_data = create_senders(train_data)\n",
    "test_data = create_senders(test_data)\n",
    "\n",
    "train_data = date_time_extraction(train_data)\n",
    "test_data = date_time_extraction(test_data)\n",
    "\n",
    "train_data = row_deleter(train_data)\n",
    "test_data = row_deleter(test_data)\n",
    "\n",
    "train_data = img_url_ratio(train_data)\n",
    "test_data = img_url_ratio(test_data)\n",
    "\n",
    "train_data = body_subject_ratio(train_data)\n",
    "test_data = body_subject_ratio(test_data)\n",
    "\n",
    "train_data = ht_hs_binary(train_data)\n",
    "test_data = ht_hs_binary(test_data)\n",
    "\n",
    "# train_data = mail_type_preprocessing(train_data)\n",
    "# test_data = mail_type_preprocessing(test_data)\n",
    "\n",
    "train_data = mailtype_dummy_vars(train_data)\n",
    "test_data = mailtype_dummy_vars(test_data)\n",
    "\n",
    "train_data = forward_level(train_data)\n",
    "test_data = forward_level(test_data)\n",
    "\n",
    "train_data = no_sender_reported(train_data)\n",
    "test_data = no_sender_reported(test_data)\n",
    "\n",
    "# train_data = dummy_transform(train_data, column_name='weekday')\n",
    "# test_data = dummy_transform(test_data, column_name='weekday')\n",
    "# train_data = dummy_transform(train_data, column_name='year')\n",
    "# test_data = dummy_transform(test_data, column_name='year')\n",
    "# train_data = dummy_transform(train_data, column_name='month')\n",
    "# test_data = dummy_transform(test_data, column_name='month')\n",
    "# train_data = dummy_transform(train_data, column_name='forward_level')\n",
    "# test_data = dummy_transform(test_data, column_name='forward_level')\n",
    "# train_data = dummy_transform(train_data, column_name='days')\n",
    "# test_data = dummy_transform(test_data, column_name='days')\n",
    "\n",
    "train_data = no_sender_reported(train_data)\n",
    "test_data = no_sender_reported(test_data)\n",
    "\n",
    "train_data = org_set_information(train_data)\n",
    "test_data = org_set_information(test_data)\n",
    "\n",
    "train_data = tld_set_information(train_data)\n",
    "test_data = tld_set_information(test_data)\n",
    "\n",
    "train_data = sender_set_information(train_data)\n",
    "test_data = sender_set_information(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No_sender_reported</th>\n",
       "      <td>6464</td>\n",
       "      <td>0.188098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender</th>\n",
       "      <td>1536</td>\n",
       "      <td>0.044697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org</th>\n",
       "      <td>1536</td>\n",
       "      <td>0.044697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tld</th>\n",
       "      <td>1536</td>\n",
       "      <td>0.044697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_offset</th>\n",
       "      <td>106</td>\n",
       "      <td>0.003085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_depth</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mail_type</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chars_in_body</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chars_in_subject</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>designation</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salutations</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urls</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>images</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_freq_prev_six_months</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_freq_prev_month</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body_subj_ratio</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_freq_prev_week</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_freq_prev_year</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_freq_total_period</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_mails_by_sender</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bcced</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ccs</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_url_ratio</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_subject</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promo_and_update_sender</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_body</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social_sender_only</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forum_sender_only</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promo_sender_only</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>update_sender_only</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>update_sender</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promo_sender</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social_sender</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forum_sender</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social_tld</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam_tld</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel_tld</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchases_tld</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forums_tld</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promotions_tld</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal_tld</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>update_tld</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchases_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forums_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promotions_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>update_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_level</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Total   Percent\n",
       "No_sender_reported            6464  0.188098\n",
       "sender                        1536  0.044697\n",
       "org                           1536  0.044697\n",
       "tld                           1536  0.044697\n",
       "hour_offset                    106  0.003085\n",
       "sender_depth                     0  0.000000\n",
       "mail_type                        0  0.000000\n",
       "date                             0  0.000000\n",
       "chars_in_body                    0  0.000000\n",
       "chars_in_subject                 0  0.000000\n",
       "designation                      0  0.000000\n",
       "salutations                      0  0.000000\n",
       "urls                             0  0.000000\n",
       "images                           0  0.000000\n",
       "sender_freq_prev_six_months      0  0.000000\n",
       "sender_freq_prev_month           0  0.000000\n",
       "body_subj_ratio                  0  0.000000\n",
       "sender_freq_prev_week            0  0.000000\n",
       "sender_freq_prev_year            0  0.000000\n",
       "sender_freq_total_period         0  0.000000\n",
       "total_mails_by_sender            0  0.000000\n",
       "bcced                            0  0.000000\n",
       "ccs                              0  0.000000\n",
       "time                             0  0.000000\n",
       "weekday                          0  0.000000\n",
       "day                              0  0.000000\n",
       "month                            0  0.000000\n",
       "year                             0  0.000000\n",
       "img_url_ratio                    0  0.000000\n",
       "has_subject                      0  0.000000\n",
       "promo_and_update_sender          0  0.000000\n",
       "has_body                         0  0.000000\n",
       "social_sender_only               0  0.000000\n",
       "forum_sender_only                0  0.000000\n",
       "promo_sender_only                0  0.000000\n",
       "update_sender_only               0  0.000000\n",
       "update_sender                    0  0.000000\n",
       "promo_sender                     0  0.000000\n",
       "social_sender                    0  0.000000\n",
       "forum_sender                     0  0.000000\n",
       "social_tld                       0  0.000000\n",
       "spam_tld                         0  0.000000\n",
       "travel_tld                       0  0.000000\n",
       "purchases_tld                    0  0.000000\n",
       "forums_tld                       0  0.000000\n",
       "promotions_tld                   0  0.000000\n",
       "personal_tld                     0  0.000000\n",
       "update_tld                       0  0.000000\n",
       "social_org                       0  0.000000\n",
       "spam_org                         0  0.000000\n",
       "travel_org                       0  0.000000\n",
       "purchases_org                    0  0.000000\n",
       "forums_org                       0  0.000000\n",
       "promotions_org                   0  0.000000\n",
       "personal_org                     0  0.000000\n",
       "update_org                       0  0.000000\n",
       "forward_level                    0  0.000000\n",
       "datetime                         0  0.000000"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-checking the missing data\n",
    "missing_data_calculator(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2 Features \n",
    "\n",
    "We decided to add in more information about the emails, such as aggregate information about the emails, the frequency count of the email, and creating more dummy columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z-score lambda function that will help to create a z-score feature for certain numeric columns \n",
    "zscore = lambda x: (x - x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in missing data\n",
    "train_data['img_url_ratio'] = train_data['img_url_ratio'].fillna(0)\n",
    "train_data['body_subj_ratio'] = train_data['img_url_ratio'].fillna(0)\n",
    "\n",
    "test_data['img_url_ratio'] = test_data['img_url_ratio'].fillna(0)\n",
    "test_data['body_subj_ratio'] = test_data['img_url_ratio'].fillna(0)\n",
    "\n",
    "#Replace all infinity values with 0 \n",
    "train_data = train_data.replace([np.inf, -np.inf],0)\n",
    "test_data = test_data.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>hour_offset</th>\n",
       "      <th>ccs</th>\n",
       "      <th>bcced</th>\n",
       "      <th>total_mails_by_sender</th>\n",
       "      <th>...</th>\n",
       "      <th>forum_sender</th>\n",
       "      <th>social_sender</th>\n",
       "      <th>promo_sender</th>\n",
       "      <th>update_sender</th>\n",
       "      <th>update_sender_only</th>\n",
       "      <th>promo_sender_only</th>\n",
       "      <th>forum_sender_only</th>\n",
       "      <th>social_sender_only</th>\n",
       "      <th>promo_and_update_sender</th>\n",
       "      <th>sender_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-06 11:13:45+01:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>11:13:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.252945</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-14 12:00:16+01:00</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>12:00:16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.399657</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-06 21:53:37+02:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>21:53:37</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.926883</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-11 11:25:40+02:00</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>11:25:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.094262</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-07 12:07:18+01:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>12:07:18</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444660</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80162</th>\n",
       "      <td>2017-11-20 16:48:47+01:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>16:48:47</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.029957</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80165</th>\n",
       "      <td>2020-04-25 14:20:07+02:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>14:20:07</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821920</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80169</th>\n",
       "      <td>2017-07-19 02:24:33+02:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>02:24:33</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.228967</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80171</th>\n",
       "      <td>2020-06-30 14:55:20+02:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>14:55:20</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315932</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80173</th>\n",
       "      <td>2020-05-26 17:18:43+02:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>17:18:43</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.836222</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49140 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        datetime  year  month  day  weekday      time  \\\n",
       "0      2017-11-06 11:13:45+01:00  2017     11    6        0  11:13:45   \n",
       "1      2018-02-14 12:00:16+01:00  2018      2   14        2  12:00:16   \n",
       "2      2016-07-06 21:53:37+02:00  2016      7    6        2  21:53:37   \n",
       "3      2019-10-11 11:25:40+02:00  2019     10   11        4  11:25:40   \n",
       "4      2017-11-07 12:07:18+01:00  2017     11    7        1  12:07:18   \n",
       "...                          ...   ...    ...  ...      ...       ...   \n",
       "80162  2017-11-20 16:48:47+01:00  2017     11   20        0  16:48:47   \n",
       "80165  2020-04-25 14:20:07+02:00  2020      4   25        5  14:20:07   \n",
       "80169  2017-07-19 02:24:33+02:00  2017      7   19        2  02:24:33   \n",
       "80171  2020-06-30 14:55:20+02:00  2020      6   30        1  14:55:20   \n",
       "80173  2020-05-26 17:18:43+02:00  2020      5   26        1  17:18:43   \n",
       "\n",
       "       hour_offset  ccs  bcced  total_mails_by_sender  ...  forum_sender  \\\n",
       "0              0.0    0      0              -0.252945  ...             1   \n",
       "1             -1.0    0      0              -1.399657  ...             1   \n",
       "2             -2.0    0      0               0.926883  ...             1   \n",
       "3              0.0    0      0              -0.094262  ...             1   \n",
       "4             -1.0    1      0              -0.444660  ...             0   \n",
       "...            ...  ...    ...                    ...  ...           ...   \n",
       "80162         17.0    0      0              -1.029957  ...             1   \n",
       "80165         15.0    0      0               0.821920  ...             1   \n",
       "80169         -2.0    0      0               0.228967  ...             1   \n",
       "80171         15.0    1      0               0.315932  ...             0   \n",
       "80173         15.0    0      0               0.836222  ...             1   \n",
       "\n",
       "       social_sender  promo_sender  update_sender update_sender_only  \\\n",
       "0                  1             0              1                  0   \n",
       "1                  1             0              1                  0   \n",
       "2                  1             0              1                  0   \n",
       "3                  1             1              1                  0   \n",
       "4                  1             1              1                  0   \n",
       "...              ...           ...            ...                ...   \n",
       "80162              1             0              0                  0   \n",
       "80165              1             0              1                  0   \n",
       "80169              1             0              1                  0   \n",
       "80171              1             1              1                  0   \n",
       "80173              1             0              1                  0   \n",
       "\n",
       "       promo_sender_only  forum_sender_only  social_sender_only  \\\n",
       "0                      0                  0                   0   \n",
       "1                      0                  0                   0   \n",
       "2                      0                  0                   0   \n",
       "3                      0                  0                   0   \n",
       "4                      0                  0                   0   \n",
       "...                  ...                ...                 ...   \n",
       "80162                  0                  0                   0   \n",
       "80165                  0                  0                   0   \n",
       "80169                  0                  0                   0   \n",
       "80171                  0                  0                   0   \n",
       "80173                  0                  0                   0   \n",
       "\n",
       "       promo_and_update_sender  sender_depth  \n",
       "0                            0             2  \n",
       "1                            0             2  \n",
       "2                            0             1  \n",
       "3                            1             1  \n",
       "4                            1             1  \n",
       "...                        ...           ...  \n",
       "80162                        0             1  \n",
       "80165                        0             1  \n",
       "80169                        0             1  \n",
       "80171                        1             1  \n",
       "80173                        0             1  \n",
       "\n",
       "[49140 rows x 59 columns]"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The processed train-data in action \n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing missing values in 'hour_offset' with -100\n",
    "train_data['hour_offset'] = train_data['hour_offset'].fillna(-100)\n",
    "test_data['hour_offset'] = test_data['hour_offset'].fillna(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the mising org, tld, sender for \"org\",\"tld\" and \"sender\" with \"DNE\"\n",
    "\n",
    "train_data['org'] = train_data['org'].fillna(\"DNE\")\n",
    "test_data['org'] = test_data['org'].fillna(\"DNE\")\n",
    "\n",
    "train_data['tld'] = train_data['tld'].fillna(\"DNE\")\n",
    "test_data['tld'] = test_data['tld'].fillna(\"DNE\")\n",
    "\n",
    "train_data['sender'] = train_data['sender'].fillna(\"DNE\")\n",
    "test_data['sender'] = test_data['sender'].fillna(\"DNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hour-offset is not a numeric column, so converting it to categorical\n",
    "train_data['hour_offset'] = train_data['hour_offset'].astype('category')\n",
    "test_data['hour_offset'] = test_data['hour_offset'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby transformations on the train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.346939\n",
       "1        0.306569\n",
       "2        0.183168\n",
       "3        0.198473\n",
       "4        0.342466\n",
       "           ...   \n",
       "80162    0.275229\n",
       "80165    0.130841\n",
       "80169    0.226027\n",
       "80171    0.200000\n",
       "80173    0.203125\n",
       "Name: ccs, Length: 49140, dtype: float64"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in item, by grouping them on the basis \n",
    "#of their sender\n",
    "for item in ['images','urls','chars_in_subject','chars_in_body','total_mails_by_sender',\n",
    "             \"img_url_ratio\",\"body_subj_ratio\"]:\n",
    "    for indicator in ['mean', 'std','skew','count','median',zscores]:\n",
    "        data_sender = train_data.groupby('sender')[item].transform(indicator).rename(f'sender_{item}_{indicator}')\n",
    "        train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "        \n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their organization\n",
    "for item in ['images','urls','chars_in_subject','chars_in_body','total_mails_by_sender',\n",
    "             \"img_url_ratio\",\"body_subj_ratio\"]:\n",
    "    for indicator in ['mean', 'std','skew','count','median',zscores]:\n",
    "        data_sender = train_data.groupby('org')[item].transform(indicator).rename(f'org_{item}_{indicator}')\n",
    "        train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "        \n",
    "        \n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their top-level domain     \n",
    "for item in ['images','urls','chars_in_subject','chars_in_body','total_mails_by_sender',\n",
    "             \"img_url_ratio\",\"body_subj_ratio\"]:\n",
    "    for indicator in ['mean', 'std','skew','count','median',zscores]:\n",
    "        data_sender = train_data.groupby('tld')[item].transform(indicator).rename(f'tld_{item}_{indicator}')\n",
    "        train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "        \n",
    "        \n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their sender and the month on which the email was sent.\n",
    "for item in ['chars_in_subject','chars_in_body','total_mails_by_sender']:\n",
    "    for indicator in ['mean', 'std','count']:\n",
    "        data_sender = train_data.groupby(['month','sender'])[item].transform(indicator).rename(f'month-sender_{item}_{indicator}')\n",
    "        train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "        \n",
    "        \n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their sender and the weekday on which the email was sent.\n",
    "for item in ['chars_in_subject','chars_in_body','total_mails_by_sender']:\n",
    "    for indicator in ['mean', 'std','count']:\n",
    "        data_sender = train_data.groupby(['weekday','sender'])[item].transform(indicator).rename(f'weekday-sender_{item}_{indicator}')\n",
    "        train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "        \n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their sender and the year on which the email was sent.\n",
    "for item in ['chars_in_subject','chars_in_body','total_mails_by_sender']:\n",
    "    for indicator in ['mean', 'std','count']:\n",
    "        data_sender = train_data.groupby(['year','sender'])[item].transform(indicator).rename(f'year-sender_{item}_{indicator}')\n",
    "        train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "        \n",
    "        \n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their sender and the day on which the email was sent.\n",
    "for item in ['chars_in_subject','chars_in_body','total_mails_by_sender']:\n",
    "    for indicator in ['mean', 'std','count']:\n",
    "        data_sender = train_data.groupby(['day','sender'])[item].transform(indicator).rename(f'day-sender_{item}_{indicator}')\n",
    "        train_data = pd.concat([train_data, data_sender], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby transformations on the test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in item, by grouping them on the basis \n",
    "#of their sender\n",
    "for item in ['images','urls','chars_in_subject','chars_in_body','total_mails_by_sender',\n",
    "             \"img_url_ratio\",\"body_subj_ratio\"]:\n",
    "    for indicator in ['mean', 'std','skew','count','median',zscores]:\n",
    "        data_sender = test_data.groupby('sender')[item].transform(indicator).rename(f'sender_{item}_{indicator}')\n",
    "        test_data = pd.concat([test_data, data_sender], axis=1)\n",
    "             \n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their organization\n",
    "for item in ['images','urls','chars_in_subject','chars_in_body','total_mails_by_sender',\n",
    "             \"img_url_ratio\",\"body_subj_ratio\"]:\n",
    "    for indicator in ['mean', 'std','skew','count','median',zscores]:\n",
    "        data_sender = test_data.groupby('org')[item].transform(indicator).rename(f'org_{item}_{indicator}')\n",
    "        test_data = pd.concat([test_data, data_sender], axis=1)\n",
    "        \n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their top-level domain    \n",
    "for item in ['images','urls','chars_in_subject','chars_in_body','total_mails_by_sender',\n",
    "             \"img_url_ratio\",\"body_subj_ratio\"]:\n",
    "    for indicator in ['mean', 'std','skew','count','median',zscores]:\n",
    "        data_sender = test_data.groupby('tld')[item].transform(indicator).rename(f'tld_{item}_{indicator}')\n",
    "        test_data = pd.concat([test_data, data_sender], axis=1)\n",
    "           \n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their sender and the month on which the email was sent.            \n",
    "for item in ['chars_in_subject','chars_in_body','total_mails_by_sender']:\n",
    "    for indicator in ['mean', 'std','count']:\n",
    "        data_sender = test_data.groupby(['month','sender'])[item].transform(indicator).rename(f'month-sender_{item}_{indicator}')\n",
    "        test_data = pd.concat([test_data, data_sender], axis=1)\n",
    "\n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their sender and the weekday on which the email was sent.\n",
    "for item in ['chars_in_subject','chars_in_body','total_mails_by_sender']:\n",
    "    for indicator in ['mean', 'std','count']:\n",
    "        data_sender = test_data.groupby(['weekday','sender'])[item].transform(indicator).rename(f'weekday-sender_{item}_{indicator}')\n",
    "        test_data = pd.concat([test_data, data_sender], axis=1)\n",
    "\n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their sender and the year on which the email was sent.\n",
    "for item in ['chars_in_subject','chars_in_body','total_mails_by_sender']:\n",
    "    for indicator in ['mean','std','count']:\n",
    "        data_sender = test_data.groupby(['year','sender'])[item].transform(indicator).rename(f'year-sender_{item}_{indicator}')\n",
    "        test_data = pd.concat([test_data, data_sender], axis=1)\n",
    "  \n",
    "#Calculate the mean, std, skewness, count, median and zscores for all the columns in \"item\", by grouping them on the basis \n",
    "#of their sender and the day on which the email was sent.\n",
    "for item in ['chars_in_subject','chars_in_body','total_mails_by_sender']:\n",
    "    for indicator in ['mean','std','count']:\n",
    "        data_sender = test_data.groupby(['day','sender'])[item].transform(indicator).rename(f'day-sender_{item}_{indicator}')\n",
    "        test_data = pd.concat([test_data, data_sender], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, further feature transformations were done for $\\texttt{img_url_ratio}$ and $\\texttt{body_subj_ratio}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ['images','urls','chars_in_subject','chars_in_body']:\n",
    "    for indicator in ['mean', 'std','skew',zscores]:\n",
    "        data_sender = train_data.groupby('img_url_ratio')[item].transform(indicator).rename(f'img_url_ratio_{item}_{indicator}')\n",
    "        train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "        \n",
    "for item in ['images','urls','chars_in_subject','chars_in_body']:\n",
    "    for indicator in ['mean', 'std','skew',zscores]:\n",
    "        data_sender = train_data.groupby('body_subj_ratio')[item].transform(indicator).rename(f'body_subj_ratio_{item}_{indicator}')\n",
    "        train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "        \n",
    "        \n",
    "for item in ['images','urls','chars_in_subject','chars_in_body']:\n",
    "    for indicator in ['mean', 'std','skew',zscores]:\n",
    "        data_sender = test_data.groupby('img_url_ratio')[item].transform(indicator).rename(f'img_url_ratio_{item}_{indicator}')\n",
    "        test_data = pd.concat([test_data, data_sender], axis=1)\n",
    "        \n",
    "for item in ['images','urls','chars_in_subject','chars_in_body']:\n",
    "    for indicator in ['mean', 'std','skew',zscores]:\n",
    "        data_sender = test_data.groupby('body_subj_ratio')[item].transform(indicator).rename(f'body_subj_ratio_{item}_{indicator}')\n",
    "        test_data = pd.concat([test_data, data_sender], axis=1)\n",
    "        \n",
    "\n",
    "for indicator in ['mean', 'std','skew',zscores]:\n",
    "    data_sender = train_data.groupby('year')['sender_freq_prev_year'].transform(indicator).rename(f'sender_freq_year_{indicator}')\n",
    "    train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "\n",
    "for indicator in ['mean', 'std','skew',zscores]:\n",
    "    data_sender = test_data.groupby('year')['sender_freq_prev_year'].transform(indicator).rename(f'sender_freq_year_{indicator}')\n",
    "    test_data = pd.concat([test_data, data_sender], axis=1)\n",
    "    \n",
    "    \n",
    "for indicator in ['mean', 'std','skew',zscores]:\n",
    "    data_sender = train_data.groupby('weekday')['sender_freq_prev_week'].transform(indicator).rename(f'sender_freq_week_{indicator}')\n",
    "    train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "\n",
    "for indicator in ['mean', 'std','skew',zscores]:\n",
    "    data_sender = test_data.groupby('weekday')['sender_freq_prev_week'].transform(indicator).rename(f'sender_freq_week_{indicator}')\n",
    "    test_data = pd.concat([test_data, data_sender], axis=1)\n",
    "\n",
    "    \n",
    "for indicator in ['mean', 'std','skew',zscores]:\n",
    "    data_sender = train_data.groupby('month')['sender_freq_prev_month'].transform(indicator).rename(f'sender_freq_month_{indicator}')\n",
    "    train_data = pd.concat([train_data, data_sender], axis=1)\n",
    "\n",
    "for indicator in ['mean', 'std','skew',zscores]:\n",
    "    data_sender = test_data.groupby('month')['sender_freq_prev_month'].transform(indicator).rename(f'sender_freq_month_{indicator}')\n",
    "    test_data = pd.concat([test_data, data_sender], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to remove after the processing is complete \n",
    "cols_to_remove = [\"datetime\",\"time\",\"org\",\"tld\",\"sender\"]\n",
    "train_data = train_data.drop(cols_to_remove, axis=1)\n",
    "test_data = test_data.drop(cols_to_remove, axis=1)\n",
    "train_data  = train_data.drop(['date'], axis=1)\n",
    "test_data  = test_data.drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dummy columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy variable transformation for mail_type complete\n",
      "Dummy variable transformation for mail_type complete\n"
     ]
    }
   ],
   "source": [
    "train_data = dummy_transform(train_data, column_name='mail_type')\n",
    "test_data = dummy_transform(test_data, column_name='mail_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy variable transformation for year complete\n",
      "Dummy variable transformation for year complete\n"
     ]
    }
   ],
   "source": [
    "train_data = dummy_transform(train_data, column_name='year')\n",
    "test_data = dummy_transform(test_data, column_name='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy variable transformation for day complete\n",
      "Dummy variable transformation for day complete\n"
     ]
    }
   ],
   "source": [
    "train_data = dummy_transform(train_data, column_name='day')\n",
    "test_data = dummy_transform(test_data, column_name='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy variable transformation for month complete\n",
      "Dummy variable transformation for month complete\n"
     ]
    }
   ],
   "source": [
    "train_data = dummy_transform(train_data, column_name='month')\n",
    "test_data = dummy_transform(test_data, column_name='month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = test_data.drop('mail_type_-1',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>month_12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_images_&lt;function &lt;lambda&gt; at 0x00000224B8D13798&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_urls_std</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_urls_skew</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_urls_count</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchases_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social_org</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>update_tld</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Total  Percent\n",
       "month_12                                                0      0.0\n",
       "sender_images_<function <lambda> at 0x00000224B...      0      0.0\n",
       "sender_urls_std                                         0      0.0\n",
       "sender_urls_skew                                        0      0.0\n",
       "sender_urls_count                                       0      0.0\n",
       "...                                                   ...      ...\n",
       "purchases_org                                           0      0.0\n",
       "travel_org                                              0      0.0\n",
       "spam_org                                                0      0.0\n",
       "social_org                                              0      0.0\n",
       "update_tld                                              0      0.0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data_calculator(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-597-d2d0a08d7a38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, to_replace, value, inplace, limit, regex, method)\u001b[0m\n\u001b[0;32m   4170\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4171\u001b[0m             \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4172\u001b[1;33m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4173\u001b[0m         )\n\u001b[0;32m   4174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, to_replace, value, inplace, limit, regex, method)\u001b[0m\n\u001b[0;32m   6702\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# [NA, ''] -> 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6703\u001b[0m                     new_data = self._data.replace(\n\u001b[1;32m-> 6704\u001b[1;33m                         \u001b[0mto_replace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_replace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6705\u001b[0m                     )\n\u001b[0;32m   6706\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mto_replace\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, value, **kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplace_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, filter, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, to_replace, value, inplace, filter, regex, convert)\u001b[0m\n\u001b[0;32m   2956\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2957\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# replace was called on a series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2958\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_replace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2959\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2960\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, to_replace, value, inplace)\u001b[0m\n\u001b[0;32m   2442\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inplace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m         \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2444\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mto_replace\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2445\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m                 \u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_categories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_replace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\numeric.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3898\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_index_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"contains\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_index_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3899\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3900\u001b[1;33m         \u001b[0mhash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3902\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "train_data = train_data.fillna(0)\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "train_data = train_data.replace([np.inf, -np.inf], 0)\n",
    "test_data = test_data.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.fillna(0)\n",
    "test_data = test_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = pd.get_dummies(train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def skew_autotransform(DF, include = None, exclude = None, plot = False, threshold = 1, exp = False):\n",
    "    \n",
    "    #Get list of column names that should be processed based on input parameters\n",
    "    if include is None and exclude is None:\n",
    "        colnames = DF.columns.values\n",
    "    elif include is not None:\n",
    "        colnames = include\n",
    "    elif exclude is not None:\n",
    "        colnames = [item for item in list(DF.columns.values) if item not in exclude]\n",
    "    else:\n",
    "        print('No columns to process!')\n",
    "    \n",
    "    #Helper function that checks if all values are positive\n",
    "    def make_positive(series):\n",
    "        minimum = np.amin(series)\n",
    "        #If minimum is negative, offset all values by a constant to move all values to positive teritory\n",
    "        if minimum <= 0:\n",
    "            series = series + abs(minimum) + 0.01\n",
    "        return series\n",
    "    \n",
    "    \n",
    "    #Go throug desired columns in DataFrame\n",
    "    for col in colnames:\n",
    "        #Get column skewness\n",
    "        skew = DF[col].skew()\n",
    "        transformed = True\n",
    "        \n",
    "        if plot:\n",
    "            #Prep the plot of original data\n",
    "            sns.set_style(\"darkgrid\")\n",
    "            sns.set_palette(\"Blues_r\")\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax1 = sns.distplot(DF[col], ax=axes[0])\n",
    "            ax1.set(xlabel='Original ' + col)\n",
    "        \n",
    "        #If skewness is larger than threshold and positively skewed; If yes, apply appropriate transformation\n",
    "        if abs(skew) > threshold and skew > 0:\n",
    "            skewType = 'positive'\n",
    "            #Make sure all values are positive\n",
    "            DF[col] = make_positive(DF[col])\n",
    "            \n",
    "            if exp:\n",
    "               #Apply log transformation \n",
    "               DF[col] = DF[col].apply(math.log)\n",
    "            else:\n",
    "                #Apply boxcox transformation\n",
    "                DF[col] = ss.boxcox(DF[col])[0]\n",
    "            skew_new = DF[col].skew()\n",
    "         \n",
    "        elif abs(skew) > threshold and skew < 0:\n",
    "            skewType = 'negative'\n",
    "            #Make sure all values are positive\n",
    "            DF[col] = make_positive(DF[col])\n",
    "            \n",
    "            if exp:\n",
    "               #Apply exp transformation \n",
    "               DF[col] = DF[col].pow(10)\n",
    "            else:\n",
    "                #Apply boxcox transformation\n",
    "                DF[col] = ss.boxcox(DF[col])[0]\n",
    "            skew_new = DF[col].skew()\n",
    "        \n",
    "        else:\n",
    "            #Flag if no transformation was performed\n",
    "            transformed = False\n",
    "            skew_new = skew\n",
    "        \n",
    "        #Compare before and after if plot is True\n",
    "        if plot:\n",
    "            print('\\n ------------------------------------------------------')     \n",
    "            if transformed:\n",
    "                print('\\n %r had %r skewness of %2.2f' %(col, skewType, skew))\n",
    "                print('\\n Transformation yielded skewness of %2.2f' %(skew_new))\n",
    "                sns.set_palette(\"Paired\")\n",
    "                ax2 = sns.distplot(DF[col], ax=axes[1], color = 'r')\n",
    "                ax2.set(xlabel='Transformed ' + col)\n",
    "                plt.show()\n",
    "            else:\n",
    "                print('\\n NO TRANSFORMATION APPLIED FOR %r . Skewness = %2.2f' %(col, skew))\n",
    "                ax2 = sns.distplot(DF[col], ax=axes[1])\n",
    "                ax2.set(xlabel='NO TRANSFORM ' + col)\n",
    "                plt.show()\n",
    "                \n",
    "\n",
    "    return DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Modelling \n",
    "\n",
    "Our step was to go with an initial modelling of the data by using a basic ML model, such as Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler, StandardScaler\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import fbeta_score, precision_score, make_scorer, average_precision_score\n",
    "# import cv2\n",
    "import warnings\n",
    "\n",
    "\n",
    "#Create a train and test partition \n",
    "X_train = train_data.drop('label', axis=1)\n",
    "y_train = train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using StandardScaler to standardize the data and make the distribution close to norma\n",
    "mms = StandardScaler()\n",
    "X_train_scaled = mms.fit_transform(X_train)\n",
    "X_test_scaled = mms.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=3, n_estimators=500)"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting RF with 500 trees and a depth of 3\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=3)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int64)"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sauraj (Work mode)\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:71: FutureWarning: Pass classes=[0 1 2 3 4 5 6 7], y=0        2\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "80162    2\n",
      "80165    0\n",
      "80169    1\n",
      "80171    3\n",
      "80173    0\n",
      "Name: label, Length: 48145, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Using LGBM for training on the data (Boosting approach)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Custom F1-metric score for calculating F1 score during 5-fold CV on LGBM\n",
    "def f1_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = preds.reshape(len(np.unique(labels)), -1)\n",
    "    preds = preds.T.argmax(axis = 1)\n",
    "    f_score = f1_score(preds, labels, average=\"micro\")\n",
    "    return 'f1_score', f_score, True\n",
    "\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#MinMax scaling done for LGBM since LGBM performs best when the data is scaled to unit length \n",
    "mms = MinMaxScaler()\n",
    "X_train_scaled = mms.fit_transform(X_train)\n",
    "X_test_scaled = mms.transform(X_test)\n",
    "\n",
    "#PCA applied to reduce the dimanesions of the dataset (We chose 75 components that captured more than 90% variance.)\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pca = PCA(n_components=75)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "#Because we have a class imbalance problem, we also \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = list(compute_class_weight('balanced',\n",
    "                                             np.unique(y_train),\n",
    "                                             y_train))\n",
    "\n",
    "w_array = np.ones(y_train.shape[0])\n",
    "for i, val in enumerate(y_train):\n",
    "    w_array[i] = class_weights[val-1]\n",
    "\n",
    "\n",
    "dataset = lgb.Dataset(X_train, label=y_train)\n",
    "# test_dataset = lgb.Dataset(X_test)\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'objective': 'multiclass',\n",
    "          'n_jobs': -1,\n",
    "          'num_leaves': 1000,\n",
    "          'learning_rate': 0.026623466966581126,\n",
    "          'max_depth': 500,\n",
    "          'num_iterations': 1000,\n",
    "          'lambda_l1': 2.959759088169741,\n",
    "#           'lambda_l2': 1.331172832164913,\n",
    "          'bagging_fraction': 0.9655406551472153,\n",
    "          'bagging_freq': 5,\n",
    "          'num_class': 8,\n",
    "          'colsample_bytree': 0.6867118652742716,\n",
    "         'learning_rate': 0.01}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#parameters dict for LightGBM\n",
    "lgb_params =  {\n",
    "    'boosting': 'gbdt', \n",
    "    'colsample_bytree': 1, \n",
    "    'class_weight':class_weights,\n",
    "    'learning_rate': 0.01, \n",
    "    'max_depth': 200, \n",
    "    'min_child_samples': 100, \n",
    "    'n_iterations':100,\n",
    "    'n_estimators': 1000, \n",
    "    'num_leaves': 500,  \n",
    "    'objective': 'multiclass',\n",
    "    'num_class':8,\n",
    "    'reg_alpha': 0.6, \n",
    "    'reg_lambda': 0.3, \n",
    "    'subsample': 0.8,\n",
    "    'verbose':1\n",
    "    }\n",
    "\n",
    "\n",
    "params_2 = {\n",
    "    'application': 'multiclass', # for binary classification\n",
    "    'num_class' : 8, # used for multi-classes\n",
    "    'boosting_type': 'goss', # traditional gradient boosting decision tree\n",
    "    'num_iterations': 100, \n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 300,\n",
    "    'device': 'cpu', # you can use GPU to achieve faster learning\n",
    "    'max_depth': 0, # <0 means no limit\n",
    "    # Small number of bins may reduce training accuracy but can deal with over-fitting\n",
    "    'lambda_l1': 2, # L1 regularization\n",
    "    'lambda_l2': 3, # L2 regularization\n",
    "    'subsample_for_bin': 300, # number of samples for constructing bins\n",
    "    'subsample': 1, # subsample ratio of the training instance\n",
    "    'colsample_bytree': 0.8, # subsample ratio of columns when constructing the tree\n",
    "    'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n",
    "    'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n",
    "    'min_child_samples': 5# minimum number of data needed in a leaf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sauraj (Work mode)\\Miniconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's multi_logloss: 1.40541\ttraining's f1_score: 0.455811\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[2]\ttraining's multi_logloss: 1.39276\ttraining's f1_score: 0.455811\n",
      "[3]\ttraining's multi_logloss: 1.38069\ttraining's f1_score: 0.455811\n",
      "[4]\ttraining's multi_logloss: 1.36901\ttraining's f1_score: 0.455811\n",
      "[5]\ttraining's multi_logloss: 1.35777\ttraining's f1_score: 0.455811\n",
      "[6]\ttraining's multi_logloss: 1.34691\ttraining's f1_score: 0.455811\n",
      "[7]\ttraining's multi_logloss: 1.33638\ttraining's f1_score: 0.455811\n",
      "[8]\ttraining's multi_logloss: 1.32619\ttraining's f1_score: 0.455811\n",
      "[9]\ttraining's multi_logloss: 1.31624\ttraining's f1_score: 0.455811\n",
      "[10]\ttraining's multi_logloss: 1.30655\ttraining's f1_score: 0.455811\n",
      "[11]\ttraining's multi_logloss: 1.2971\ttraining's f1_score: 0.455811\n",
      "[12]\ttraining's multi_logloss: 1.28787\ttraining's f1_score: 0.455811\n",
      "[13]\ttraining's multi_logloss: 1.27887\ttraining's f1_score: 0.455811\n",
      "[14]\ttraining's multi_logloss: 1.27005\ttraining's f1_score: 0.455811\n",
      "[15]\ttraining's multi_logloss: 1.26143\ttraining's f1_score: 0.455811\n",
      "[16]\ttraining's multi_logloss: 1.25299\ttraining's f1_score: 0.455811\n",
      "[17]\ttraining's multi_logloss: 1.24473\ttraining's f1_score: 0.455811\n",
      "[18]\ttraining's multi_logloss: 1.23664\ttraining's f1_score: 0.455811\n",
      "[19]\ttraining's multi_logloss: 1.22868\ttraining's f1_score: 0.455811\n",
      "[20]\ttraining's multi_logloss: 1.22091\ttraining's f1_score: 0.455811\n",
      "[21]\ttraining's multi_logloss: 1.21327\ttraining's f1_score: 0.455811\n",
      "[22]\ttraining's multi_logloss: 1.20578\ttraining's f1_score: 0.455811\n",
      "[23]\ttraining's multi_logloss: 1.19842\ttraining's f1_score: 0.455811\n",
      "[24]\ttraining's multi_logloss: 1.19121\ttraining's f1_score: 0.455811\n",
      "[25]\ttraining's multi_logloss: 1.18415\ttraining's f1_score: 0.455811\n",
      "[26]\ttraining's multi_logloss: 1.17721\ttraining's f1_score: 0.472531\n",
      "[27]\ttraining's multi_logloss: 1.17036\ttraining's f1_score: 0.490227\n",
      "[28]\ttraining's multi_logloss: 1.16362\ttraining's f1_score: 0.496625\n",
      "[29]\ttraining's multi_logloss: 1.157\ttraining's f1_score: 0.49974\n",
      "[30]\ttraining's multi_logloss: 1.15049\ttraining's f1_score: 0.501589\n",
      "[31]\ttraining's multi_logloss: 1.14408\ttraining's f1_score: 0.502814\n",
      "[32]\ttraining's multi_logloss: 1.13777\ttraining's f1_score: 0.504891\n",
      "[33]\ttraining's multi_logloss: 1.13156\ttraining's f1_score: 0.509586\n",
      "[34]\ttraining's multi_logloss: 1.12543\ttraining's f1_score: 0.511683\n",
      "[35]\ttraining's multi_logloss: 1.11939\ttraining's f1_score: 0.515422\n",
      "[36]\ttraining's multi_logloss: 1.11343\ttraining's f1_score: 0.517333\n",
      "[37]\ttraining's multi_logloss: 1.10755\ttraining's f1_score: 0.518745\n",
      "[38]\ttraining's multi_logloss: 1.10177\ttraining's f1_score: 0.520303\n",
      "[39]\ttraining's multi_logloss: 1.09608\ttraining's f1_score: 0.522069\n",
      "[40]\ttraining's multi_logloss: 1.09047\ttraining's f1_score: 0.523087\n",
      "[41]\ttraining's multi_logloss: 1.08496\ttraining's f1_score: 0.523834\n",
      "[42]\ttraining's multi_logloss: 1.07951\ttraining's f1_score: 0.527988\n",
      "[43]\ttraining's multi_logloss: 1.07414\ttraining's f1_score: 0.543753\n",
      "[44]\ttraining's multi_logloss: 1.06883\ttraining's f1_score: 0.546557\n",
      "[45]\ttraining's multi_logloss: 1.06358\ttraining's f1_score: 0.550691\n",
      "[46]\ttraining's multi_logloss: 1.05841\ttraining's f1_score: 0.551916\n",
      "[47]\ttraining's multi_logloss: 1.05333\ttraining's f1_score: 0.555177\n",
      "[48]\ttraining's multi_logloss: 1.0483\ttraining's f1_score: 0.558604\n",
      "[49]\ttraining's multi_logloss: 1.04333\ttraining's f1_score: 0.560266\n",
      "[50]\ttraining's multi_logloss: 1.03846\ttraining's f1_score: 0.562696\n",
      "[51]\ttraining's multi_logloss: 1.03362\ttraining's f1_score: 0.565292\n",
      "[52]\ttraining's multi_logloss: 1.02885\ttraining's f1_score: 0.567556\n",
      "[53]\ttraining's multi_logloss: 1.02414\ttraining's f1_score: 0.569883\n",
      "[54]\ttraining's multi_logloss: 1.01948\ttraining's f1_score: 0.572313\n",
      "[55]\ttraining's multi_logloss: 1.01488\ttraining's f1_score: 0.573954\n",
      "[56]\ttraining's multi_logloss: 1.01032\ttraining's f1_score: 0.575595\n",
      "[57]\ttraining's multi_logloss: 1.00583\ttraining's f1_score: 0.57763\n",
      "[58]\ttraining's multi_logloss: 1.00139\ttraining's f1_score: 0.580434\n",
      "[59]\ttraining's multi_logloss: 0.997008\ttraining's f1_score: 0.581847\n",
      "[60]\ttraining's multi_logloss: 0.992685\ttraining's f1_score: 0.583217\n",
      "[61]\ttraining's multi_logloss: 0.98841\ttraining's f1_score: 0.584754\n",
      "[62]\ttraining's multi_logloss: 0.984186\ttraining's f1_score: 0.586748\n",
      "[63]\ttraining's multi_logloss: 0.980023\ttraining's f1_score: 0.588555\n",
      "[64]\ttraining's multi_logloss: 0.975928\ttraining's f1_score: 0.590217\n",
      "[65]\ttraining's multi_logloss: 0.971852\ttraining's f1_score: 0.591422\n",
      "[66]\ttraining's multi_logloss: 0.967831\ttraining's f1_score: 0.593187\n",
      "[67]\ttraining's multi_logloss: 0.963863\ttraining's f1_score: 0.595036\n",
      "[68]\ttraining's multi_logloss: 0.959946\ttraining's f1_score: 0.596531\n",
      "[69]\ttraining's multi_logloss: 0.956065\ttraining's f1_score: 0.598027\n",
      "[70]\ttraining's multi_logloss: 0.952215\ttraining's f1_score: 0.599834\n",
      "[71]\ttraining's multi_logloss: 0.948409\ttraining's f1_score: 0.601309\n",
      "[72]\ttraining's multi_logloss: 0.944653\ttraining's f1_score: 0.603053\n",
      "[73]\ttraining's multi_logloss: 0.94094\ttraining's f1_score: 0.604466\n",
      "[74]\ttraining's multi_logloss: 0.937272\ttraining's f1_score: 0.605878\n",
      "[75]\ttraining's multi_logloss: 0.933645\ttraining's f1_score: 0.607664\n",
      "[76]\ttraining's multi_logloss: 0.930043\ttraining's f1_score: 0.609326\n",
      "[77]\ttraining's multi_logloss: 0.926491\ttraining's f1_score: 0.610593\n",
      "[78]\ttraining's multi_logloss: 0.922984\ttraining's f1_score: 0.611632\n",
      "[79]\ttraining's multi_logloss: 0.919492\ttraining's f1_score: 0.613148\n",
      "[80]\ttraining's multi_logloss: 0.916054\ttraining's f1_score: 0.614581\n",
      "[81]\ttraining's multi_logloss: 0.91265\ttraining's f1_score: 0.616056\n",
      "[82]\ttraining's multi_logloss: 0.90928\ttraining's f1_score: 0.617738\n",
      "[83]\ttraining's multi_logloss: 0.905939\ttraining's f1_score: 0.619296\n",
      "[84]\ttraining's multi_logloss: 0.902631\ttraining's f1_score: 0.620397\n",
      "[85]\ttraining's multi_logloss: 0.89937\ttraining's f1_score: 0.621768\n",
      "[86]\ttraining's multi_logloss: 0.896132\ttraining's f1_score: 0.623138\n",
      "[87]\ttraining's multi_logloss: 0.892921\ttraining's f1_score: 0.624572\n",
      "[88]\ttraining's multi_logloss: 0.889748\ttraining's f1_score: 0.626233\n",
      "[89]\ttraining's multi_logloss: 0.886595\ttraining's f1_score: 0.628165\n",
      "[90]\ttraining's multi_logloss: 0.883477\ttraining's f1_score: 0.630408\n",
      "[91]\ttraining's multi_logloss: 0.880392\ttraining's f1_score: 0.632548\n",
      "[92]\ttraining's multi_logloss: 0.877337\ttraining's f1_score: 0.634251\n",
      "[93]\ttraining's multi_logloss: 0.874324\ttraining's f1_score: 0.635871\n",
      "[94]\ttraining's multi_logloss: 0.871323\ttraining's f1_score: 0.637387\n",
      "[95]\ttraining's multi_logloss: 0.86837\ttraining's f1_score: 0.638986\n",
      "[96]\ttraining's multi_logloss: 0.865421\ttraining's f1_score: 0.64015\n",
      "[97]\ttraining's multi_logloss: 0.862515\ttraining's f1_score: 0.64231\n",
      "[98]\ttraining's multi_logloss: 0.859643\ttraining's f1_score: 0.644428\n",
      "[99]\ttraining's multi_logloss: 0.856791\ttraining's f1_score: 0.646464\n",
      "[100]\ttraining's multi_logloss: 0.853977\ttraining's f1_score: 0.648977\n",
      "[101]\ttraining's multi_logloss: 0.851186\ttraining's f1_score: 0.651137\n",
      "[102]\ttraining's multi_logloss: 0.848424\ttraining's f1_score: 0.652965\n",
      "[103]\ttraining's multi_logloss: 0.845685\ttraining's f1_score: 0.655104\n",
      "[104]\ttraining's multi_logloss: 0.84297\ttraining's f1_score: 0.657161\n",
      "[105]\ttraining's multi_logloss: 0.840283\ttraining's f1_score: 0.658968\n",
      "[106]\ttraining's multi_logloss: 0.837619\ttraining's f1_score: 0.660505\n",
      "[107]\ttraining's multi_logloss: 0.834993\ttraining's f1_score: 0.662561\n",
      "[108]\ttraining's multi_logloss: 0.832387\ttraining's f1_score: 0.66416\n",
      "[109]\ttraining's multi_logloss: 0.829801\ttraining's f1_score: 0.666113\n",
      "[110]\ttraining's multi_logloss: 0.827243\ttraining's f1_score: 0.668065\n",
      "[111]\ttraining's multi_logloss: 0.824711\ttraining's f1_score: 0.669976\n",
      "[112]\ttraining's multi_logloss: 0.822186\ttraining's f1_score: 0.671347\n",
      "[113]\ttraining's multi_logloss: 0.819695\ttraining's f1_score: 0.672905\n",
      "[114]\ttraining's multi_logloss: 0.817208\ttraining's f1_score: 0.673819\n",
      "[115]\ttraining's multi_logloss: 0.814756\ttraining's f1_score: 0.675418\n",
      "[116]\ttraining's multi_logloss: 0.812317\ttraining's f1_score: 0.677017\n",
      "[117]\ttraining's multi_logloss: 0.809923\ttraining's f1_score: 0.6787\n",
      "[118]\ttraining's multi_logloss: 0.807531\ttraining's f1_score: 0.680361\n",
      "[119]\ttraining's multi_logloss: 0.80516\ttraining's f1_score: 0.681379\n",
      "[120]\ttraining's multi_logloss: 0.802802\ttraining's f1_score: 0.683145\n",
      "[121]\ttraining's multi_logloss: 0.800476\ttraining's f1_score: 0.684412\n",
      "[122]\ttraining's multi_logloss: 0.798162\ttraining's f1_score: 0.685284\n",
      "[123]\ttraining's multi_logloss: 0.795866\ttraining's f1_score: 0.686593\n",
      "[124]\ttraining's multi_logloss: 0.79359\ttraining's f1_score: 0.688587\n",
      "[125]\ttraining's multi_logloss: 0.791324\ttraining's f1_score: 0.690373\n",
      "[126]\ttraining's multi_logloss: 0.789083\ttraining's f1_score: 0.691972\n",
      "[127]\ttraining's multi_logloss: 0.786856\ttraining's f1_score: 0.693572\n",
      "[128]\ttraining's multi_logloss: 0.784645\ttraining's f1_score: 0.695358\n",
      "[129]\ttraining's multi_logloss: 0.78245\ttraining's f1_score: 0.696749\n",
      "[130]\ttraining's multi_logloss: 0.780274\ttraining's f1_score: 0.698058\n",
      "[131]\ttraining's multi_logloss: 0.778118\ttraining's f1_score: 0.699657\n",
      "[132]\ttraining's multi_logloss: 0.775986\ttraining's f1_score: 0.701174\n",
      "[133]\ttraining's multi_logloss: 0.773862\ttraining's f1_score: 0.702378\n",
      "[134]\ttraining's multi_logloss: 0.771758\ttraining's f1_score: 0.703708\n",
      "[135]\ttraining's multi_logloss: 0.769662\ttraining's f1_score: 0.705078\n",
      "[136]\ttraining's multi_logloss: 0.767592\ttraining's f1_score: 0.706595\n",
      "[137]\ttraining's multi_logloss: 0.765537\ttraining's f1_score: 0.707592\n",
      "[138]\ttraining's multi_logloss: 0.763493\ttraining's f1_score: 0.709129\n",
      "[139]\ttraining's multi_logloss: 0.76147\ttraining's f1_score: 0.71025\n",
      "[140]\ttraining's multi_logloss: 0.75947\ttraining's f1_score: 0.711497\n",
      "[141]\ttraining's multi_logloss: 0.757477\ttraining's f1_score: 0.712743\n",
      "[142]\ttraining's multi_logloss: 0.755503\ttraining's f1_score: 0.713906\n",
      "[143]\ttraining's multi_logloss: 0.753533\ttraining's f1_score: 0.715152\n",
      "[144]\ttraining's multi_logloss: 0.751578\ttraining's f1_score: 0.716232\n",
      "[145]\ttraining's multi_logloss: 0.749636\ttraining's f1_score: 0.717354\n",
      "[146]\ttraining's multi_logloss: 0.747704\ttraining's f1_score: 0.718455\n",
      "[147]\ttraining's multi_logloss: 0.745798\ttraining's f1_score: 0.719888\n",
      "[148]\ttraining's multi_logloss: 0.743899\ttraining's f1_score: 0.7213\n",
      "[149]\ttraining's multi_logloss: 0.742008\ttraining's f1_score: 0.72238\n",
      "[150]\ttraining's multi_logloss: 0.740144\ttraining's f1_score: 0.723398\n",
      "[151]\ttraining's multi_logloss: 0.738304\ttraining's f1_score: 0.724644\n",
      "[152]\ttraining's multi_logloss: 0.736472\ttraining's f1_score: 0.725309\n",
      "[153]\ttraining's multi_logloss: 0.734663\ttraining's f1_score: 0.726327\n",
      "[154]\ttraining's multi_logloss: 0.732859\ttraining's f1_score: 0.726971\n",
      "[155]\ttraining's multi_logloss: 0.731085\ttraining's f1_score: 0.728591\n",
      "[156]\ttraining's multi_logloss: 0.729318\ttraining's f1_score: 0.729442\n",
      "[157]\ttraining's multi_logloss: 0.727569\ttraining's f1_score: 0.730564\n",
      "[158]\ttraining's multi_logloss: 0.725832\ttraining's f1_score: 0.731602\n",
      "[159]\ttraining's multi_logloss: 0.724104\ttraining's f1_score: 0.732516\n",
      "[160]\ttraining's multi_logloss: 0.722397\ttraining's f1_score: 0.733555\n",
      "[161]\ttraining's multi_logloss: 0.720681\ttraining's f1_score: 0.734697\n",
      "[162]\ttraining's multi_logloss: 0.718977\ttraining's f1_score: 0.735403\n",
      "[163]\ttraining's multi_logloss: 0.717292\ttraining's f1_score: 0.736255\n",
      "[164]\ttraining's multi_logloss: 0.715618\ttraining's f1_score: 0.737377\n",
      "[165]\ttraining's multi_logloss: 0.713942\ttraining's f1_score: 0.738394\n",
      "[166]\ttraining's multi_logloss: 0.712277\ttraining's f1_score: 0.739163\n",
      "[167]\ttraining's multi_logloss: 0.710631\ttraining's f1_score: 0.740181\n",
      "[168]\ttraining's multi_logloss: 0.708988\ttraining's f1_score: 0.741323\n",
      "[169]\ttraining's multi_logloss: 0.707355\ttraining's f1_score: 0.742279\n",
      "[170]\ttraining's multi_logloss: 0.705739\ttraining's f1_score: 0.742985\n",
      "[171]\ttraining's multi_logloss: 0.704142\ttraining's f1_score: 0.743587\n",
      "[172]\ttraining's multi_logloss: 0.702557\ttraining's f1_score: 0.744418\n",
      "[173]\ttraining's multi_logloss: 0.700983\ttraining's f1_score: 0.745353\n",
      "[174]\ttraining's multi_logloss: 0.699413\ttraining's f1_score: 0.746017\n",
      "[175]\ttraining's multi_logloss: 0.697858\ttraining's f1_score: 0.746682\n",
      "[176]\ttraining's multi_logloss: 0.696314\ttraining's f1_score: 0.747367\n",
      "[177]\ttraining's multi_logloss: 0.694785\ttraining's f1_score: 0.748094\n",
      "[178]\ttraining's multi_logloss: 0.693271\ttraining's f1_score: 0.74851\n",
      "[179]\ttraining's multi_logloss: 0.691765\ttraining's f1_score: 0.749486\n",
      "[180]\ttraining's multi_logloss: 0.690271\ttraining's f1_score: 0.7504\n",
      "[181]\ttraining's multi_logloss: 0.688785\ttraining's f1_score: 0.751314\n",
      "[182]\ttraining's multi_logloss: 0.687306\ttraining's f1_score: 0.752124\n",
      "[183]\ttraining's multi_logloss: 0.685845\ttraining's f1_score: 0.753142\n",
      "[184]\ttraining's multi_logloss: 0.684385\ttraining's f1_score: 0.753889\n",
      "[185]\ttraining's multi_logloss: 0.68294\ttraining's f1_score: 0.754907\n",
      "[186]\ttraining's multi_logloss: 0.681515\ttraining's f1_score: 0.75553\n",
      "[187]\ttraining's multi_logloss: 0.680083\ttraining's f1_score: 0.756216\n",
      "[188]\ttraining's multi_logloss: 0.678665\ttraining's f1_score: 0.757046\n",
      "[189]\ttraining's multi_logloss: 0.67726\ttraining's f1_score: 0.758189\n",
      "[190]\ttraining's multi_logloss: 0.675865\ttraining's f1_score: 0.758833\n",
      "[191]\ttraining's multi_logloss: 0.674487\ttraining's f1_score: 0.759497\n",
      "[192]\ttraining's multi_logloss: 0.673117\ttraining's f1_score: 0.760266\n",
      "[193]\ttraining's multi_logloss: 0.671763\ttraining's f1_score: 0.760577\n",
      "[194]\ttraining's multi_logloss: 0.670422\ttraining's f1_score: 0.761304\n",
      "[195]\ttraining's multi_logloss: 0.669074\ttraining's f1_score: 0.762281\n",
      "[196]\ttraining's multi_logloss: 0.667746\ttraining's f1_score: 0.762883\n",
      "[197]\ttraining's multi_logloss: 0.666426\ttraining's f1_score: 0.763257\n",
      "[198]\ttraining's multi_logloss: 0.66511\ttraining's f1_score: 0.764046\n",
      "[199]\ttraining's multi_logloss: 0.663797\ttraining's f1_score: 0.764254\n",
      "[200]\ttraining's multi_logloss: 0.6625\ttraining's f1_score: 0.765188\n",
      "[201]\ttraining's multi_logloss: 0.661204\ttraining's f1_score: 0.765832\n",
      "[202]\ttraining's multi_logloss: 0.659924\ttraining's f1_score: 0.766435\n",
      "[203]\ttraining's multi_logloss: 0.658643\ttraining's f1_score: 0.766518\n",
      "[204]\ttraining's multi_logloss: 0.657372\ttraining's f1_score: 0.766892\n",
      "[205]\ttraining's multi_logloss: 0.656125\ttraining's f1_score: 0.767452\n",
      "[206]\ttraining's multi_logloss: 0.654872\ttraining's f1_score: 0.767993\n",
      "[207]\ttraining's multi_logloss: 0.653621\ttraining's f1_score: 0.768678\n",
      "[208]\ttraining's multi_logloss: 0.652394\ttraining's f1_score: 0.769073\n",
      "[209]\ttraining's multi_logloss: 0.651171\ttraining's f1_score: 0.769903\n",
      "[210]\ttraining's multi_logloss: 0.649954\ttraining's f1_score: 0.770693\n",
      "[211]\ttraining's multi_logloss: 0.648753\ttraining's f1_score: 0.77115\n",
      "[212]\ttraining's multi_logloss: 0.647547\ttraining's f1_score: 0.771669\n",
      "[213]\ttraining's multi_logloss: 0.646357\ttraining's f1_score: 0.771918\n",
      "[214]\ttraining's multi_logloss: 0.645184\ttraining's f1_score: 0.772313\n",
      "[215]\ttraining's multi_logloss: 0.644004\ttraining's f1_score: 0.772977\n",
      "[216]\ttraining's multi_logloss: 0.642832\ttraining's f1_score: 0.773601\n",
      "[217]\ttraining's multi_logloss: 0.641669\ttraining's f1_score: 0.774058\n",
      "[218]\ttraining's multi_logloss: 0.640508\ttraining's f1_score: 0.774909\n",
      "[219]\ttraining's multi_logloss: 0.639364\ttraining's f1_score: 0.7752\n",
      "[220]\ttraining's multi_logloss: 0.638203\ttraining's f1_score: 0.775657\n",
      "[221]\ttraining's multi_logloss: 0.637062\ttraining's f1_score: 0.776218\n",
      "[222]\ttraining's multi_logloss: 0.635924\ttraining's f1_score: 0.776612\n",
      "[223]\ttraining's multi_logloss: 0.634791\ttraining's f1_score: 0.776965\n",
      "[224]\ttraining's multi_logloss: 0.633661\ttraining's f1_score: 0.777173\n",
      "[225]\ttraining's multi_logloss: 0.632543\ttraining's f1_score: 0.777713\n",
      "[226]\ttraining's multi_logloss: 0.631424\ttraining's f1_score: 0.778378\n",
      "[227]\ttraining's multi_logloss: 0.630299\ttraining's f1_score: 0.778835\n",
      "[228]\ttraining's multi_logloss: 0.629197\ttraining's f1_score: 0.778793\n",
      "[229]\ttraining's multi_logloss: 0.628102\ttraining's f1_score: 0.779229\n",
      "[230]\ttraining's multi_logloss: 0.627021\ttraining's f1_score: 0.780019\n",
      "[231]\ttraining's multi_logloss: 0.625941\ttraining's f1_score: 0.780476\n",
      "[232]\ttraining's multi_logloss: 0.624862\ttraining's f1_score: 0.781036\n",
      "[233]\ttraining's multi_logloss: 0.623799\ttraining's f1_score: 0.78141\n",
      "[234]\ttraining's multi_logloss: 0.622726\ttraining's f1_score: 0.781556\n",
      "[235]\ttraining's multi_logloss: 0.62167\ttraining's f1_score: 0.78193\n",
      "[236]\ttraining's multi_logloss: 0.620619\ttraining's f1_score: 0.782303\n",
      "[237]\ttraining's multi_logloss: 0.619559\ttraining's f1_score: 0.78274\n",
      "[238]\ttraining's multi_logloss: 0.618512\ttraining's f1_score: 0.783633\n",
      "[239]\ttraining's multi_logloss: 0.617476\ttraining's f1_score: 0.784194\n",
      "[240]\ttraining's multi_logloss: 0.616447\ttraining's f1_score: 0.784464\n",
      "[241]\ttraining's multi_logloss: 0.615427\ttraining's f1_score: 0.785087\n",
      "[242]\ttraining's multi_logloss: 0.614411\ttraining's f1_score: 0.785294\n",
      "[243]\ttraining's multi_logloss: 0.613402\ttraining's f1_score: 0.785897\n",
      "[244]\ttraining's multi_logloss: 0.612395\ttraining's f1_score: 0.786229\n",
      "[245]\ttraining's multi_logloss: 0.611392\ttraining's f1_score: 0.78679\n",
      "[246]\ttraining's multi_logloss: 0.610402\ttraining's f1_score: 0.787288\n",
      "[247]\ttraining's multi_logloss: 0.609417\ttraining's f1_score: 0.7876\n",
      "[248]\ttraining's multi_logloss: 0.608426\ttraining's f1_score: 0.788161\n",
      "[249]\ttraining's multi_logloss: 0.607457\ttraining's f1_score: 0.788597\n",
      "[250]\ttraining's multi_logloss: 0.606488\ttraining's f1_score: 0.789116\n",
      "[251]\ttraining's multi_logloss: 0.605524\ttraining's f1_score: 0.789635\n",
      "[252]\ttraining's multi_logloss: 0.604567\ttraining's f1_score: 0.790238\n",
      "[253]\ttraining's multi_logloss: 0.603611\ttraining's f1_score: 0.790736\n",
      "[254]\ttraining's multi_logloss: 0.602672\ttraining's f1_score: 0.790965\n",
      "[255]\ttraining's multi_logloss: 0.601729\ttraining's f1_score: 0.791214\n",
      "[256]\ttraining's multi_logloss: 0.60079\ttraining's f1_score: 0.791422\n",
      "[257]\ttraining's multi_logloss: 0.599855\ttraining's f1_score: 0.791692\n",
      "[258]\ttraining's multi_logloss: 0.598927\ttraining's f1_score: 0.79219\n",
      "[259]\ttraining's multi_logloss: 0.598004\ttraining's f1_score: 0.792502\n",
      "[260]\ttraining's multi_logloss: 0.597075\ttraining's f1_score: 0.792834\n",
      "[261]\ttraining's multi_logloss: 0.596155\ttraining's f1_score: 0.793187\n",
      "[262]\ttraining's multi_logloss: 0.595241\ttraining's f1_score: 0.79354\n",
      "[263]\ttraining's multi_logloss: 0.594331\ttraining's f1_score: 0.79408\n",
      "[264]\ttraining's multi_logloss: 0.59343\ttraining's f1_score: 0.794163\n",
      "[265]\ttraining's multi_logloss: 0.592526\ttraining's f1_score: 0.79462\n",
      "[266]\ttraining's multi_logloss: 0.591635\ttraining's f1_score: 0.79489\n",
      "[267]\ttraining's multi_logloss: 0.590739\ttraining's f1_score: 0.795493\n",
      "[268]\ttraining's multi_logloss: 0.589846\ttraining's f1_score: 0.79568\n",
      "[269]\ttraining's multi_logloss: 0.588957\ttraining's f1_score: 0.796054\n",
      "[270]\ttraining's multi_logloss: 0.58807\ttraining's f1_score: 0.796739\n",
      "[271]\ttraining's multi_logloss: 0.587198\ttraining's f1_score: 0.797175\n",
      "[272]\ttraining's multi_logloss: 0.586323\ttraining's f1_score: 0.797424\n",
      "[273]\ttraining's multi_logloss: 0.585456\ttraining's f1_score: 0.797715\n",
      "[274]\ttraining's multi_logloss: 0.584586\ttraining's f1_score: 0.797944\n",
      "[275]\ttraining's multi_logloss: 0.583722\ttraining's f1_score: 0.798131\n",
      "[276]\ttraining's multi_logloss: 0.582867\ttraining's f1_score: 0.798546\n",
      "[277]\ttraining's multi_logloss: 0.582009\ttraining's f1_score: 0.799398\n",
      "[278]\ttraining's multi_logloss: 0.581158\ttraining's f1_score: 0.799875\n",
      "[279]\ttraining's multi_logloss: 0.58031\ttraining's f1_score: 0.800104\n",
      "[280]\ttraining's multi_logloss: 0.579467\ttraining's f1_score: 0.800519\n",
      "[281]\ttraining's multi_logloss: 0.578624\ttraining's f1_score: 0.800914\n",
      "[282]\ttraining's multi_logloss: 0.577794\ttraining's f1_score: 0.801433\n",
      "[283]\ttraining's multi_logloss: 0.57697\ttraining's f1_score: 0.801766\n",
      "[284]\ttraining's multi_logloss: 0.576149\ttraining's f1_score: 0.802015\n",
      "[285]\ttraining's multi_logloss: 0.575334\ttraining's f1_score: 0.802472\n",
      "[286]\ttraining's multi_logloss: 0.574516\ttraining's f1_score: 0.802492\n",
      "[287]\ttraining's multi_logloss: 0.573714\ttraining's f1_score: 0.802804\n",
      "[288]\ttraining's multi_logloss: 0.572905\ttraining's f1_score: 0.80324\n",
      "[289]\ttraining's multi_logloss: 0.572107\ttraining's f1_score: 0.803323\n",
      "[290]\ttraining's multi_logloss: 0.571305\ttraining's f1_score: 0.803552\n",
      "[291]\ttraining's multi_logloss: 0.57051\ttraining's f1_score: 0.803967\n",
      "[292]\ttraining's multi_logloss: 0.569706\ttraining's f1_score: 0.80432\n",
      "[293]\ttraining's multi_logloss: 0.568907\ttraining's f1_score: 0.804632\n",
      "[294]\ttraining's multi_logloss: 0.568105\ttraining's f1_score: 0.804715\n",
      "[295]\ttraining's multi_logloss: 0.567309\ttraining's f1_score: 0.805089\n",
      "[296]\ttraining's multi_logloss: 0.566522\ttraining's f1_score: 0.805712\n",
      "[297]\ttraining's multi_logloss: 0.565743\ttraining's f1_score: 0.80594\n",
      "[298]\ttraining's multi_logloss: 0.564958\ttraining's f1_score: 0.806418\n",
      "[299]\ttraining's multi_logloss: 0.564178\ttraining's f1_score: 0.80648\n",
      "[300]\ttraining's multi_logloss: 0.563401\ttraining's f1_score: 0.806834\n",
      "[301]\ttraining's multi_logloss: 0.562622\ttraining's f1_score: 0.807415\n",
      "[302]\ttraining's multi_logloss: 0.561861\ttraining's f1_score: 0.807644\n",
      "[303]\ttraining's multi_logloss: 0.561102\ttraining's f1_score: 0.808121\n",
      "[304]\ttraining's multi_logloss: 0.560339\ttraining's f1_score: 0.808204\n",
      "[305]\ttraining's multi_logloss: 0.559588\ttraining's f1_score: 0.808661\n",
      "[306]\ttraining's multi_logloss: 0.558831\ttraining's f1_score: 0.809035\n",
      "[307]\ttraining's multi_logloss: 0.558077\ttraining's f1_score: 0.809284\n",
      "[308]\ttraining's multi_logloss: 0.557332\ttraining's f1_score: 0.809804\n",
      "[309]\ttraining's multi_logloss: 0.556584\ttraining's f1_score: 0.810178\n",
      "[310]\ttraining's multi_logloss: 0.555845\ttraining's f1_score: 0.810572\n",
      "[311]\ttraining's multi_logloss: 0.555101\ttraining's f1_score: 0.811133\n",
      "[312]\ttraining's multi_logloss: 0.554369\ttraining's f1_score: 0.811569\n",
      "[313]\ttraining's multi_logloss: 0.553635\ttraining's f1_score: 0.811943\n",
      "[314]\ttraining's multi_logloss: 0.552907\ttraining's f1_score: 0.812649\n",
      "[315]\ttraining's multi_logloss: 0.552176\ttraining's f1_score: 0.81267\n",
      "[316]\ttraining's multi_logloss: 0.551453\ttraining's f1_score: 0.813002\n",
      "[317]\ttraining's multi_logloss: 0.550733\ttraining's f1_score: 0.813252\n",
      "[318]\ttraining's multi_logloss: 0.550022\ttraining's f1_score: 0.81348\n",
      "[319]\ttraining's multi_logloss: 0.549315\ttraining's f1_score: 0.813792\n",
      "[320]\ttraining's multi_logloss: 0.548604\ttraining's f1_score: 0.814103\n",
      "[321]\ttraining's multi_logloss: 0.547903\ttraining's f1_score: 0.814436\n",
      "[322]\ttraining's multi_logloss: 0.547203\ttraining's f1_score: 0.814747\n",
      "[323]\ttraining's multi_logloss: 0.546495\ttraining's f1_score: 0.815079\n",
      "[324]\ttraining's multi_logloss: 0.5458\ttraining's f1_score: 0.815391\n",
      "[325]\ttraining's multi_logloss: 0.545102\ttraining's f1_score: 0.815952\n",
      "[326]\ttraining's multi_logloss: 0.54442\ttraining's f1_score: 0.81618\n",
      "[327]\ttraining's multi_logloss: 0.543724\ttraining's f1_score: 0.816554\n",
      "[328]\ttraining's multi_logloss: 0.543032\ttraining's f1_score: 0.816845\n",
      "[329]\ttraining's multi_logloss: 0.542346\ttraining's f1_score: 0.817136\n",
      "[330]\ttraining's multi_logloss: 0.541671\ttraining's f1_score: 0.817073\n",
      "[331]\ttraining's multi_logloss: 0.540996\ttraining's f1_score: 0.817406\n",
      "[332]\ttraining's multi_logloss: 0.540309\ttraining's f1_score: 0.8178\n",
      "[333]\ttraining's multi_logloss: 0.53964\ttraining's f1_score: 0.818153\n",
      "[334]\ttraining's multi_logloss: 0.538964\ttraining's f1_score: 0.818569\n",
      "[335]\ttraining's multi_logloss: 0.538292\ttraining's f1_score: 0.819005\n",
      "[336]\ttraining's multi_logloss: 0.537616\ttraining's f1_score: 0.819587\n",
      "[337]\ttraining's multi_logloss: 0.536953\ttraining's f1_score: 0.819815\n",
      "[338]\ttraining's multi_logloss: 0.5363\ttraining's f1_score: 0.819961\n",
      "[339]\ttraining's multi_logloss: 0.535638\ttraining's f1_score: 0.820147\n",
      "[340]\ttraining's multi_logloss: 0.534994\ttraining's f1_score: 0.820501\n",
      "[341]\ttraining's multi_logloss: 0.534346\ttraining's f1_score: 0.820584\n",
      "[342]\ttraining's multi_logloss: 0.53368\ttraining's f1_score: 0.820874\n",
      "[343]\ttraining's multi_logloss: 0.533021\ttraining's f1_score: 0.820978\n",
      "[344]\ttraining's multi_logloss: 0.532378\ttraining's f1_score: 0.821228\n",
      "[345]\ttraining's multi_logloss: 0.531734\ttraining's f1_score: 0.821581\n",
      "[346]\ttraining's multi_logloss: 0.531088\ttraining's f1_score: 0.821975\n",
      "[347]\ttraining's multi_logloss: 0.530445\ttraining's f1_score: 0.822079\n",
      "[348]\ttraining's multi_logloss: 0.529811\ttraining's f1_score: 0.822328\n",
      "[349]\ttraining's multi_logloss: 0.529173\ttraining's f1_score: 0.822453\n",
      "[350]\ttraining's multi_logloss: 0.528535\ttraining's f1_score: 0.822702\n",
      "[351]\ttraining's multi_logloss: 0.5279\ttraining's f1_score: 0.822993\n",
      "[352]\ttraining's multi_logloss: 0.52727\ttraining's f1_score: 0.823222\n",
      "[353]\ttraining's multi_logloss: 0.52664\ttraining's f1_score: 0.823471\n",
      "[354]\ttraining's multi_logloss: 0.526019\ttraining's f1_score: 0.823575\n",
      "[355]\ttraining's multi_logloss: 0.525397\ttraining's f1_score: 0.823824\n",
      "[356]\ttraining's multi_logloss: 0.524782\ttraining's f1_score: 0.82426\n",
      "[357]\ttraining's multi_logloss: 0.524165\ttraining's f1_score: 0.824219\n",
      "[358]\ttraining's multi_logloss: 0.523549\ttraining's f1_score: 0.824509\n",
      "[359]\ttraining's multi_logloss: 0.522928\ttraining's f1_score: 0.824862\n",
      "[360]\ttraining's multi_logloss: 0.522319\ttraining's f1_score: 0.825029\n",
      "[361]\ttraining's multi_logloss: 0.521703\ttraining's f1_score: 0.825319\n",
      "[362]\ttraining's multi_logloss: 0.521092\ttraining's f1_score: 0.825444\n",
      "[363]\ttraining's multi_logloss: 0.520491\ttraining's f1_score: 0.825984\n",
      "[364]\ttraining's multi_logloss: 0.519892\ttraining's f1_score: 0.826026\n",
      "[365]\ttraining's multi_logloss: 0.519285\ttraining's f1_score: 0.826337\n",
      "[366]\ttraining's multi_logloss: 0.518692\ttraining's f1_score: 0.826503\n",
      "[367]\ttraining's multi_logloss: 0.518085\ttraining's f1_score: 0.826815\n",
      "[368]\ttraining's multi_logloss: 0.517489\ttraining's f1_score: 0.827313\n",
      "[369]\ttraining's multi_logloss: 0.516891\ttraining's f1_score: 0.8275\n",
      "[370]\ttraining's multi_logloss: 0.516305\ttraining's f1_score: 0.827625\n",
      "[371]\ttraining's multi_logloss: 0.515716\ttraining's f1_score: 0.827957\n",
      "[372]\ttraining's multi_logloss: 0.515133\ttraining's f1_score: 0.828123\n",
      "[373]\ttraining's multi_logloss: 0.514545\ttraining's f1_score: 0.82858\n",
      "[374]\ttraining's multi_logloss: 0.513965\ttraining's f1_score: 0.828933\n",
      "[375]\ttraining's multi_logloss: 0.513376\ttraining's f1_score: 0.829037\n",
      "[376]\ttraining's multi_logloss: 0.512799\ttraining's f1_score: 0.829453\n",
      "[377]\ttraining's multi_logloss: 0.51223\ttraining's f1_score: 0.82964\n",
      "[378]\ttraining's multi_logloss: 0.511651\ttraining's f1_score: 0.829889\n",
      "[379]\ttraining's multi_logloss: 0.51108\ttraining's f1_score: 0.830242\n",
      "[380]\ttraining's multi_logloss: 0.510509\ttraining's f1_score: 0.830782\n",
      "[381]\ttraining's multi_logloss: 0.509932\ttraining's f1_score: 0.831114\n",
      "[382]\ttraining's multi_logloss: 0.50937\ttraining's f1_score: 0.831239\n",
      "[383]\ttraining's multi_logloss: 0.508802\ttraining's f1_score: 0.831447\n",
      "[384]\ttraining's multi_logloss: 0.508239\ttraining's f1_score: 0.8318\n",
      "[385]\ttraining's multi_logloss: 0.507684\ttraining's f1_score: 0.831883\n",
      "[386]\ttraining's multi_logloss: 0.507124\ttraining's f1_score: 0.832319\n",
      "[387]\ttraining's multi_logloss: 0.506569\ttraining's f1_score: 0.832485\n",
      "[388]\ttraining's multi_logloss: 0.506017\ttraining's f1_score: 0.832734\n",
      "[389]\ttraining's multi_logloss: 0.50547\ttraining's f1_score: 0.832942\n",
      "[390]\ttraining's multi_logloss: 0.504914\ttraining's f1_score: 0.833129\n",
      "[391]\ttraining's multi_logloss: 0.504368\ttraining's f1_score: 0.833482\n",
      "[392]\ttraining's multi_logloss: 0.503815\ttraining's f1_score: 0.833669\n",
      "[393]\ttraining's multi_logloss: 0.503262\ttraining's f1_score: 0.833981\n",
      "[394]\ttraining's multi_logloss: 0.502717\ttraining's f1_score: 0.834168\n",
      "[395]\ttraining's multi_logloss: 0.502177\ttraining's f1_score: 0.834521\n",
      "[396]\ttraining's multi_logloss: 0.501633\ttraining's f1_score: 0.834728\n",
      "[397]\ttraining's multi_logloss: 0.501094\ttraining's f1_score: 0.834895\n",
      "[398]\ttraining's multi_logloss: 0.500549\ttraining's f1_score: 0.835165\n",
      "[399]\ttraining's multi_logloss: 0.500003\ttraining's f1_score: 0.835435\n",
      "[400]\ttraining's multi_logloss: 0.499464\ttraining's f1_score: 0.835601\n",
      "[401]\ttraining's multi_logloss: 0.498927\ttraining's f1_score: 0.836016\n",
      "[402]\ttraining's multi_logloss: 0.498383\ttraining's f1_score: 0.836203\n",
      "[403]\ttraining's multi_logloss: 0.49784\ttraining's f1_score: 0.836411\n",
      "[404]\ttraining's multi_logloss: 0.497304\ttraining's f1_score: 0.836598\n",
      "[405]\ttraining's multi_logloss: 0.496767\ttraining's f1_score: 0.836868\n",
      "[406]\ttraining's multi_logloss: 0.496236\ttraining's f1_score: 0.837304\n",
      "[407]\ttraining's multi_logloss: 0.495706\ttraining's f1_score: 0.837449\n",
      "[408]\ttraining's multi_logloss: 0.495168\ttraining's f1_score: 0.837802\n",
      "[409]\ttraining's multi_logloss: 0.494637\ttraining's f1_score: 0.837865\n",
      "[410]\ttraining's multi_logloss: 0.494113\ttraining's f1_score: 0.838135\n",
      "[411]\ttraining's multi_logloss: 0.493594\ttraining's f1_score: 0.838426\n",
      "[412]\ttraining's multi_logloss: 0.493069\ttraining's f1_score: 0.838426\n",
      "[413]\ttraining's multi_logloss: 0.492547\ttraining's f1_score: 0.838737\n",
      "[414]\ttraining's multi_logloss: 0.492038\ttraining's f1_score: 0.838862\n",
      "[415]\ttraining's multi_logloss: 0.491512\ttraining's f1_score: 0.839153\n",
      "[416]\ttraining's multi_logloss: 0.490997\ttraining's f1_score: 0.839381\n",
      "[417]\ttraining's multi_logloss: 0.490492\ttraining's f1_score: 0.839547\n",
      "[418]\ttraining's multi_logloss: 0.489986\ttraining's f1_score: 0.839838\n",
      "[419]\ttraining's multi_logloss: 0.489472\ttraining's f1_score: 0.840212\n",
      "[420]\ttraining's multi_logloss: 0.488968\ttraining's f1_score: 0.84069\n",
      "[421]\ttraining's multi_logloss: 0.488467\ttraining's f1_score: 0.841084\n",
      "[422]\ttraining's multi_logloss: 0.487953\ttraining's f1_score: 0.841105\n",
      "[423]\ttraining's multi_logloss: 0.487448\ttraining's f1_score: 0.84152\n",
      "[424]\ttraining's multi_logloss: 0.486948\ttraining's f1_score: 0.84177\n",
      "[425]\ttraining's multi_logloss: 0.486449\ttraining's f1_score: 0.84206\n",
      "[426]\ttraining's multi_logloss: 0.48595\ttraining's f1_score: 0.842414\n",
      "[427]\ttraining's multi_logloss: 0.485453\ttraining's f1_score: 0.842746\n",
      "[428]\ttraining's multi_logloss: 0.484958\ttraining's f1_score: 0.843057\n",
      "[429]\ttraining's multi_logloss: 0.484461\ttraining's f1_score: 0.843556\n",
      "[430]\ttraining's multi_logloss: 0.483968\ttraining's f1_score: 0.843805\n",
      "[431]\ttraining's multi_logloss: 0.483472\ttraining's f1_score: 0.844117\n",
      "[432]\ttraining's multi_logloss: 0.482972\ttraining's f1_score: 0.844241\n",
      "[433]\ttraining's multi_logloss: 0.482481\ttraining's f1_score: 0.844553\n",
      "[434]\ttraining's multi_logloss: 0.481999\ttraining's f1_score: 0.844823\n",
      "[435]\ttraining's multi_logloss: 0.481511\ttraining's f1_score: 0.845093\n",
      "[436]\ttraining's multi_logloss: 0.481022\ttraining's f1_score: 0.845405\n",
      "[437]\ttraining's multi_logloss: 0.480537\ttraining's f1_score: 0.845508\n",
      "[438]\ttraining's multi_logloss: 0.480048\ttraining's f1_score: 0.845384\n",
      "[439]\ttraining's multi_logloss: 0.479571\ttraining's f1_score: 0.845799\n",
      "[440]\ttraining's multi_logloss: 0.47909\ttraining's f1_score: 0.845903\n",
      "[441]\ttraining's multi_logloss: 0.478606\ttraining's f1_score: 0.845924\n",
      "[442]\ttraining's multi_logloss: 0.478134\ttraining's f1_score: 0.846464\n",
      "[443]\ttraining's multi_logloss: 0.477666\ttraining's f1_score: 0.846713\n",
      "[444]\ttraining's multi_logloss: 0.477183\ttraining's f1_score: 0.846942\n",
      "[445]\ttraining's multi_logloss: 0.476709\ttraining's f1_score: 0.847274\n",
      "[446]\ttraining's multi_logloss: 0.476231\ttraining's f1_score: 0.847606\n",
      "[447]\ttraining's multi_logloss: 0.475761\ttraining's f1_score: 0.847793\n",
      "[448]\ttraining's multi_logloss: 0.475293\ttraining's f1_score: 0.84825\n",
      "[449]\ttraining's multi_logloss: 0.474822\ttraining's f1_score: 0.848395\n",
      "[450]\ttraining's multi_logloss: 0.474362\ttraining's f1_score: 0.848707\n",
      "[451]\ttraining's multi_logloss: 0.473894\ttraining's f1_score: 0.848998\n",
      "[452]\ttraining's multi_logloss: 0.473428\ttraining's f1_score: 0.849392\n",
      "[453]\ttraining's multi_logloss: 0.47296\ttraining's f1_score: 0.849538\n",
      "[454]\ttraining's multi_logloss: 0.472499\ttraining's f1_score: 0.849912\n",
      "[455]\ttraining's multi_logloss: 0.472046\ttraining's f1_score: 0.850078\n",
      "[456]\ttraining's multi_logloss: 0.471595\ttraining's f1_score: 0.850182\n",
      "[457]\ttraining's multi_logloss: 0.471139\ttraining's f1_score: 0.850369\n",
      "[458]\ttraining's multi_logloss: 0.470673\ttraining's f1_score: 0.850473\n",
      "[459]\ttraining's multi_logloss: 0.470213\ttraining's f1_score: 0.850784\n",
      "[460]\ttraining's multi_logloss: 0.469757\ttraining's f1_score: 0.851283\n",
      "[461]\ttraining's multi_logloss: 0.469287\ttraining's f1_score: 0.851615\n",
      "[462]\ttraining's multi_logloss: 0.468829\ttraining's f1_score: 0.851636\n",
      "[463]\ttraining's multi_logloss: 0.468383\ttraining's f1_score: 0.851823\n",
      "[464]\ttraining's multi_logloss: 0.467927\ttraining's f1_score: 0.852176\n",
      "[465]\ttraining's multi_logloss: 0.467478\ttraining's f1_score: 0.852363\n",
      "[466]\ttraining's multi_logloss: 0.467035\ttraining's f1_score: 0.852653\n",
      "[467]\ttraining's multi_logloss: 0.466596\ttraining's f1_score: 0.852882\n",
      "[468]\ttraining's multi_logloss: 0.466152\ttraining's f1_score: 0.85311\n",
      "[469]\ttraining's multi_logloss: 0.465702\ttraining's f1_score: 0.853277\n",
      "[470]\ttraining's multi_logloss: 0.465264\ttraining's f1_score: 0.853463\n",
      "[471]\ttraining's multi_logloss: 0.46482\ttraining's f1_score: 0.853837\n",
      "[472]\ttraining's multi_logloss: 0.464386\ttraining's f1_score: 0.854066\n",
      "[473]\ttraining's multi_logloss: 0.463948\ttraining's f1_score: 0.85419\n",
      "[474]\ttraining's multi_logloss: 0.463509\ttraining's f1_score: 0.854481\n",
      "[475]\ttraining's multi_logloss: 0.463075\ttraining's f1_score: 0.854523\n",
      "[476]\ttraining's multi_logloss: 0.462641\ttraining's f1_score: 0.854689\n",
      "[477]\ttraining's multi_logloss: 0.462208\ttraining's f1_score: 0.855125\n",
      "[478]\ttraining's multi_logloss: 0.461763\ttraining's f1_score: 0.855416\n",
      "[479]\ttraining's multi_logloss: 0.46132\ttraining's f1_score: 0.855769\n",
      "[480]\ttraining's multi_logloss: 0.460875\ttraining's f1_score: 0.856018\n",
      "[481]\ttraining's multi_logloss: 0.460427\ttraining's f1_score: 0.856351\n",
      "[482]\ttraining's multi_logloss: 0.459988\ttraining's f1_score: 0.856496\n",
      "[483]\ttraining's multi_logloss: 0.459556\ttraining's f1_score: 0.856538\n",
      "[484]\ttraining's multi_logloss: 0.459115\ttraining's f1_score: 0.856683\n",
      "[485]\ttraining's multi_logloss: 0.458679\ttraining's f1_score: 0.856911\n",
      "[486]\ttraining's multi_logloss: 0.458241\ttraining's f1_score: 0.857389\n",
      "[487]\ttraining's multi_logloss: 0.45781\ttraining's f1_score: 0.857597\n",
      "[488]\ttraining's multi_logloss: 0.457373\ttraining's f1_score: 0.857805\n",
      "[489]\ttraining's multi_logloss: 0.456942\ttraining's f1_score: 0.858054\n",
      "[490]\ttraining's multi_logloss: 0.456505\ttraining's f1_score: 0.858241\n",
      "[491]\ttraining's multi_logloss: 0.456076\ttraining's f1_score: 0.858469\n",
      "[492]\ttraining's multi_logloss: 0.455648\ttraining's f1_score: 0.858718\n",
      "[493]\ttraining's multi_logloss: 0.455221\ttraining's f1_score: 0.858947\n",
      "[494]\ttraining's multi_logloss: 0.454784\ttraining's f1_score: 0.8593\n",
      "[495]\ttraining's multi_logloss: 0.454357\ttraining's f1_score: 0.859362\n",
      "[496]\ttraining's multi_logloss: 0.45393\ttraining's f1_score: 0.859736\n",
      "[497]\ttraining's multi_logloss: 0.453494\ttraining's f1_score: 0.859902\n",
      "[498]\ttraining's multi_logloss: 0.453068\ttraining's f1_score: 0.860027\n",
      "[499]\ttraining's multi_logloss: 0.452646\ttraining's f1_score: 0.860505\n",
      "[500]\ttraining's multi_logloss: 0.452218\ttraining's f1_score: 0.860899\n",
      "[501]\ttraining's multi_logloss: 0.451788\ttraining's f1_score: 0.861045\n",
      "[502]\ttraining's multi_logloss: 0.451376\ttraining's f1_score: 0.861232\n",
      "[503]\ttraining's multi_logloss: 0.450949\ttraining's f1_score: 0.861315\n",
      "[504]\ttraining's multi_logloss: 0.450538\ttraining's f1_score: 0.861543\n",
      "[505]\ttraining's multi_logloss: 0.450127\ttraining's f1_score: 0.86173\n",
      "[506]\ttraining's multi_logloss: 0.449715\ttraining's f1_score: 0.861813\n",
      "[507]\ttraining's multi_logloss: 0.449297\ttraining's f1_score: 0.86227\n",
      "[508]\ttraining's multi_logloss: 0.44887\ttraining's f1_score: 0.862519\n",
      "[509]\ttraining's multi_logloss: 0.448453\ttraining's f1_score: 0.862727\n",
      "[510]\ttraining's multi_logloss: 0.448045\ttraining's f1_score: 0.862873\n",
      "[511]\ttraining's multi_logloss: 0.447638\ttraining's f1_score: 0.863101\n",
      "[512]\ttraining's multi_logloss: 0.447233\ttraining's f1_score: 0.863392\n",
      "[513]\ttraining's multi_logloss: 0.446835\ttraining's f1_score: 0.863786\n",
      "[514]\ttraining's multi_logloss: 0.446425\ttraining's f1_score: 0.863994\n",
      "[515]\ttraining's multi_logloss: 0.446025\ttraining's f1_score: 0.86416\n",
      "[516]\ttraining's multi_logloss: 0.445604\ttraining's f1_score: 0.864576\n",
      "[517]\ttraining's multi_logloss: 0.445189\ttraining's f1_score: 0.864804\n",
      "[518]\ttraining's multi_logloss: 0.444785\ttraining's f1_score: 0.86497\n",
      "[519]\ttraining's multi_logloss: 0.444382\ttraining's f1_score: 0.865074\n",
      "[520]\ttraining's multi_logloss: 0.443985\ttraining's f1_score: 0.865407\n",
      "[521]\ttraining's multi_logloss: 0.44357\ttraining's f1_score: 0.865656\n",
      "[522]\ttraining's multi_logloss: 0.443161\ttraining's f1_score: 0.865718\n",
      "[523]\ttraining's multi_logloss: 0.44276\ttraining's f1_score: 0.865905\n",
      "[524]\ttraining's multi_logloss: 0.442352\ttraining's f1_score: 0.866258\n",
      "[525]\ttraining's multi_logloss: 0.441952\ttraining's f1_score: 0.866445\n",
      "[526]\ttraining's multi_logloss: 0.441549\ttraining's f1_score: 0.866757\n",
      "[527]\ttraining's multi_logloss: 0.441158\ttraining's f1_score: 0.866944\n",
      "[528]\ttraining's multi_logloss: 0.440769\ttraining's f1_score: 0.867068\n",
      "[529]\ttraining's multi_logloss: 0.440364\ttraining's f1_score: 0.86738\n",
      "[530]\ttraining's multi_logloss: 0.439968\ttraining's f1_score: 0.867463\n",
      "[531]\ttraining's multi_logloss: 0.439575\ttraining's f1_score: 0.867878\n",
      "[532]\ttraining's multi_logloss: 0.439188\ttraining's f1_score: 0.868107\n",
      "[533]\ttraining's multi_logloss: 0.438808\ttraining's f1_score: 0.868252\n",
      "[534]\ttraining's multi_logloss: 0.438418\ttraining's f1_score: 0.868335\n",
      "[535]\ttraining's multi_logloss: 0.438031\ttraining's f1_score: 0.868439\n",
      "[536]\ttraining's multi_logloss: 0.43765\ttraining's f1_score: 0.868481\n",
      "[537]\ttraining's multi_logloss: 0.437264\ttraining's f1_score: 0.868875\n",
      "[538]\ttraining's multi_logloss: 0.436867\ttraining's f1_score: 0.869021\n",
      "[539]\ttraining's multi_logloss: 0.436484\ttraining's f1_score: 0.869249\n",
      "[540]\ttraining's multi_logloss: 0.43609\ttraining's f1_score: 0.869311\n",
      "[541]\ttraining's multi_logloss: 0.435691\ttraining's f1_score: 0.869768\n",
      "[542]\ttraining's multi_logloss: 0.435307\ttraining's f1_score: 0.869935\n",
      "[543]\ttraining's multi_logloss: 0.434915\ttraining's f1_score: 0.870163\n",
      "[544]\ttraining's multi_logloss: 0.434536\ttraining's f1_score: 0.870288\n",
      "[545]\ttraining's multi_logloss: 0.434152\ttraining's f1_score: 0.870537\n",
      "[546]\ttraining's multi_logloss: 0.433771\ttraining's f1_score: 0.870745\n",
      "[547]\ttraining's multi_logloss: 0.433394\ttraining's f1_score: 0.870952\n",
      "[548]\ttraining's multi_logloss: 0.433009\ttraining's f1_score: 0.870952\n",
      "[549]\ttraining's multi_logloss: 0.432631\ttraining's f1_score: 0.871202\n",
      "[550]\ttraining's multi_logloss: 0.432256\ttraining's f1_score: 0.871181\n",
      "[551]\ttraining's multi_logloss: 0.431872\ttraining's f1_score: 0.87143\n",
      "[552]\ttraining's multi_logloss: 0.431498\ttraining's f1_score: 0.8717\n",
      "[553]\ttraining's multi_logloss: 0.431119\ttraining's f1_score: 0.871991\n",
      "[554]\ttraining's multi_logloss: 0.430744\ttraining's f1_score: 0.872053\n",
      "[555]\ttraining's multi_logloss: 0.430358\ttraining's f1_score: 0.87224\n",
      "[556]\ttraining's multi_logloss: 0.429969\ttraining's f1_score: 0.872489\n",
      "[557]\ttraining's multi_logloss: 0.429594\ttraining's f1_score: 0.872801\n",
      "[558]\ttraining's multi_logloss: 0.429217\ttraining's f1_score: 0.873029\n",
      "[559]\ttraining's multi_logloss: 0.428843\ttraining's f1_score: 0.873112\n",
      "[560]\ttraining's multi_logloss: 0.428486\ttraining's f1_score: 0.873237\n",
      "[561]\ttraining's multi_logloss: 0.428118\ttraining's f1_score: 0.873486\n",
      "[562]\ttraining's multi_logloss: 0.427749\ttraining's f1_score: 0.873569\n",
      "[563]\ttraining's multi_logloss: 0.42737\ttraining's f1_score: 0.873756\n",
      "[564]\ttraining's multi_logloss: 0.427002\ttraining's f1_score: 0.874026\n",
      "[565]\ttraining's multi_logloss: 0.426637\ttraining's f1_score: 0.873985\n",
      "[566]\ttraining's multi_logloss: 0.426272\ttraining's f1_score: 0.87413\n",
      "[567]\ttraining's multi_logloss: 0.425898\ttraining's f1_score: 0.874483\n",
      "[568]\ttraining's multi_logloss: 0.425535\ttraining's f1_score: 0.874816\n",
      "[569]\ttraining's multi_logloss: 0.42517\ttraining's f1_score: 0.875086\n",
      "[570]\ttraining's multi_logloss: 0.424808\ttraining's f1_score: 0.87548\n",
      "[571]\ttraining's multi_logloss: 0.424448\ttraining's f1_score: 0.875667\n",
      "[572]\ttraining's multi_logloss: 0.424092\ttraining's f1_score: 0.875896\n",
      "[573]\ttraining's multi_logloss: 0.423728\ttraining's f1_score: 0.876041\n",
      "[574]\ttraining's multi_logloss: 0.423375\ttraining's f1_score: 0.876394\n",
      "[575]\ttraining's multi_logloss: 0.423017\ttraining's f1_score: 0.876394\n",
      "[576]\ttraining's multi_logloss: 0.422656\ttraining's f1_score: 0.876602\n",
      "[577]\ttraining's multi_logloss: 0.422294\ttraining's f1_score: 0.876851\n",
      "[578]\ttraining's multi_logloss: 0.421937\ttraining's f1_score: 0.877163\n",
      "[579]\ttraining's multi_logloss: 0.421584\ttraining's f1_score: 0.877225\n",
      "[580]\ttraining's multi_logloss: 0.421221\ttraining's f1_score: 0.87735\n",
      "[581]\ttraining's multi_logloss: 0.420871\ttraining's f1_score: 0.877433\n",
      "[582]\ttraining's multi_logloss: 0.420523\ttraining's f1_score: 0.877703\n",
      "[583]\ttraining's multi_logloss: 0.420169\ttraining's f1_score: 0.87791\n",
      "[584]\ttraining's multi_logloss: 0.419814\ttraining's f1_score: 0.878222\n",
      "[585]\ttraining's multi_logloss: 0.41946\ttraining's f1_score: 0.878284\n",
      "[586]\ttraining's multi_logloss: 0.419121\ttraining's f1_score: 0.87843\n",
      "[587]\ttraining's multi_logloss: 0.418767\ttraining's f1_score: 0.878658\n",
      "[588]\ttraining's multi_logloss: 0.418412\ttraining's f1_score: 0.878991\n",
      "[589]\ttraining's multi_logloss: 0.418054\ttraining's f1_score: 0.879219\n",
      "[590]\ttraining's multi_logloss: 0.417696\ttraining's f1_score: 0.879406\n",
      "[591]\ttraining's multi_logloss: 0.417345\ttraining's f1_score: 0.879572\n",
      "[592]\ttraining's multi_logloss: 0.416995\ttraining's f1_score: 0.879946\n",
      "[593]\ttraining's multi_logloss: 0.416641\ttraining's f1_score: 0.880112\n",
      "[594]\ttraining's multi_logloss: 0.416302\ttraining's f1_score: 0.880071\n",
      "[595]\ttraining's multi_logloss: 0.415947\ttraining's f1_score: 0.880341\n",
      "[596]\ttraining's multi_logloss: 0.415596\ttraining's f1_score: 0.880673\n",
      "[597]\ttraining's multi_logloss: 0.415246\ttraining's f1_score: 0.880964\n",
      "[598]\ttraining's multi_logloss: 0.414893\ttraining's f1_score: 0.881171\n",
      "[599]\ttraining's multi_logloss: 0.414536\ttraining's f1_score: 0.881504\n",
      "[600]\ttraining's multi_logloss: 0.414194\ttraining's f1_score: 0.881628\n",
      "[601]\ttraining's multi_logloss: 0.41385\ttraining's f1_score: 0.881836\n",
      "[602]\ttraining's multi_logloss: 0.413499\ttraining's f1_score: 0.882168\n",
      "[603]\ttraining's multi_logloss: 0.413151\ttraining's f1_score: 0.882314\n",
      "[604]\ttraining's multi_logloss: 0.412813\ttraining's f1_score: 0.88248\n",
      "[605]\ttraining's multi_logloss: 0.412472\ttraining's f1_score: 0.882708\n",
      "[606]\ttraining's multi_logloss: 0.412124\ttraining's f1_score: 0.882895\n",
      "[607]\ttraining's multi_logloss: 0.411784\ttraining's f1_score: 0.883228\n",
      "[608]\ttraining's multi_logloss: 0.411439\ttraining's f1_score: 0.883415\n",
      "[609]\ttraining's multi_logloss: 0.411102\ttraining's f1_score: 0.883519\n",
      "[610]\ttraining's multi_logloss: 0.410753\ttraining's f1_score: 0.88356\n",
      "[611]\ttraining's multi_logloss: 0.410416\ttraining's f1_score: 0.883685\n",
      "[612]\ttraining's multi_logloss: 0.41008\ttraining's f1_score: 0.884142\n",
      "[613]\ttraining's multi_logloss: 0.40975\ttraining's f1_score: 0.884329\n",
      "[614]\ttraining's multi_logloss: 0.409405\ttraining's f1_score: 0.88464\n",
      "[615]\ttraining's multi_logloss: 0.409067\ttraining's f1_score: 0.884723\n",
      "[616]\ttraining's multi_logloss: 0.408727\ttraining's f1_score: 0.885118\n",
      "[617]\ttraining's multi_logloss: 0.408385\ttraining's f1_score: 0.88545\n",
      "[618]\ttraining's multi_logloss: 0.40804\ttraining's f1_score: 0.885554\n",
      "[619]\ttraining's multi_logloss: 0.407713\ttraining's f1_score: 0.885783\n",
      "[620]\ttraining's multi_logloss: 0.407368\ttraining's f1_score: 0.885845\n",
      "[621]\ttraining's multi_logloss: 0.407022\ttraining's f1_score: 0.88599\n",
      "[622]\ttraining's multi_logloss: 0.406687\ttraining's f1_score: 0.886219\n",
      "[623]\ttraining's multi_logloss: 0.406332\ttraining's f1_score: 0.886406\n",
      "[624]\ttraining's multi_logloss: 0.405996\ttraining's f1_score: 0.886572\n",
      "[625]\ttraining's multi_logloss: 0.405663\ttraining's f1_score: 0.88678\n",
      "[626]\ttraining's multi_logloss: 0.405328\ttraining's f1_score: 0.886883\n",
      "[627]\ttraining's multi_logloss: 0.404982\ttraining's f1_score: 0.886946\n",
      "[628]\ttraining's multi_logloss: 0.404649\ttraining's f1_score: 0.88707\n",
      "[629]\ttraining's multi_logloss: 0.404309\ttraining's f1_score: 0.887527\n",
      "[630]\ttraining's multi_logloss: 0.403984\ttraining's f1_score: 0.88759\n",
      "[631]\ttraining's multi_logloss: 0.40366\ttraining's f1_score: 0.887797\n",
      "[632]\ttraining's multi_logloss: 0.403333\ttraining's f1_score: 0.887963\n",
      "[633]\ttraining's multi_logloss: 0.403015\ttraining's f1_score: 0.888171\n",
      "[634]\ttraining's multi_logloss: 0.402682\ttraining's f1_score: 0.888379\n",
      "[635]\ttraining's multi_logloss: 0.402355\ttraining's f1_score: 0.888566\n",
      "[636]\ttraining's multi_logloss: 0.402027\ttraining's f1_score: 0.888836\n",
      "[637]\ttraining's multi_logloss: 0.401702\ttraining's f1_score: 0.88894\n",
      "[638]\ttraining's multi_logloss: 0.401371\ttraining's f1_score: 0.889168\n",
      "[639]\ttraining's multi_logloss: 0.401051\ttraining's f1_score: 0.889376\n",
      "[640]\ttraining's multi_logloss: 0.400717\ttraining's f1_score: 0.889542\n",
      "[641]\ttraining's multi_logloss: 0.400394\ttraining's f1_score: 0.889625\n",
      "[642]\ttraining's multi_logloss: 0.400076\ttraining's f1_score: 0.889708\n",
      "[643]\ttraining's multi_logloss: 0.399752\ttraining's f1_score: 0.889874\n",
      "[644]\ttraining's multi_logloss: 0.399419\ttraining's f1_score: 0.890227\n",
      "[645]\ttraining's multi_logloss: 0.399085\ttraining's f1_score: 0.890352\n",
      "[646]\ttraining's multi_logloss: 0.398761\ttraining's f1_score: 0.890518\n",
      "[647]\ttraining's multi_logloss: 0.398428\ttraining's f1_score: 0.890601\n",
      "[648]\ttraining's multi_logloss: 0.398102\ttraining's f1_score: 0.890705\n",
      "[649]\ttraining's multi_logloss: 0.397787\ttraining's f1_score: 0.891141\n",
      "[650]\ttraining's multi_logloss: 0.39747\ttraining's f1_score: 0.891245\n",
      "[651]\ttraining's multi_logloss: 0.397148\ttraining's f1_score: 0.891474\n",
      "[652]\ttraining's multi_logloss: 0.396834\ttraining's f1_score: 0.891474\n",
      "[653]\ttraining's multi_logloss: 0.396511\ttraining's f1_score: 0.891661\n",
      "[654]\ttraining's multi_logloss: 0.396194\ttraining's f1_score: 0.891951\n",
      "[655]\ttraining's multi_logloss: 0.395876\ttraining's f1_score: 0.892076\n",
      "[656]\ttraining's multi_logloss: 0.395555\ttraining's f1_score: 0.892221\n",
      "[657]\ttraining's multi_logloss: 0.395245\ttraining's f1_score: 0.892346\n",
      "[658]\ttraining's multi_logloss: 0.394927\ttraining's f1_score: 0.892388\n",
      "[659]\ttraining's multi_logloss: 0.394609\ttraining's f1_score: 0.892658\n",
      "[660]\ttraining's multi_logloss: 0.394283\ttraining's f1_score: 0.892886\n",
      "[661]\ttraining's multi_logloss: 0.393964\ttraining's f1_score: 0.893135\n",
      "[662]\ttraining's multi_logloss: 0.393645\ttraining's f1_score: 0.893468\n",
      "[663]\ttraining's multi_logloss: 0.393327\ttraining's f1_score: 0.893634\n",
      "[664]\ttraining's multi_logloss: 0.393012\ttraining's f1_score: 0.893675\n",
      "[665]\ttraining's multi_logloss: 0.392693\ttraining's f1_score: 0.893862\n",
      "[666]\ttraining's multi_logloss: 0.392371\ttraining's f1_score: 0.894008\n",
      "[667]\ttraining's multi_logloss: 0.39205\ttraining's f1_score: 0.894195\n",
      "[668]\ttraining's multi_logloss: 0.391741\ttraining's f1_score: 0.894423\n",
      "[669]\ttraining's multi_logloss: 0.391423\ttraining's f1_score: 0.894465\n",
      "[670]\ttraining's multi_logloss: 0.391108\ttraining's f1_score: 0.894631\n",
      "[671]\ttraining's multi_logloss: 0.390788\ttraining's f1_score: 0.894776\n",
      "[672]\ttraining's multi_logloss: 0.390471\ttraining's f1_score: 0.895005\n",
      "[673]\ttraining's multi_logloss: 0.390164\ttraining's f1_score: 0.895171\n",
      "[674]\ttraining's multi_logloss: 0.389845\ttraining's f1_score: 0.895337\n",
      "[675]\ttraining's multi_logloss: 0.389537\ttraining's f1_score: 0.895586\n",
      "[676]\ttraining's multi_logloss: 0.389228\ttraining's f1_score: 0.895649\n",
      "[677]\ttraining's multi_logloss: 0.388911\ttraining's f1_score: 0.895815\n",
      "[678]\ttraining's multi_logloss: 0.388602\ttraining's f1_score: 0.896022\n",
      "[679]\ttraining's multi_logloss: 0.38829\ttraining's f1_score: 0.896147\n",
      "[680]\ttraining's multi_logloss: 0.387982\ttraining's f1_score: 0.896272\n",
      "[681]\ttraining's multi_logloss: 0.387673\ttraining's f1_score: 0.896479\n",
      "[682]\ttraining's multi_logloss: 0.387364\ttraining's f1_score: 0.896708\n",
      "[683]\ttraining's multi_logloss: 0.387052\ttraining's f1_score: 0.896832\n",
      "[684]\ttraining's multi_logloss: 0.386743\ttraining's f1_score: 0.897123\n",
      "[685]\ttraining's multi_logloss: 0.386428\ttraining's f1_score: 0.897289\n",
      "[686]\ttraining's multi_logloss: 0.386115\ttraining's f1_score: 0.897476\n",
      "[687]\ttraining's multi_logloss: 0.385808\ttraining's f1_score: 0.897559\n",
      "[688]\ttraining's multi_logloss: 0.385503\ttraining's f1_score: 0.897726\n",
      "[689]\ttraining's multi_logloss: 0.385206\ttraining's f1_score: 0.897767\n",
      "[690]\ttraining's multi_logloss: 0.384897\ttraining's f1_score: 0.898016\n",
      "[691]\ttraining's multi_logloss: 0.384594\ttraining's f1_score: 0.898245\n",
      "[692]\ttraining's multi_logloss: 0.384301\ttraining's f1_score: 0.898432\n",
      "[693]\ttraining's multi_logloss: 0.383996\ttraining's f1_score: 0.898536\n",
      "[694]\ttraining's multi_logloss: 0.383685\ttraining's f1_score: 0.898806\n",
      "[695]\ttraining's multi_logloss: 0.383386\ttraining's f1_score: 0.898889\n",
      "[696]\ttraining's multi_logloss: 0.383083\ttraining's f1_score: 0.89891\n",
      "[697]\ttraining's multi_logloss: 0.38278\ttraining's f1_score: 0.89918\n",
      "[698]\ttraining's multi_logloss: 0.382487\ttraining's f1_score: 0.899283\n",
      "[699]\ttraining's multi_logloss: 0.382188\ttraining's f1_score: 0.899657\n",
      "[700]\ttraining's multi_logloss: 0.381892\ttraining's f1_score: 0.899699\n",
      "[701]\ttraining's multi_logloss: 0.38161\ttraining's f1_score: 0.899865\n",
      "[702]\ttraining's multi_logloss: 0.38132\ttraining's f1_score: 0.900031\n",
      "[703]\ttraining's multi_logloss: 0.381028\ttraining's f1_score: 0.900177\n",
      "[704]\ttraining's multi_logloss: 0.380736\ttraining's f1_score: 0.90028\n",
      "[705]\ttraining's multi_logloss: 0.380449\ttraining's f1_score: 0.900301\n",
      "[706]\ttraining's multi_logloss: 0.380156\ttraining's f1_score: 0.900405\n",
      "[707]\ttraining's multi_logloss: 0.379868\ttraining's f1_score: 0.900592\n",
      "[708]\ttraining's multi_logloss: 0.379571\ttraining's f1_score: 0.900779\n",
      "[709]\ttraining's multi_logloss: 0.379283\ttraining's f1_score: 0.900904\n",
      "[710]\ttraining's multi_logloss: 0.378987\ttraining's f1_score: 0.901153\n",
      "[711]\ttraining's multi_logloss: 0.378704\ttraining's f1_score: 0.90134\n",
      "[712]\ttraining's multi_logloss: 0.378406\ttraining's f1_score: 0.901547\n",
      "[713]\ttraining's multi_logloss: 0.378122\ttraining's f1_score: 0.901797\n",
      "[714]\ttraining's multi_logloss: 0.377834\ttraining's f1_score: 0.901755\n",
      "[715]\ttraining's multi_logloss: 0.377531\ttraining's f1_score: 0.901942\n",
      "[716]\ttraining's multi_logloss: 0.377251\ttraining's f1_score: 0.902108\n",
      "[717]\ttraining's multi_logloss: 0.376953\ttraining's f1_score: 0.902212\n",
      "[718]\ttraining's multi_logloss: 0.376664\ttraining's f1_score: 0.902441\n",
      "[719]\ttraining's multi_logloss: 0.376378\ttraining's f1_score: 0.902461\n",
      "[720]\ttraining's multi_logloss: 0.376075\ttraining's f1_score: 0.902648\n",
      "[721]\ttraining's multi_logloss: 0.375779\ttraining's f1_score: 0.902814\n",
      "[722]\ttraining's multi_logloss: 0.375498\ttraining's f1_score: 0.903188\n",
      "[723]\ttraining's multi_logloss: 0.375206\ttraining's f1_score: 0.903209\n",
      "[724]\ttraining's multi_logloss: 0.374924\ttraining's f1_score: 0.903292\n",
      "[725]\ttraining's multi_logloss: 0.374643\ttraining's f1_score: 0.903417\n",
      "[726]\ttraining's multi_logloss: 0.374352\ttraining's f1_score: 0.903687\n",
      "[727]\ttraining's multi_logloss: 0.374078\ttraining's f1_score: 0.90377\n",
      "[728]\ttraining's multi_logloss: 0.373801\ttraining's f1_score: 0.903936\n",
      "[729]\ttraining's multi_logloss: 0.373519\ttraining's f1_score: 0.904081\n",
      "[730]\ttraining's multi_logloss: 0.373235\ttraining's f1_score: 0.904185\n",
      "[731]\ttraining's multi_logloss: 0.372957\ttraining's f1_score: 0.904248\n",
      "[732]\ttraining's multi_logloss: 0.372683\ttraining's f1_score: 0.904206\n",
      "[733]\ttraining's multi_logloss: 0.372394\ttraining's f1_score: 0.904476\n",
      "[734]\ttraining's multi_logloss: 0.372115\ttraining's f1_score: 0.904621\n",
      "[735]\ttraining's multi_logloss: 0.371835\ttraining's f1_score: 0.904705\n",
      "[736]\ttraining's multi_logloss: 0.371549\ttraining's f1_score: 0.904912\n",
      "[737]\ttraining's multi_logloss: 0.371266\ttraining's f1_score: 0.905078\n",
      "[738]\ttraining's multi_logloss: 0.370982\ttraining's f1_score: 0.905286\n",
      "[739]\ttraining's multi_logloss: 0.370705\ttraining's f1_score: 0.905473\n",
      "[740]\ttraining's multi_logloss: 0.370417\ttraining's f1_score: 0.905639\n",
      "[741]\ttraining's multi_logloss: 0.370127\ttraining's f1_score: 0.905951\n",
      "[742]\ttraining's multi_logloss: 0.369845\ttraining's f1_score: 0.906096\n",
      "[743]\ttraining's multi_logloss: 0.369558\ttraining's f1_score: 0.906262\n",
      "[744]\ttraining's multi_logloss: 0.369276\ttraining's f1_score: 0.906428\n",
      "[745]\ttraining's multi_logloss: 0.368993\ttraining's f1_score: 0.906802\n",
      "[746]\ttraining's multi_logloss: 0.368705\ttraining's f1_score: 0.907052\n",
      "[747]\ttraining's multi_logloss: 0.368426\ttraining's f1_score: 0.907176\n",
      "[748]\ttraining's multi_logloss: 0.368148\ttraining's f1_score: 0.907446\n",
      "[749]\ttraining's multi_logloss: 0.367862\ttraining's f1_score: 0.90755\n",
      "[750]\ttraining's multi_logloss: 0.367579\ttraining's f1_score: 0.907654\n",
      "[751]\ttraining's multi_logloss: 0.367301\ttraining's f1_score: 0.907779\n",
      "[752]\ttraining's multi_logloss: 0.367015\ttraining's f1_score: 0.907841\n",
      "[753]\ttraining's multi_logloss: 0.366736\ttraining's f1_score: 0.908007\n",
      "[754]\ttraining's multi_logloss: 0.366463\ttraining's f1_score: 0.908111\n",
      "[755]\ttraining's multi_logloss: 0.366183\ttraining's f1_score: 0.908111\n",
      "[756]\ttraining's multi_logloss: 0.36592\ttraining's f1_score: 0.908111\n",
      "[757]\ttraining's multi_logloss: 0.365644\ttraining's f1_score: 0.908298\n",
      "[758]\ttraining's multi_logloss: 0.365378\ttraining's f1_score: 0.908568\n",
      "[759]\ttraining's multi_logloss: 0.365098\ttraining's f1_score: 0.908692\n",
      "[760]\ttraining's multi_logloss: 0.364836\ttraining's f1_score: 0.908817\n",
      "[761]\ttraining's multi_logloss: 0.364562\ttraining's f1_score: 0.908963\n",
      "[762]\ttraining's multi_logloss: 0.364287\ttraining's f1_score: 0.909295\n",
      "[763]\ttraining's multi_logloss: 0.364011\ttraining's f1_score: 0.909357\n",
      "[764]\ttraining's multi_logloss: 0.363737\ttraining's f1_score: 0.909482\n",
      "[765]\ttraining's multi_logloss: 0.363466\ttraining's f1_score: 0.909731\n",
      "[766]\ttraining's multi_logloss: 0.363202\ttraining's f1_score: 0.909752\n",
      "[767]\ttraining's multi_logloss: 0.362913\ttraining's f1_score: 0.910043\n",
      "[768]\ttraining's multi_logloss: 0.362634\ttraining's f1_score: 0.910063\n",
      "[769]\ttraining's multi_logloss: 0.362347\ttraining's f1_score: 0.91023\n",
      "[770]\ttraining's multi_logloss: 0.362071\ttraining's f1_score: 0.910396\n",
      "[771]\ttraining's multi_logloss: 0.361784\ttraining's f1_score: 0.910707\n",
      "[772]\ttraining's multi_logloss: 0.361503\ttraining's f1_score: 0.910811\n",
      "[773]\ttraining's multi_logloss: 0.361229\ttraining's f1_score: 0.910998\n",
      "[774]\ttraining's multi_logloss: 0.360938\ttraining's f1_score: 0.910956\n",
      "[775]\ttraining's multi_logloss: 0.360661\ttraining's f1_score: 0.911247\n",
      "[776]\ttraining's multi_logloss: 0.360392\ttraining's f1_score: 0.91133\n",
      "[777]\ttraining's multi_logloss: 0.360123\ttraining's f1_score: 0.911413\n",
      "[778]\ttraining's multi_logloss: 0.359857\ttraining's f1_score: 0.911663\n",
      "[779]\ttraining's multi_logloss: 0.359595\ttraining's f1_score: 0.911767\n",
      "[780]\ttraining's multi_logloss: 0.35932\ttraining's f1_score: 0.91185\n",
      "[781]\ttraining's multi_logloss: 0.359053\ttraining's f1_score: 0.911933\n",
      "[782]\ttraining's multi_logloss: 0.358778\ttraining's f1_score: 0.912161\n",
      "[783]\ttraining's multi_logloss: 0.358505\ttraining's f1_score: 0.912099\n",
      "[784]\ttraining's multi_logloss: 0.358229\ttraining's f1_score: 0.912244\n",
      "[785]\ttraining's multi_logloss: 0.357963\ttraining's f1_score: 0.912307\n",
      "[786]\ttraining's multi_logloss: 0.357698\ttraining's f1_score: 0.912535\n",
      "[787]\ttraining's multi_logloss: 0.357442\ttraining's f1_score: 0.912618\n",
      "[788]\ttraining's multi_logloss: 0.357178\ttraining's f1_score: 0.912784\n",
      "[789]\ttraining's multi_logloss: 0.356915\ttraining's f1_score: 0.912909\n",
      "[790]\ttraining's multi_logloss: 0.356648\ttraining's f1_score: 0.913054\n",
      "[791]\ttraining's multi_logloss: 0.356382\ttraining's f1_score: 0.913304\n",
      "[792]\ttraining's multi_logloss: 0.356123\ttraining's f1_score: 0.913594\n",
      "[793]\ttraining's multi_logloss: 0.355858\ttraining's f1_score: 0.91374\n",
      "[794]\ttraining's multi_logloss: 0.355598\ttraining's f1_score: 0.913927\n",
      "[795]\ttraining's multi_logloss: 0.355335\ttraining's f1_score: 0.913989\n",
      "[796]\ttraining's multi_logloss: 0.35508\ttraining's f1_score: 0.914176\n",
      "[797]\ttraining's multi_logloss: 0.354818\ttraining's f1_score: 0.914321\n",
      "[798]\ttraining's multi_logloss: 0.354559\ttraining's f1_score: 0.914384\n",
      "[799]\ttraining's multi_logloss: 0.354294\ttraining's f1_score: 0.914612\n",
      "[800]\ttraining's multi_logloss: 0.354035\ttraining's f1_score: 0.914633\n",
      "[801]\ttraining's multi_logloss: 0.353774\ttraining's f1_score: 0.914778\n",
      "[802]\ttraining's multi_logloss: 0.35351\ttraining's f1_score: 0.914903\n",
      "[803]\ttraining's multi_logloss: 0.353256\ttraining's f1_score: 0.915048\n",
      "[804]\ttraining's multi_logloss: 0.352992\ttraining's f1_score: 0.915194\n",
      "[805]\ttraining's multi_logloss: 0.352736\ttraining's f1_score: 0.915298\n",
      "[806]\ttraining's multi_logloss: 0.352485\ttraining's f1_score: 0.915526\n",
      "[807]\ttraining's multi_logloss: 0.352227\ttraining's f1_score: 0.915671\n",
      "[808]\ttraining's multi_logloss: 0.35196\ttraining's f1_score: 0.915671\n",
      "[809]\ttraining's multi_logloss: 0.351693\ttraining's f1_score: 0.915754\n",
      "[810]\ttraining's multi_logloss: 0.351436\ttraining's f1_score: 0.915983\n",
      "[811]\ttraining's multi_logloss: 0.351183\ttraining's f1_score: 0.916149\n",
      "[812]\ttraining's multi_logloss: 0.350917\ttraining's f1_score: 0.916295\n",
      "[813]\ttraining's multi_logloss: 0.350661\ttraining's f1_score: 0.916398\n",
      "[814]\ttraining's multi_logloss: 0.350417\ttraining's f1_score: 0.91644\n",
      "[815]\ttraining's multi_logloss: 0.350174\ttraining's f1_score: 0.916523\n",
      "[816]\ttraining's multi_logloss: 0.349927\ttraining's f1_score: 0.916668\n",
      "[817]\ttraining's multi_logloss: 0.349667\ttraining's f1_score: 0.916793\n",
      "[818]\ttraining's multi_logloss: 0.349412\ttraining's f1_score: 0.916876\n",
      "[819]\ttraining's multi_logloss: 0.349163\ttraining's f1_score: 0.917001\n",
      "[820]\ttraining's multi_logloss: 0.348902\ttraining's f1_score: 0.91725\n",
      "[821]\ttraining's multi_logloss: 0.348657\ttraining's f1_score: 0.91725\n",
      "[822]\ttraining's multi_logloss: 0.348406\ttraining's f1_score: 0.917292\n",
      "[823]\ttraining's multi_logloss: 0.348145\ttraining's f1_score: 0.917437\n",
      "[824]\ttraining's multi_logloss: 0.347889\ttraining's f1_score: 0.917499\n",
      "[825]\ttraining's multi_logloss: 0.347642\ttraining's f1_score: 0.91752\n",
      "[826]\ttraining's multi_logloss: 0.34737\ttraining's f1_score: 0.91779\n",
      "[827]\ttraining's multi_logloss: 0.347121\ttraining's f1_score: 0.91779\n",
      "[828]\ttraining's multi_logloss: 0.346878\ttraining's f1_score: 0.917998\n",
      "[829]\ttraining's multi_logloss: 0.346625\ttraining's f1_score: 0.918268\n",
      "[830]\ttraining's multi_logloss: 0.346375\ttraining's f1_score: 0.918351\n",
      "[831]\ttraining's multi_logloss: 0.346125\ttraining's f1_score: 0.918538\n",
      "[832]\ttraining's multi_logloss: 0.345877\ttraining's f1_score: 0.918662\n",
      "[833]\ttraining's multi_logloss: 0.345633\ttraining's f1_score: 0.918683\n",
      "[834]\ttraining's multi_logloss: 0.345392\ttraining's f1_score: 0.918808\n",
      "[835]\ttraining's multi_logloss: 0.345148\ttraining's f1_score: 0.918891\n",
      "[836]\ttraining's multi_logloss: 0.344888\ttraining's f1_score: 0.918912\n",
      "[837]\ttraining's multi_logloss: 0.344647\ttraining's f1_score: 0.919161\n",
      "[838]\ttraining's multi_logloss: 0.344407\ttraining's f1_score: 0.91941\n",
      "[839]\ttraining's multi_logloss: 0.344158\ttraining's f1_score: 0.919493\n",
      "[840]\ttraining's multi_logloss: 0.343906\ttraining's f1_score: 0.919597\n",
      "[841]\ttraining's multi_logloss: 0.34364\ttraining's f1_score: 0.919701\n",
      "[842]\ttraining's multi_logloss: 0.34339\ttraining's f1_score: 0.919784\n",
      "[843]\ttraining's multi_logloss: 0.343137\ttraining's f1_score: 0.919888\n",
      "[844]\ttraining's multi_logloss: 0.342879\ttraining's f1_score: 0.920012\n",
      "[845]\ttraining's multi_logloss: 0.342621\ttraining's f1_score: 0.920116\n",
      "[846]\ttraining's multi_logloss: 0.342374\ttraining's f1_score: 0.92022\n",
      "[847]\ttraining's multi_logloss: 0.342132\ttraining's f1_score: 0.920428\n",
      "[848]\ttraining's multi_logloss: 0.341888\ttraining's f1_score: 0.920552\n",
      "[849]\ttraining's multi_logloss: 0.341627\ttraining's f1_score: 0.920698\n",
      "[850]\ttraining's multi_logloss: 0.34137\ttraining's f1_score: 0.92103\n",
      "[851]\ttraining's multi_logloss: 0.341117\ttraining's f1_score: 0.921155\n",
      "[852]\ttraining's multi_logloss: 0.340876\ttraining's f1_score: 0.921466\n",
      "[853]\ttraining's multi_logloss: 0.34063\ttraining's f1_score: 0.921508\n",
      "[854]\ttraining's multi_logloss: 0.340395\ttraining's f1_score: 0.921778\n",
      "[855]\ttraining's multi_logloss: 0.340146\ttraining's f1_score: 0.92182\n",
      "[856]\ttraining's multi_logloss: 0.339905\ttraining's f1_score: 0.921923\n",
      "[857]\ttraining's multi_logloss: 0.339652\ttraining's f1_score: 0.922131\n",
      "[858]\ttraining's multi_logloss: 0.339424\ttraining's f1_score: 0.922173\n",
      "[859]\ttraining's multi_logloss: 0.339171\ttraining's f1_score: 0.922318\n",
      "[860]\ttraining's multi_logloss: 0.338925\ttraining's f1_score: 0.922401\n",
      "[861]\ttraining's multi_logloss: 0.338688\ttraining's f1_score: 0.922567\n",
      "[862]\ttraining's multi_logloss: 0.338433\ttraining's f1_score: 0.922692\n",
      "[863]\ttraining's multi_logloss: 0.33818\ttraining's f1_score: 0.922983\n",
      "[864]\ttraining's multi_logloss: 0.337944\ttraining's f1_score: 0.923107\n",
      "[865]\ttraining's multi_logloss: 0.337698\ttraining's f1_score: 0.923149\n",
      "[866]\ttraining's multi_logloss: 0.337459\ttraining's f1_score: 0.923211\n",
      "[867]\ttraining's multi_logloss: 0.337219\ttraining's f1_score: 0.92344\n",
      "[868]\ttraining's multi_logloss: 0.336975\ttraining's f1_score: 0.92344\n",
      "[869]\ttraining's multi_logloss: 0.336735\ttraining's f1_score: 0.92371\n",
      "[870]\ttraining's multi_logloss: 0.336487\ttraining's f1_score: 0.923917\n",
      "[871]\ttraining's multi_logloss: 0.336249\ttraining's f1_score: 0.92398\n",
      "[872]\ttraining's multi_logloss: 0.336007\ttraining's f1_score: 0.924042\n",
      "[873]\ttraining's multi_logloss: 0.335757\ttraining's f1_score: 0.924125\n",
      "[874]\ttraining's multi_logloss: 0.335521\ttraining's f1_score: 0.92427\n",
      "[875]\ttraining's multi_logloss: 0.335283\ttraining's f1_score: 0.924229\n",
      "[876]\ttraining's multi_logloss: 0.335028\ttraining's f1_score: 0.924437\n",
      "[877]\ttraining's multi_logloss: 0.334776\ttraining's f1_score: 0.924686\n",
      "[878]\ttraining's multi_logloss: 0.334528\ttraining's f1_score: 0.924977\n",
      "[879]\ttraining's multi_logloss: 0.334284\ttraining's f1_score: 0.924997\n",
      "[880]\ttraining's multi_logloss: 0.334038\ttraining's f1_score: 0.925143\n",
      "[881]\ttraining's multi_logloss: 0.333794\ttraining's f1_score: 0.92533\n",
      "[882]\ttraining's multi_logloss: 0.333552\ttraining's f1_score: 0.925413\n",
      "[883]\ttraining's multi_logloss: 0.333332\ttraining's f1_score: 0.925641\n",
      "[884]\ttraining's multi_logloss: 0.333092\ttraining's f1_score: 0.925662\n",
      "[885]\ttraining's multi_logloss: 0.332856\ttraining's f1_score: 0.925911\n",
      "[886]\ttraining's multi_logloss: 0.332613\ttraining's f1_score: 0.926036\n",
      "[887]\ttraining's multi_logloss: 0.332365\ttraining's f1_score: 0.926161\n",
      "[888]\ttraining's multi_logloss: 0.332145\ttraining's f1_score: 0.926057\n",
      "[889]\ttraining's multi_logloss: 0.33191\ttraining's f1_score: 0.92614\n",
      "[890]\ttraining's multi_logloss: 0.331668\ttraining's f1_score: 0.926389\n",
      "[891]\ttraining's multi_logloss: 0.331434\ttraining's f1_score: 0.926493\n",
      "[892]\ttraining's multi_logloss: 0.331201\ttraining's f1_score: 0.926721\n",
      "[893]\ttraining's multi_logloss: 0.330967\ttraining's f1_score: 0.926701\n",
      "[894]\ttraining's multi_logloss: 0.330738\ttraining's f1_score: 0.926846\n",
      "[895]\ttraining's multi_logloss: 0.330512\ttraining's f1_score: 0.927054\n",
      "[896]\ttraining's multi_logloss: 0.330286\ttraining's f1_score: 0.927074\n",
      "[897]\ttraining's multi_logloss: 0.330049\ttraining's f1_score: 0.927178\n",
      "[898]\ttraining's multi_logloss: 0.329807\ttraining's f1_score: 0.927282\n",
      "[899]\ttraining's multi_logloss: 0.329563\ttraining's f1_score: 0.927511\n",
      "[900]\ttraining's multi_logloss: 0.329336\ttraining's f1_score: 0.927677\n",
      "[901]\ttraining's multi_logloss: 0.329095\ttraining's f1_score: 0.927988\n",
      "[902]\ttraining's multi_logloss: 0.328865\ttraining's f1_score: 0.928113\n",
      "[903]\ttraining's multi_logloss: 0.328641\ttraining's f1_score: 0.9283\n",
      "[904]\ttraining's multi_logloss: 0.328401\ttraining's f1_score: 0.928362\n",
      "[905]\ttraining's multi_logloss: 0.32818\ttraining's f1_score: 0.928466\n",
      "[906]\ttraining's multi_logloss: 0.327949\ttraining's f1_score: 0.928611\n",
      "[907]\ttraining's multi_logloss: 0.327712\ttraining's f1_score: 0.928778\n",
      "[908]\ttraining's multi_logloss: 0.327476\ttraining's f1_score: 0.92884\n",
      "[909]\ttraining's multi_logloss: 0.327251\ttraining's f1_score: 0.928861\n",
      "[910]\ttraining's multi_logloss: 0.327013\ttraining's f1_score: 0.929089\n",
      "[911]\ttraining's multi_logloss: 0.326789\ttraining's f1_score: 0.929193\n",
      "[912]\ttraining's multi_logloss: 0.326557\ttraining's f1_score: 0.929297\n",
      "[913]\ttraining's multi_logloss: 0.326336\ttraining's f1_score: 0.929276\n",
      "[914]\ttraining's multi_logloss: 0.326093\ttraining's f1_score: 0.929401\n",
      "[915]\ttraining's multi_logloss: 0.325866\ttraining's f1_score: 0.929525\n",
      "[916]\ttraining's multi_logloss: 0.325639\ttraining's f1_score: 0.929608\n",
      "[917]\ttraining's multi_logloss: 0.325412\ttraining's f1_score: 0.929754\n",
      "[918]\ttraining's multi_logloss: 0.325178\ttraining's f1_score: 0.929941\n",
      "[919]\ttraining's multi_logloss: 0.324954\ttraining's f1_score: 0.929941\n",
      "[920]\ttraining's multi_logloss: 0.32472\ttraining's f1_score: 0.930086\n",
      "[921]\ttraining's multi_logloss: 0.324492\ttraining's f1_score: 0.930335\n",
      "[922]\ttraining's multi_logloss: 0.324265\ttraining's f1_score: 0.930543\n",
      "[923]\ttraining's multi_logloss: 0.324028\ttraining's f1_score: 0.93073\n",
      "[924]\ttraining's multi_logloss: 0.323805\ttraining's f1_score: 0.930813\n",
      "[925]\ttraining's multi_logloss: 0.323587\ttraining's f1_score: 0.930896\n",
      "[926]\ttraining's multi_logloss: 0.323368\ttraining's f1_score: 0.931083\n",
      "[927]\ttraining's multi_logloss: 0.323143\ttraining's f1_score: 0.931291\n",
      "[928]\ttraining's multi_logloss: 0.32292\ttraining's f1_score: 0.931353\n",
      "[929]\ttraining's multi_logloss: 0.322705\ttraining's f1_score: 0.931416\n",
      "[930]\ttraining's multi_logloss: 0.322489\ttraining's f1_score: 0.931519\n",
      "[931]\ttraining's multi_logloss: 0.322269\ttraining's f1_score: 0.931602\n",
      "[932]\ttraining's multi_logloss: 0.322054\ttraining's f1_score: 0.931623\n",
      "[933]\ttraining's multi_logloss: 0.321827\ttraining's f1_score: 0.931748\n",
      "[934]\ttraining's multi_logloss: 0.321607\ttraining's f1_score: 0.931935\n",
      "[935]\ttraining's multi_logloss: 0.321388\ttraining's f1_score: 0.931976\n",
      "[936]\ttraining's multi_logloss: 0.321172\ttraining's f1_score: 0.931935\n",
      "[937]\ttraining's multi_logloss: 0.320942\ttraining's f1_score: 0.932059\n",
      "[938]\ttraining's multi_logloss: 0.320719\ttraining's f1_score: 0.932059\n",
      "[939]\ttraining's multi_logloss: 0.320501\ttraining's f1_score: 0.932246\n",
      "[940]\ttraining's multi_logloss: 0.320291\ttraining's f1_score: 0.932371\n",
      "[941]\ttraining's multi_logloss: 0.32006\ttraining's f1_score: 0.932454\n",
      "[942]\ttraining's multi_logloss: 0.319838\ttraining's f1_score: 0.932516\n",
      "[943]\ttraining's multi_logloss: 0.319607\ttraining's f1_score: 0.932828\n",
      "[944]\ttraining's multi_logloss: 0.319384\ttraining's f1_score: 0.932953\n",
      "[945]\ttraining's multi_logloss: 0.319155\ttraining's f1_score: 0.933077\n",
      "[946]\ttraining's multi_logloss: 0.318934\ttraining's f1_score: 0.933098\n",
      "[947]\ttraining's multi_logloss: 0.31872\ttraining's f1_score: 0.933285\n",
      "[948]\ttraining's multi_logloss: 0.318499\ttraining's f1_score: 0.933493\n",
      "[949]\ttraining's multi_logloss: 0.318273\ttraining's f1_score: 0.933638\n",
      "[950]\ttraining's multi_logloss: 0.318046\ttraining's f1_score: 0.93368\n",
      "[951]\ttraining's multi_logloss: 0.317825\ttraining's f1_score: 0.933721\n",
      "[952]\ttraining's multi_logloss: 0.317602\ttraining's f1_score: 0.933929\n",
      "[953]\ttraining's multi_logloss: 0.317378\ttraining's f1_score: 0.934053\n",
      "[954]\ttraining's multi_logloss: 0.317165\ttraining's f1_score: 0.934136\n",
      "[955]\ttraining's multi_logloss: 0.316945\ttraining's f1_score: 0.934178\n",
      "[956]\ttraining's multi_logloss: 0.316731\ttraining's f1_score: 0.934303\n",
      "[957]\ttraining's multi_logloss: 0.316509\ttraining's f1_score: 0.934344\n",
      "[958]\ttraining's multi_logloss: 0.316292\ttraining's f1_score: 0.93451\n",
      "[959]\ttraining's multi_logloss: 0.316069\ttraining's f1_score: 0.934739\n",
      "[960]\ttraining's multi_logloss: 0.315856\ttraining's f1_score: 0.93478\n",
      "[961]\ttraining's multi_logloss: 0.315629\ttraining's f1_score: 0.934884\n",
      "[962]\ttraining's multi_logloss: 0.315411\ttraining's f1_score: 0.935217\n",
      "[963]\ttraining's multi_logloss: 0.315188\ttraining's f1_score: 0.935217\n",
      "[964]\ttraining's multi_logloss: 0.31498\ttraining's f1_score: 0.935424\n",
      "[965]\ttraining's multi_logloss: 0.314769\ttraining's f1_score: 0.935466\n",
      "[966]\ttraining's multi_logloss: 0.314557\ttraining's f1_score: 0.935611\n",
      "[967]\ttraining's multi_logloss: 0.314336\ttraining's f1_score: 0.93584\n",
      "[968]\ttraining's multi_logloss: 0.314112\ttraining's f1_score: 0.935881\n",
      "[969]\ttraining's multi_logloss: 0.313901\ttraining's f1_score: 0.936047\n",
      "[970]\ttraining's multi_logloss: 0.313678\ttraining's f1_score: 0.93611\n",
      "[971]\ttraining's multi_logloss: 0.313463\ttraining's f1_score: 0.93611\n",
      "[972]\ttraining's multi_logloss: 0.313247\ttraining's f1_score: 0.936297\n",
      "[973]\ttraining's multi_logloss: 0.313027\ttraining's f1_score: 0.936317\n",
      "[974]\ttraining's multi_logloss: 0.312818\ttraining's f1_score: 0.936463\n",
      "[975]\ttraining's multi_logloss: 0.312598\ttraining's f1_score: 0.936442\n",
      "[976]\ttraining's multi_logloss: 0.312389\ttraining's f1_score: 0.936567\n",
      "[977]\ttraining's multi_logloss: 0.312188\ttraining's f1_score: 0.93665\n",
      "[978]\ttraining's multi_logloss: 0.311977\ttraining's f1_score: 0.936712\n",
      "[979]\ttraining's multi_logloss: 0.311765\ttraining's f1_score: 0.936816\n",
      "[980]\ttraining's multi_logloss: 0.311562\ttraining's f1_score: 0.93694\n",
      "[981]\ttraining's multi_logloss: 0.311347\ttraining's f1_score: 0.937065\n",
      "[982]\ttraining's multi_logloss: 0.311135\ttraining's f1_score: 0.937127\n",
      "[983]\ttraining's multi_logloss: 0.310923\ttraining's f1_score: 0.937211\n",
      "[984]\ttraining's multi_logloss: 0.310707\ttraining's f1_score: 0.937294\n",
      "[985]\ttraining's multi_logloss: 0.310494\ttraining's f1_score: 0.937356\n",
      "[986]\ttraining's multi_logloss: 0.310285\ttraining's f1_score: 0.93746\n",
      "[987]\ttraining's multi_logloss: 0.310067\ttraining's f1_score: 0.937647\n",
      "[988]\ttraining's multi_logloss: 0.309852\ttraining's f1_score: 0.937667\n",
      "[989]\ttraining's multi_logloss: 0.309651\ttraining's f1_score: 0.937771\n",
      "[990]\ttraining's multi_logloss: 0.309437\ttraining's f1_score: 0.937937\n",
      "[991]\ttraining's multi_logloss: 0.309235\ttraining's f1_score: 0.937979\n",
      "[992]\ttraining's multi_logloss: 0.309023\ttraining's f1_score: 0.938104\n",
      "[993]\ttraining's multi_logloss: 0.308818\ttraining's f1_score: 0.938207\n",
      "[994]\ttraining's multi_logloss: 0.308608\ttraining's f1_score: 0.938207\n",
      "[995]\ttraining's multi_logloss: 0.308404\ttraining's f1_score: 0.938478\n",
      "[996]\ttraining's multi_logloss: 0.308185\ttraining's f1_score: 0.938478\n",
      "[997]\ttraining's multi_logloss: 0.307983\ttraining's f1_score: 0.938644\n",
      "[998]\ttraining's multi_logloss: 0.307781\ttraining's f1_score: 0.938664\n",
      "[999]\ttraining's multi_logloss: 0.307573\ttraining's f1_score: 0.938914\n",
      "[1000]\ttraining's multi_logloss: 0.307361\ttraining's f1_score: 0.938893\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's multi_logloss: 0.307361\ttraining's f1_score: 0.938893\n"
     ]
    }
   ],
   "source": [
    "mod = lgb.train(lgb_params, dataset, valid_sets=[dataset], feval=f1_eval,early_stopping_rounds = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = mod.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour_offset</th>\n",
       "      <th>ccs</th>\n",
       "      <th>bcced</th>\n",
       "      <th>total_mails_by_sender</th>\n",
       "      <th>sender_freq_total_period</th>\n",
       "      <th>sender_freq_prev_year</th>\n",
       "      <th>sender_freq_prev_week</th>\n",
       "      <th>sender_freq_prev_month</th>\n",
       "      <th>sender_freq_prev_six_months</th>\n",
       "      <th>...</th>\n",
       "      <th>month_3</th>\n",
       "      <th>month_4</th>\n",
       "      <th>month_5</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.252945</td>\n",
       "      <td>-0.785055</td>\n",
       "      <td>-0.170463</td>\n",
       "      <td>0.114185</td>\n",
       "      <td>0.191052</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.399657</td>\n",
       "      <td>-1.288755</td>\n",
       "      <td>-1.275817</td>\n",
       "      <td>-0.799083</td>\n",
       "      <td>-1.120205</td>\n",
       "      <td>-1.223003</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.926883</td>\n",
       "      <td>0.705070</td>\n",
       "      <td>1.022306</td>\n",
       "      <td>0.870846</td>\n",
       "      <td>0.498429</td>\n",
       "      <td>0.738248</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.094262</td>\n",
       "      <td>-0.350209</td>\n",
       "      <td>-0.056577</td>\n",
       "      <td>0.308666</td>\n",
       "      <td>0.106119</td>\n",
       "      <td>-0.125140</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444660</td>\n",
       "      <td>-1.103774</td>\n",
       "      <td>-0.414117</td>\n",
       "      <td>0.321971</td>\n",
       "      <td>-0.101452</td>\n",
       "      <td>-0.280212</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80162</th>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.029957</td>\n",
       "      <td>-1.042092</td>\n",
       "      <td>-0.927754</td>\n",
       "      <td>-1.335178</td>\n",
       "      <td>-1.549706</td>\n",
       "      <td>-0.853898</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80165</th>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821920</td>\n",
       "      <td>0.238388</td>\n",
       "      <td>0.663768</td>\n",
       "      <td>0.375793</td>\n",
       "      <td>0.421485</td>\n",
       "      <td>0.529557</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80169</th>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.228967</td>\n",
       "      <td>-0.033017</td>\n",
       "      <td>0.357598</td>\n",
       "      <td>0.216904</td>\n",
       "      <td>0.330141</td>\n",
       "      <td>0.366399</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80171</th>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315932</td>\n",
       "      <td>-0.307292</td>\n",
       "      <td>0.188187</td>\n",
       "      <td>-0.037988</td>\n",
       "      <td>0.377650</td>\n",
       "      <td>0.137154</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80173</th>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.836222</td>\n",
       "      <td>0.253742</td>\n",
       "      <td>0.636121</td>\n",
       "      <td>0.530220</td>\n",
       "      <td>0.529764</td>\n",
       "      <td>0.531273</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48145 rows Ã— 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       weekday hour_offset  ccs  bcced  total_mails_by_sender  \\\n",
       "0            0         0.0    0      0              -0.252945   \n",
       "1            2        -1.0    0      0              -1.399657   \n",
       "2            2        -2.0    0      0               0.926883   \n",
       "3            4         0.0    0      0              -0.094262   \n",
       "4            1        -1.0    1      0              -0.444660   \n",
       "...        ...         ...  ...    ...                    ...   \n",
       "80162        0        17.0    0      0              -1.029957   \n",
       "80165        5        15.0    0      0               0.821920   \n",
       "80169        2        -2.0    0      0               0.228967   \n",
       "80171        1        15.0    1      0               0.315932   \n",
       "80173        1        15.0    0      0               0.836222   \n",
       "\n",
       "       sender_freq_total_period  sender_freq_prev_year  sender_freq_prev_week  \\\n",
       "0                     -0.785055              -0.170463               0.114185   \n",
       "1                     -1.288755              -1.275817              -0.799083   \n",
       "2                      0.705070               1.022306               0.870846   \n",
       "3                     -0.350209              -0.056577               0.308666   \n",
       "4                     -1.103774              -0.414117               0.321971   \n",
       "...                         ...                    ...                    ...   \n",
       "80162                 -1.042092              -0.927754              -1.335178   \n",
       "80165                  0.238388               0.663768               0.375793   \n",
       "80169                 -0.033017               0.357598               0.216904   \n",
       "80171                 -0.307292               0.188187              -0.037988   \n",
       "80173                  0.253742               0.636121               0.530220   \n",
       "\n",
       "       sender_freq_prev_month  sender_freq_prev_six_months  ...  month_3  \\\n",
       "0                    0.191052                     0.012660  ...        0   \n",
       "1                   -1.120205                    -1.223003  ...        0   \n",
       "2                    0.498429                     0.738248  ...        0   \n",
       "3                    0.106119                    -0.125140  ...        0   \n",
       "4                   -0.101452                    -0.280212  ...        0   \n",
       "...                       ...                          ...  ...      ...   \n",
       "80162               -1.549706                    -0.853898  ...        0   \n",
       "80165                0.421485                     0.529557  ...        0   \n",
       "80169                0.330141                     0.366399  ...        0   \n",
       "80171                0.377650                     0.137154  ...        0   \n",
       "80173                0.529764                     0.531273  ...        0   \n",
       "\n",
       "       month_4  month_5  month_6  month_7  month_8  month_9  month_10  \\\n",
       "0            0        0        0        0        0        0         0   \n",
       "1            0        0        0        0        0        0         0   \n",
       "2            0        0        0        1        0        0         0   \n",
       "3            0        0        0        0        0        0         1   \n",
       "4            0        0        0        0        0        0         0   \n",
       "...        ...      ...      ...      ...      ...      ...       ...   \n",
       "80162        0        0        0        0        0        0         0   \n",
       "80165        1        0        0        0        0        0         0   \n",
       "80169        0        0        0        1        0        0         0   \n",
       "80171        0        0        1        0        0        0         0   \n",
       "80173        0        1        0        0        0        0         0   \n",
       "\n",
       "       month_11  month_12  \n",
       "0             1         0  \n",
       "1             0         0  \n",
       "2             0         0  \n",
       "3             0         0  \n",
       "4             1         0  \n",
       "...         ...       ...  \n",
       "80162         1         0  \n",
       "80165         0         0  \n",
       "80169         0         0  \n",
       "80171         0         0  \n",
       "80173         0         0  \n",
       "\n",
       "[48145 rows x 324 columns]"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"C://Users//Sauraj (Work mode)//Desktop//OP.csv\")\n",
    "test_data.to_csv(\"C://Users//Sauraj (Work mode)//Desktop//OP_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = train_preds.round().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9326617509606397"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, train_preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mod.predict(test_data)\n",
    "pred_df = pd.DataFrame(preds.round().argmax(axis=1), columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14387\n",
       "3    10087\n",
       "2     5195\n",
       "7     2496\n",
       "1     2156\n",
       "6       42\n",
       "5        1\n",
       "4        1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('C://Users//Sauraj (Work mode)//Desktop//gg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, 7, 1, 5, 6, 4], dtype=int64)"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df['label'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
