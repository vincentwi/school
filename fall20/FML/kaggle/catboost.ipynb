{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/op-dataset/OP_test.csv\n",
      "/kaggle/input/op-dataset/OP.csv\n",
      "/kaggle/input/dsba-fml-foundations-of-machine-learning/test.csv\n",
      "/kaggle/input/dsba-fml-foundations-of-machine-learning/sample_submission.csv\n",
      "/kaggle/input/dsba-fml-foundations-of-machine-learning/train.csv\n",
      "/kaggle/input/dsba-fml-foundations-of-machine-learning/skeleton_code.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import catboost \n",
    "import sklearn as sk \n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import datetime\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "from copy import deepcopy\n",
    "import pprint\n",
    "import shap\n",
    "import os\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Classifiers\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Skopt functions\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.callbacks import DeadlineStopper, VerboseCallback, DeltaXStopper\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from time import time\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../input/op-dataset/OP.csv\")\n",
    "test_data = pd.read_csv(\"../input/op-dataset/OP_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Unnamed: 0 column (This column is always)\n",
    "train_data = train_data.drop('Unnamed: 0', axis=1)\n",
    "test_data = test_data.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "train_data = train_data.fillna(0)\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "train_data = train_data.replace([np.inf, -np.inf], 0)\n",
    "test_data = test_data.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# train_data_copy = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('label', axis=1)\n",
    "y_train = train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train = train_data_copy[~train_data_copy.duplicated(X_train.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = new_train.drop('label', axis=1)\n",
    "# y_train = new_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Mutual Information gain for the columns of the train set\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mif = mutual_info_classif(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the columns where the MF is more than 0.6\n",
    "col_indexes = np.where(mif > 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The columns that will be used after capturing information gain\n",
    "X_train = X_train.to_numpy()[:, col_indexes]\n",
    "test_data = test_data.to_numpy()[:, col_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the data \n",
    "X_train = X_train.reshape(48145, 31)\n",
    "test_data = test_data.reshape(34365,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data scaling \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "mms = StandardScaler()\n",
    "X_train = mms.fit_transform(X_train)\n",
    "test_data = mms.transform(test_data)\n",
    "\n",
    "# pca = PCA(n_components=75)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# test_data = pca.transform(test_data)\n",
    "\n",
    "# lb = LabelBinarizer()\n",
    "# y_train = lb.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGBM model which was used as a baseline against Catboost \n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def f1_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = preds.reshape(len(np.unique(labels)), -1)\n",
    "    preds = preds.T.argmax(axis = 1)\n",
    "    f_score = f1_score(preds, labels, average=\"micro\")\n",
    "    return 'f1_score', f_score, True\n",
    "\n",
    "\n",
    "\n",
    "dataset = lgb.Dataset(X_train, label=y_train)\n",
    "test_dataset = lgb.Dataset(test_data)\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'objective': 'multiclass',\n",
    "          'n_jobs': -1,\n",
    "          'num_leaves': 1000,\n",
    "          'learning_rate': 0.026623466966581126,\n",
    "          'max_depth': 500,\n",
    "          'num_iterations': 1000,\n",
    "          'lambda_l1': 2.959759088169741,\n",
    "#           'lambda_l2': 1.331172832164913,\n",
    "          'bagging_fraction': 0.9655406551472153,\n",
    "          'bagging_freq': 5,\n",
    "          'num_class': 8,\n",
    "          'colsample_bytree': 0.6867118652742716,\n",
    "         'learning_rate': 0.01}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#parameters dict for LightGBM\n",
    "lgb_params =  {\n",
    "    'boosting': 'dart', \n",
    "    'colsample_bytree': 1, \n",
    "    'learning_rate': 0.1, \n",
    "    'max_depth': 20, \n",
    "    'min_child_samples': 20, \n",
    "    'n_estimators': 1000, \n",
    "    'num_leaves': 500,  \n",
    "    'objective': 'multiclass',\n",
    "    'num_class':8,\n",
    "    'reg_alpha': 0.6, \n",
    "    'reg_lambda': 0.3, \n",
    "    'subsample': 0.8,\n",
    "    'verbose':1\n",
    "    }\n",
    "\n",
    "\n",
    "params_2 = {\n",
    "    'application': 'multiclass', # for binary classification\n",
    "    'num_class' : 8, # used for multi-classes\n",
    "    'boosting_type': 'goss', # traditional gradient boosting decision tree\n",
    "    'num_iterations': 600, \n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 300,\n",
    "    'device': 'gpu', # you can use GPU to achieve faster learning\n",
    "    'max_depth': 0, # <0 means no limit\n",
    "    # Small number of bins may reduce training accuracy but can deal with over-fitting\n",
    "    'lambda_l1': 0.5, # L1 regularization\n",
    "    'lambda_l2': 0.3, # L2 regularization\n",
    "    'subsample_for_bin': 300, # number of samples for constructing bins\n",
    "    'subsample': 1, # subsample ratio of the training instance\n",
    "    'colsample_bytree': 0.8, # subsample ratio of columns when constructing the tree\n",
    "    'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n",
    "    'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n",
    "    'min_child_samples': 5# minimum number of data needed in a leaf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization for LGBM and Catboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing Bayesian Search for optimal parameters \n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import * \n",
    "from sklearn.metrics import *\n",
    "\n",
    "#Define the Bayesian parameter search function\n",
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6,n_estimators=10000, output_process=False):\n",
    "    # prepare data for parameter search \n",
    "    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n",
    "    # parameters to be searched for \n",
    "    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, \n",
    "                 min_data_in_leaf,min_sum_hessian_in_leaf,subsample, lambda_l1, lambda_l2,\n",
    "                min_split_gain, force_col_wise, force_row_wise, feature_fraction_bynode):\n",
    "        params = {'application':'multiclass', 'metric':'multi_error','num_classes':8}\n",
    "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['max_bin'] = int(round(max_depth))\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        params['lambda_l1'] = int(lambda_l1)\n",
    "        params['lambda_l2'] = int(lambda_l2)\n",
    "        params['min_split_gain'] = int(min_split_gain)\n",
    "        params['force_col_wise'] = force_col_wise\n",
    "        params['force_row_wise'] = force_row_wise\n",
    "        params['feature_fraction_bynode'] = feature_fraction_bynode\n",
    "        \n",
    "        \n",
    "        #Compute the CV scores and then give the max value for the multi_logloss_mean \n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['multi_logloss'])\n",
    "        return max(cv_result['multi_logloss-mean'])\n",
    "     \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
    "                                            'num_leaves': (100, 500),\n",
    "                                            'feature_fraction': (0.1, 1.0),\n",
    "                                            'bagging_fraction': (0.8, 1),\n",
    "                                            'max_depth': (5, 30),\n",
    "                                            'max_bin':(20,200),\n",
    "                                            'min_data_in_leaf': (20, 100),\n",
    "                                            'min_sum_hessian_in_leaf':(0,100),\n",
    "                                           'subsample': (0.01, 1.0),\n",
    "                                           'lambda_l1': (0.01, 1.0),\n",
    "                                           'lambda_l2': (0.01, 1.0),\n",
    "                                            'min_split_gain':(0.01, 2.0),\n",
    "                                            'force_col_wise':[True, False],\n",
    "                                            'force_row_wise':[True,False],\n",
    "                                            'feature_fraction_bynode':(0.05, 0.9)\n",
    "                                           })\n",
    "\n",
    "    \n",
    "    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n",
    "    \n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    model_score=[]\n",
    "    for model in range(len(lgbBO.res)):\n",
    "        model_score.append(lgbBO.res[model]['target'])\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbBO.res[pd.Series(model_score).idxmax()]['target'],lgbBO.res[pd.Series(model_score).idxmax()]['params']\n",
    "\n",
    "opt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=20, opt_round=20, n_folds=5, random_seed=23,n_estimators=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We find the best paramters \n",
    "bayesian_params = opt_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dictionary of bayesian params \n",
    "bayesian_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The parameters were float, so it needs to be changed to int type \n",
    "bayesian_params['num_leaves'] = int(round(bayesian_params['num_leaves']))\n",
    "bayesian_params['max_bin'] = int(round(bayesian_params['max_bin']))\n",
    "bayesian_params['max_depth'] = int(round(bayesian_params['max_depth']))\n",
    "bayesian_params['min_data_in_leaf'] = int(round(bayesian_params['min_data_in_leaf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The extra parameters \n",
    "bayesian_params['objective'] = \"multiclass\"\n",
    "bayesian_params['metric'] = \"multi_error\"\n",
    "bayesian_params['num_classes'] = 8\n",
    "bayesian_params['num_iterations'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train LGBM on these new parameters\n",
    "dataset = lgb.Dataset(X_train, y_train)\n",
    "mod = lgb.train(params_2, dataset, valid_sets=[dataset], feval=f1_eval,early_stopping_rounds = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = mod.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pred_train.round().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, pred_train, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = mod.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = prediction.round().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_preds).to_csv(\"lgbm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training using SKF approach\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=31416)\n",
    "oof = np.zeros(len(train_data))\n",
    "predictions = np.zeros(len(test_data))\n",
    "# feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(X_train[trn_idx], label=y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train[val_idx], label=y_train[val_idx])\n",
    "\n",
    "    num_round = 15000\n",
    "    clf = lgb.train(bayesian_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 250)\n",
    "    predictions = clf.predict(X_train[val_idx], num_iteration=clf.best_iteration)\n",
    "    oof[val_idx] = predictions.round().argmax(axis=1)\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"Feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "#     predictions += clf.predict(test_data, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(f1_score(y_train, oof, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Optimizer class for optimizing catboost through cross-validation\n",
    "class ModelOptimizer:\n",
    "    best_score = None\n",
    "    opt = None\n",
    "    \n",
    "    def __init__(self, model, Xtrain, ytrain, categorical_columns_indices=None, n_fold=5, seed=1000, early_stopping_rounds=30, is_stratified=True, is_shuffle=True):\n",
    "        self.model = model\n",
    "        self.Xtrain = Xtrain\n",
    "        self.ytrain = ytrain\n",
    "        self.categorical_columns_indices = categorical_columns_indices\n",
    "        self.n_fold = n_fold\n",
    "        self.seed = seed\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.is_stratified = is_stratified\n",
    "        self.is_shuffle = is_shuffle\n",
    "        \n",
    "        \n",
    "    def update_model(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self.model, k, v)\n",
    "            \n",
    "    def evaluate_model(self):\n",
    "        pass\n",
    "    \n",
    "    def optimize(self, param_space, max_evals=10):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "        @use_named_args(param_space)\n",
    "        def _minimize(**params):\n",
    "            self.model.set_params(**params)\n",
    "            return self.evaluate_model()\n",
    "        \n",
    "        opt = gp_minimize(_minimize, param_space, n_calls=max_evals, random_state=2405, n_jobs=-1)\n",
    "        best_values = opt.x\n",
    "        optimal_values = dict(zip([param.name for param in param_space], best_values))\n",
    "        best_score = opt.fun\n",
    "        self.best_score = best_score\n",
    "        self.opt = opt\n",
    "        \n",
    "        print('optimal_parameters: {}\\noptimal score: {}\\noptimization time: {}'.format(optimal_values, best_score))\n",
    "        print('updating model with optimal values')\n",
    "        self.update_model(**optimal_values)\n",
    "        plot_convergence(opt)\n",
    "        return optimal_values\n",
    "    \n",
    "class CatboostOptimizer(ModelOptimizer):\n",
    "    #The catboostoptimizer class will inherit from ModelOptimizer and run 5-fold CV with the best parameters that have bee \n",
    "    #searched, and then return the best scores. \n",
    "    def evaluate_model(self):\n",
    "        validation_scores = catboost.cv(\n",
    "        catboost.Pool(self.Xtrain, \n",
    "                      self.ytrain, \n",
    "                      cat_features=self.categorical_columns_indices),\n",
    "        params=self.model.get_params(), \n",
    "        nfold=self.n_fold,\n",
    "        stratified=self.is_stratified,\n",
    "        seed=self.seed,\n",
    "        early_stopping_rounds=self.early_stopping_rounds,\n",
    "        shuffle=self.is_shuffle,\n",
    "        verbose_eval=False,\n",
    "        plot=False)\n",
    "        self.scores = validation_scores\n",
    "        test_scores = validation_scores.iloc[:, 2]\n",
    "        best_metric = test_scores.max()\n",
    "        return 1 - best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since catboost runs best on categorical columns that are pre-specified, these are the categorical columns\n",
    "cat_cols = [\"month\",\"weekday\",\"designation\",\"No_sender_reported\",\"update_org\",\n",
    "           \"personal_org\",\"promotions_org\",\"forums_org\",\"purchases_org\",\"travel_org\",\"spam_org\",\"social_org\",\"update_tld\",\n",
    "            \"personal_tld\",\"promotions_tld\",\"forums_tld\",\"purchases_tld\",\"spam_tld\",\"social_tld\",\"forum_sender\",\"social_sender\",\n",
    "            \"promo_sender\",\"update_sender\",\"update_sender_only\",\"promo_sender_only\",\"forum_sender_only\",\"social_sender_only\",\n",
    "            \"promo_and_update_sender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Catboost Cross validation run using randomly chosen parameters \n",
    "\n",
    "params_cb = {\n",
    "    \"iterations\": 600,\n",
    "    \"learning_rate\":0.01,\n",
    "    \"depth\":8,\n",
    "    \"l2_leaf_reg\":0.5,\n",
    "    \"loss_function\":\"MultiClass\",\n",
    "    \"custom_metric\":\"F1\",\n",
    "    \"min_child_samples\":100,\n",
    "    \"bootstrap_type\":\"Bayesian\",\n",
    "    \"auto_class_weights\":\"Balanced\",\n",
    "    \"classes_count\":8\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "grid_params = {'min_data_in_leaf': 600,\n",
    "  'depth': 4,\n",
    "#   'bagging_temperature': 42.68421052631579,\n",
    "  'rsm': 0.43157894736842106,\n",
    "#   'random_strength': 58.315789473684205,\n",
    "  'learning_rate': 0.16631578947368422,\n",
    "  'l2_leaf_reg': 0.8,\n",
    "    'bootstrap_type':\"Bayesian\",\n",
    "    'auto_class_weights':\"Balanced\",\n",
    "    'classes_count':8,\n",
    "    'loss_function':\"MultiClass\",\n",
    "    'custom_metric':\"F1\"\n",
    "        }\n",
    "\n",
    "\n",
    "cv_dataset = catboost.Pool(data=X_train, label=y_train)\n",
    "\n",
    "cv = catboost.cv(pool=cv_dataset, params=params_cb, iterations=500, \n",
    "                fold_count=5, nfold=5, shuffle=True, stratified=True,\n",
    "                verbose_eval=True, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance calculated using Shapley Values (Was dropped later due to high computation cost and memory overflow)\n",
    "catboost.get_feature_importance(cv_dataset, type='ShapValues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Hyperparameter Tuning using Bayesian search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, cv, CatBoostClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import * \n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "#Catboost optimization using Bayesian parameter search\n",
    "def CB_opt(n_estimators, depth, learning_rate, max_bin, max_leaves, min_child_samples, l2_leaf_reg, model_size_reg, bagging_temperature,\n",
    "          random_strength): \n",
    "    scores = []\n",
    "    #Run stratifiedKfold to get the train and validation scores \n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1944)\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "\n",
    "        trainx, valx = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        trainy, valy = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "        #Instantiate the class \n",
    "        reg = CatBoostClassifier(verbose = 0,\n",
    "                                n_estimators = int(n_estimators),\n",
    "                                learning_rate = learning_rate,\n",
    "#                                 subsample = subsample, \n",
    "                                l2_leaf_reg = l2_leaf_reg,\n",
    "                                max_depth = int(depth),\n",
    "                                min_child_samples = int(min_child_samples),\n",
    "                                bagging_temperature = bagging_temperature,\n",
    "                                random_strength = random_strength,\n",
    "                                max_leaves = int(max_leaves),\n",
    "                                random_state = 100,\n",
    "                                grow_policy = \"Lossguide\",\n",
    "                                max_bin = int(max_bin),  \n",
    "                                use_best_model = True, \n",
    "                                model_size_reg = model_size_reg,\n",
    "                                auto_class_weights = \"Balanced\",\n",
    "                                classes_count = 8\n",
    "\n",
    "                                )\n",
    "        #Fit the model and let the training begin, evaluate the prediction using F1-micro score. \n",
    "        reg.fit(trainx, trainy, eval_set = (valx, valy))\n",
    "        scores.append(f1_score(valy, reg.predict(valx), average=\"micro\"))\n",
    "    return np.mean(scores)\n",
    "\n",
    "# pbounds = {\"n_estimators\": (150,1000),\n",
    "#            \"depth\": (2,10),\n",
    "#            \"learning_rate\": (.01, 0.2),\n",
    "#            \"subsample\":(0.4, 1.),\n",
    "#            \"num_leaves\": (16,40),\n",
    "#            \"max_bin\":(150,300),\n",
    "#            \"l2_leaf_reg\":(0,10),\n",
    "#            \"model_size_reg\": (0,10)\n",
    "# }\n",
    "#The search space defined for the model.\n",
    "search_spaces = {'n_estimators': (200, 800),\n",
    "                 'depth': (6, 8),\n",
    "#                  'subsample':(0.1, 1.0),\n",
    "                 'learning_rate': (0.01, 1.0),\n",
    "                 'random_strength': (1e-9, 10),\n",
    "                 'bagging_temperature': (0.0, 1.0),\n",
    "                 'l2_leaf_reg': (2, 30),\n",
    "                 'min_child_samples': (100, 400),\n",
    "                 'max_leaves':(10,40),\n",
    "                 'max_bin':(50, 200),\n",
    "                 'model_size_reg':(0, 10)\n",
    "#                  'classes_count':8\n",
    "                }\n",
    "optimizer = BayesianOptimization(f = CB_opt, pbounds = search_spaces, verbose = 2,random_state = 888)\n",
    "\n",
    "print(optimizer.maximize(init_points = 5, n_iter = 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal paramters\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#Calculate the class weights \n",
    "uniques = np.unique(y_train)\n",
    "weights = compute_class_weight('balanced',uniques, y_train)\n",
    "\n",
    "\n",
    "#Train the model \n",
    "model = catboost.CatBoostClassifier(iterations=1000,class_weights=weights,use_best_model=True\n",
    "                                    , custom_metric=\"TotalF1\", loss_function=\"MultiClass\",\n",
    "                                   od_type=\"Iter\", grow_policy=\"Lossguide\")\n",
    "\n",
    "default_cb_optimizer = CatboostOptimizer(model, X_train, y_train)\n",
    "\n",
    "\n",
    "params_space = [Real(0.01, 1.0, name='learning_rate'), \n",
    "                Integer(2, 12, name='max_depth'), \n",
    "                Real(0.4, 1.0, name='colsample_bylevel'), \n",
    "                Real(0.0, 100, name='bagging_temperature'), \n",
    "                Real(0.0, 100, name='random_strength'), \n",
    "                Real(1.0, 100, name='l2_leaf_reg'),\n",
    "                Integer(100, 500, name='min_child_samples'),\n",
    "                Integer(20, 50, name='max_leaves')]\n",
    "# optimal_values = default_cb_optimizer.optimize(params_space, max_evals=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize the model (Took too much time, so skipped it.)\n",
    "optimal_values = default_cb_optimizer.optimize(params_space, max_evals=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = catboost.CatBoost()\n",
    "\n",
    "\n",
    "#Ran basic randomized search for Catboost to get the parameters \n",
    "params_space = {\n",
    "    \"learning_rate\": np.linspace(0.01, 1.0, 20),\n",
    "    \"max_depth\":[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    \"colsample_bylevel\": np.linspace(0.4, 1.0, 20),\n",
    "    \"random_strength\":np.linspace(1.0, 100, 20),\n",
    "    \"l2_leaf_reg\":np.linspace(0.1, 1.0, 10),\n",
    "    \"min_child_samples\":[100, 200, 300, 400, 500, 600]\n",
    "#     \"max_leaves\":[20, 25, 30, 35, 40, 45, 50]\n",
    "}\n",
    "\n",
    "#Basic randomized search \n",
    "model.randomized_search(params_space,\n",
    "                  X_train,\n",
    "                  y_train,\n",
    "                  cv=5,\n",
    "                  n_iter=10,\n",
    "                  partition_random_seed=0,\n",
    "                  calc_cv_statistics=True, \n",
    "                  search_by_train_test_split=True,\n",
    "                  refit=True, \n",
    "                  shuffle=True, \n",
    "                  stratified=True, \n",
    "                  train_size=0.8,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the class weights for the training\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "uniques = np.unique(y_train.to_numpy())\n",
    "weights = compute_class_weight(None,uniques, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "\n",
    "params_cb = {\n",
    "    \"iterations\": 600,\n",
    "    \"learning_rate\":0.01,\n",
    "    \"depth\":8,\n",
    "    \"l2_leaf_reg\":0.5,\n",
    "    \"loss_function\":\"MultiClass\",\n",
    "    \"eval_metric\":\"TotalF1\",\n",
    "    \"min_child_samples\":500,\n",
    "    \"bootstrap_type\":\"Bayesian\",\n",
    "    \"auto_class_weights\":\"Balanced\",\n",
    "    \"classes_count\":8\n",
    "}\n",
    "\n",
    "\n",
    "# 'params': {'min_data_in_leaf': 600,\n",
    "#   'depth': 4,\n",
    "#   'rsm': 0.6526315789473685,\n",
    "#   'random_strength': 58.315789473684205,\n",
    "#   'learning_rate': 0.16631578947368422,\n",
    "#   'l2_leaf_reg': 0.8},\n",
    "\n",
    "#The final catboost model.\n",
    "model = catboost.CatBoostClassifier(iterations=5000,custom_metric=\"F1\", class_weights=weights, \n",
    "                                    loss_function=\"MultiClass\"\n",
    "                                    , learning_rate=0.01, depth=9, task_type='GPU', l2_leaf_reg=0.8,\n",
    "                                   classes_count=8, min_child_samples=600, bootstrap_type=\"Bayesian\", od_type=\"Iter\")\n",
    "\n",
    "model.fit(X_train, y_train, eval_set=(X_train, y_train), plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The final predictions after Catboost was done training \n",
    "preds_2 = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(preds_2.reshape(-1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CatBoostClassifier(iterations=600, learning_rate=0.01, depth=8, \n",
    "#                           l2_leaf_reg=0.5,loss_function=\"MultiClass\",\n",
    "#                            bagging_temperature=0.6,\n",
    "#                           eval_metric=\"TotalF1\",classes_count=8, verbose=2,\n",
    "#                             class_weights=weights)\n",
    "\n",
    "# default_cb_optimizer = CatboostOptimizer(model, X_train, y_train)\n",
    "# default_cb_optimizer.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def report_perf(optimizer, X, y, title, callbacks=None):\n",
    "#     \"\"\"\n",
    "#     A wrapper for measuring time and performances of different optmizers\n",
    "    \n",
    "#     optimizer = a sklearn or a skopt optimizer\n",
    "#     X = the training set \n",
    "#     y = our target\n",
    "#     title = a string label for the experiment\n",
    "#     \"\"\"\n",
    "#     start = time()\n",
    "#     if callbacks:\n",
    "#         optimizer.fit(X, y, callback=callbacks)\n",
    "#     else:\n",
    "#         optimizer.fit(X, y)\n",
    "#     d=pd.DataFrame(optimizer.cv_results_)\n",
    "#     best_score = optimizer.best_score_\n",
    "#     best_score_std = d.iloc[optimizer.best_index_].std_test_score\n",
    "#     best_params = optimizer.best_params_\n",
    "#     print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n",
    "#            +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n",
    "#                                   len(optimizer.cv_results_['params']),\n",
    "#                                   best_score,\n",
    "#                                   best_score_std))    \n",
    "#     print('Best parameters:')\n",
    "#     pprint.pprint(best_params)\n",
    "#     print()\n",
    "#     return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# clf = CatBoostClassifier(thread_count=2,\n",
    "#                          loss_function='MultiClass',\n",
    "#                          od_type = 'Iter',\n",
    "#                          eval_metric = \"TotalF1\",\n",
    "#                          verbose= True)\n",
    "\n",
    "# # Defining your search space\n",
    "# search_spaces = {'iterations': Integer(10, 1000),\n",
    "#                  'depth': Integer(1, 8),\n",
    "#                  'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "#                  'random_strength': Real(1e-9, 10, 'log-uniform'),\n",
    "#                  'bagging_temperature': Real(0.0, 1.0),\n",
    "#                  'border_count': Integer(1, 255),\n",
    "#                  'l2_leaf_reg': Integer(2, 30),\n",
    "#                  'scale_pos_weight':Real(0.01, 1.0, 'uniform')}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f1 = make_scorer(f1_score, average=\"micro\")\n",
    "\n",
    "# opt = BayesSearchCV(clf,\n",
    "#                     search_spaces,\n",
    "#                     scoring=f1,\n",
    "#                     cv=skf,\n",
    "#                     n_iter=200,\n",
    "#                     n_jobs=-1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n",
    "#                     return_train_score=True,\n",
    "#                     refit=True,\n",
    "#                     optimizer_kwargs={'base_estimator': 'GP'},\n",
    "#                     random_state=42)\n",
    "\n",
    "# best_params = report_perf(opt, X_train, y_train,'CatBoost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
