{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Load Libraries","metadata":{}},{"cell_type":"code","source":"##########################Load Libraries  ####################################\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import preprocessing, metrics\nfrom ipywidgets import widgets, interactive\nimport gc\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime, timedelta \nfrom typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom itertools import cycle\nimport datetime as dt\nfrom torch.autograd import Variable\nimport random \nimport os\nfrom matplotlib.pyplot import figure\nfrom fastprogress import master_bar, progress_bar\nimport torch\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nimport time \nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import mean_squared_error\nimport torch \n\n%matplotlib inline\n\n#from gensim.models import Word2Vec\n#import gensim.downloader as api\n\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\n \n\n ","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:02.857924Z","iopub.execute_input":"2022-01-15T20:47:02.858393Z","iopub.status.idle":"2022-01-15T20:47:02.890146Z","shell.execute_reply.started":"2022-01-15T20:47:02.858346Z","shell.execute_reply":"2022-01-15T20:47:02.888993Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## GPU use \nSince this is a deep learning model, The use of GPU will accelerate the training. \nThe first models are not so demanding so you can still use CPU training (but it will be slower).","metadata":{}},{"cell_type":"code","source":"device = 'cuda:0'\n#device = 'cpu'","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:02.894818Z","iopub.execute_input":"2022-01-15T20:47:02.895119Z","iopub.status.idle":"2022-01-15T20:47:02.908763Z","shell.execute_reply.started":"2022-01-15T20:47:02.895071Z","shell.execute_reply":"2022-01-15T20:47:02.907765Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR_PATH = '../input/m5-forecasting-accuracy/'","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:02.910885Z","iopub.execute_input":"2022-01-15T20:47:02.911344Z","iopub.status.idle":"2022-01-15T20:47:02.921124Z","shell.execute_reply.started":"2022-01-15T20:47:02.911299Z","shell.execute_reply":"2022-01-15T20:47:02.920095Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics: \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n\ndef read_data():\n    sell_prices_df = pd.read_csv(INPUT_DIR_PATH + 'sell_prices.csv')\n    sell_prices_df = reduce_mem_usage(sell_prices_df)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices_df.shape[0], sell_prices_df.shape[1]))\n\n    calendar_df = pd.read_csv(INPUT_DIR_PATH + 'calendar.csv')\n    calendar_df = reduce_mem_usage(calendar_df)\n    print('Calendar has {} rows and {} columns'.format(calendar_df.shape[0], calendar_df.shape[1]))\n\n    sales_train_validation_df = pd.read_csv(INPUT_DIR_PATH + 'sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation_df.shape[0], sales_train_validation_df.shape[1]))\n\n    submission_df = pd.read_csv(INPUT_DIR_PATH + 'sample_submission.csv')\n    return sell_prices_df, calendar_df, sales_train_validation_df, submission_df\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:02.923384Z","iopub.execute_input":"2022-01-15T20:47:02.924073Z","iopub.status.idle":"2022-01-15T20:47:02.949471Z","shell.execute_reply.started":"2022-01-15T20:47:02.924002Z","shell.execute_reply":"2022-01-15T20:47:02.948127Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Load Data <a id=\"2\"></a>","metadata":{}},{"cell_type":"code","source":"_,  calendar_df, sales_train_validation_df, _ = read_data()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:02.954580Z","iopub.execute_input":"2022-01-15T20:47:02.954854Z","iopub.status.idle":"2022-01-15T20:47:13.890546Z","shell.execute_reply.started":"2022-01-15T20:47:02.954824Z","shell.execute_reply":"2022-01-15T20:47:13.889497Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Date List\nHere we create dates list, that will help later on to display the Time Series, with the right dates ","metadata":{}},{"cell_type":"code","source":"#Create date index\ndate_index = calendar_df['date']\ndates = date_index[0:1913]\ndates_list = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in dates]","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:13.894208Z","iopub.execute_input":"2022-01-15T20:47:13.894868Z","iopub.status.idle":"2022-01-15T20:47:13.930134Z","shell.execute_reply.started":"2022-01-15T20:47:13.894824Z","shell.execute_reply":"2022-01-15T20:47:13.929137Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Select One Time Series as an Example <a id=\"3\"></a>\nSelecting one arbitrary Time Series ","metadata":{}},{"cell_type":"code","source":"# Create a data frame for items sales per day with item ids (with Store Id) as columns names  and dates as the index \nsales_train_validation_df['item_store_id'] = sales_train_validation_df.apply(lambda x: x['item_id']+'_'+x['store_id'],axis=1)\nDF_Sales = sales_train_validation_df.loc[:,'d_1':'d_1913'].T\nDF_Sales.columns = sales_train_validation_df['item_store_id'].values\n\n#Set Dates as index \nDF_Sales = pd.DataFrame(DF_Sales).set_index([dates_list])\nDF_Sales.index = pd.to_datetime(DF_Sales.index)\nDF_Sales.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:13.932143Z","iopub.execute_input":"2022-01-15T20:47:13.932695Z","iopub.status.idle":"2022-01-15T20:47:22.343124Z","shell.execute_reply.started":"2022-01-15T20:47:13.932650Z","shell.execute_reply":"2022-01-15T20:47:22.342275Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Plot The TS","metadata":{}},{"cell_type":"code","source":"#Select arbitrary index and plot the time series\nindex = 6780\ny = pd.DataFrame(DF_Sales.iloc[:,index])\ny = pd.DataFrame(y).set_index([dates_list])\nTS_selected = y \ny.index = pd.to_datetime(y.index)\nax = y.plot(figsize=(30, 9),color='red')\nax.set_facecolor('lightgrey')\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.legend(fontsize=20)\nplt.title(label = 'Sales Demand Selected Time Series Over Time',fontsize = 23)\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.345157Z","iopub.execute_input":"2022-01-15T20:47:22.345586Z","iopub.status.idle":"2022-01-15T20:47:22.768310Z","shell.execute_reply.started":"2022-01-15T20:47:22.345508Z","shell.execute_reply":"2022-01-15T20:47:22.767279Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#del calendar_df, sales_train_validation_df,DF_Sales\n#gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.770470Z","iopub.execute_input":"2022-01-15T20:47:22.770848Z","iopub.status.idle":"2022-01-15T20:47:22.776088Z","shell.execute_reply.started":"2022-01-15T20:47:22.770809Z","shell.execute_reply":"2022-01-15T20:47:22.775098Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## SEED all ","metadata":{}},{"cell_type":"code","source":"SEED = 1345\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.777683Z","iopub.execute_input":"2022-01-15T20:47:22.778374Z","iopub.status.idle":"2022-01-15T20:47:22.794298Z","shell.execute_reply.started":"2022-01-15T20:47:22.778321Z","shell.execute_reply":"2022-01-15T20:47:22.792981Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Normlize Data <a id=\"4\"></a>\nNormalization is a technique often applied as part of data preparation for machine learning.\nThe goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values.\nFor machine learning, every dataset does not require normalization.\nIt is required only when features have different rangesor scales.\n\n\n\nNormalizes our data using the min/max scaler with minimum and maximum values of -1 and 1, respectively","metadata":{}},{"cell_type":"code","source":"data = np.array(y)\nscaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_data_normalized = scaler.fit_transform(data.reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.796338Z","iopub.execute_input":"2022-01-15T20:47:22.797151Z","iopub.status.idle":"2022-01-15T20:47:22.805191Z","shell.execute_reply.started":"2022-01-15T20:47:22.797100Z","shell.execute_reply":"2022-01-15T20:47:22.804078Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"If we print some of the examples, we can see that the values are now between -1 and 1 ","metadata":{}},{"cell_type":"code","source":"print(train_data_normalized[:5])\nprint(train_data_normalized[-5:])","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.807140Z","iopub.execute_input":"2022-01-15T20:47:22.807789Z","iopub.status.idle":"2022-01-15T20:47:22.819506Z","shell.execute_reply.started":"2022-01-15T20:47:22.807644Z","shell.execute_reply":"2022-01-15T20:47:22.818659Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"\nWe create a sliding window which builds sequences and labels.\nIn our case, we have sequences of 28 days, that will use to predict the next day.","metadata":{}},{"cell_type":"code","source":"###  This function creates a sliding window or sequences of 28 days and one day label ####\ndef sliding_windows(data, seq_length):\n    x = y = []\n\n    for i in range(len(data)-seq_length-1):\n        _x = data[i:(i+seq_length)]\n        _y = data[i+seq_length]\n        x.append(_x)\n        y.append(_y)\n\n    return np.array(x),np.array(y)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.821907Z","iopub.execute_input":"2022-01-15T20:47:22.822390Z","iopub.status.idle":"2022-01-15T20:47:22.833193Z","shell.execute_reply.started":"2022-01-15T20:47:22.822346Z","shell.execute_reply":"2022-01-15T20:47:22.832153Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Lets see the shape of our data","metadata":{}},{"cell_type":"code","source":"#train_inout_seq = create_inout_sequences(train_data_normalized, train_window)\nseq_length = 28\nx, y = sliding_windows(train_data_normalized, seq_length)\nprint(x.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.835634Z","iopub.execute_input":"2022-01-15T20:47:22.836012Z","iopub.status.idle":"2022-01-15T20:47:22.850596Z","shell.execute_reply.started":"2022-01-15T20:47:22.835965Z","shell.execute_reply":"2022-01-15T20:47:22.848342Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train_size = int(len(y) * 0.67)\ntest_size = len(y) - train_size\n\ndataX = Variable(torch.Tensor(np.array(x)))\ndataY = Variable(torch.Tensor(np.array(y)))\n\ntrainX = Variable(torch.Tensor(np.array(x[0:train_size])))\ntrainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n\ntestX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\ntestY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.852823Z","iopub.execute_input":"2022-01-15T20:47:22.853255Z","iopub.status.idle":"2022-01-15T20:47:22.880960Z","shell.execute_reply.started":"2022-01-15T20:47:22.853210Z","shell.execute_reply":"2022-01-15T20:47:22.879431Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(\"train shape is:\",trainX.size())\nprint(\"train label shape is:\",trainY.size())\nprint(\"test shape is:\",testX.size())\nprint(\"test label shape is:\",testY.size())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.882887Z","iopub.status.idle":"2022-01-15T20:47:22.883652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we got :\n* 1262 sets of 28 samples each as the features (X) and 1262 labels as our target(y) in the training set \n* 622 sets of 28 samples with 622 labels in our tests set \n\nYou can see that in Pytorch the tensor dimensions are opposite to the NumPy dimensions ","metadata":{}},{"cell_type":"markdown","source":"# LSTM model ","metadata":{}},{"cell_type":"markdown","source":"The following parameters are provided to the net\n* Num-classes - is the number of output in this case 1\n* Input size - we don't use batch, so we have one input (of 28 samples)\n* Hidden layers, number of hidden layer in each cell, the more is better, but also will slow down the training\n* Num layers - we have one layer of LSTM (layer we will increase it)","metadata":{}},{"cell_type":"code","source":"class LSTM2(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM2, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        self.batch_size = 1\n        #self.seq_length = seq_length\n        \n        self.LSTM2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,batch_first=True,dropout = 0.25)\n       \n        \n        \n        self.fc = nn.Linear(hidden_size, num_classes)\n        self.dropout = nn.Dropout(p=0.2)\n    def forward(self, x):\n        h_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n         \n        \n        c_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size).to(device))\n        \n       \n        _, (hn, cn) = self.LSTM2(x, (h_1, c_1))\n     \n        #print(\"hidden state shpe is:\",hn.size())\n        y = hn.view(-1, self.hidden_size)\n        \n        final_state = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1]\n        #print(\"final state shape is:\",final_state.shape)\n        out = self.fc(final_state)\n        #out = self.dropout(out)\n        #print(out.size())\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.885564Z","iopub.status.idle":"2022-01-15T20:47:22.886332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using RMSE loss as rec by guillaume\nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self,yhat,y):\n        return torch.sqrt(self.mse(yhat,y))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.888841Z","iopub.status.idle":"2022-01-15T20:47:22.889807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.891512Z","iopub.status.idle":"2022-01-15T20:47:22.892443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 700\nlearning_rate = 1e-3\ninput_size = 1\nhidden_size = 512\nnum_layers = 2\n\nnum_classes = 1\n\nlstm = LSTM2(num_classes, input_size, hidden_size, num_layers)\nlstm.to(device)\n\n\nlstm.apply(init_weights)\n\ncriterion = torch.nn.MSELoss().to(device)    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate,weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=100, factor =0.5 ,min_lr=1e-7, eps=1e-08)\n#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n\n# Train the model\n\nfor epoch in progress_bar(range(num_epochs)): \n    lstm.train()\n    outputs = lstm(trainX.to(device))\n    optimizer.zero_grad()\n    torch.nn.utils.clip_grad_norm_(lstm.parameters(), 1)\n    # obtain the loss function\n    loss = criterion(outputs, trainY.to(device))\n    \n    loss.backward()\n    \n    scheduler.step(loss)\n    optimizer.step()\n    lstm.eval()\n    valid = lstm(testX.to(device))\n    vall_loss = criterion(valid, testY.to(device))\n    scheduler.step(vall_loss)\n    \n    if epoch % 50 == 0:\n      print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f \" %(epoch, loss.cpu().item(),vall_loss.cpu().item()))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.894024Z","iopub.status.idle":"2022-01-15T20:47:22.894989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######Prediction###############\n\n\nlstm.eval()\ntrain_predict = lstm(dataX.to(device))\ndata_predict = train_predict.cpu().data.numpy()\ndataY_plot = dataY.data.numpy()\n\n## Inverse Normalize \ndata_predict = scaler.inverse_transform(data_predict)\ndataY_plot = scaler.inverse_transform(dataY_plot)\n\n## Add dates\ndf_predict = pd.DataFrame(data_predict)\ndf_predict = df_predict.set_index([dates_list[:-29]])\ndf_labels = pd.DataFrame(dataY_plot)\ndf_labels = df_labels.set_index([dates_list[:-29]])\n\n# Plot \nfigure(num=None, figsize=(19, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.axvline(x=dates_list[train_size], c='r')\nplt.plot( df_labels[0])\nplt.plot(df_predict[0])\nplt.legend(['Prediction','Time Series'],fontsize = 21)\nplt.suptitle('Time-Series Prediction Entire Set',fontsize = 23)\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.896676Z","iopub.status.idle":"2022-01-15T20:47:22.897653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#######Plot the test set ##########################\nfigure(num=None, figsize=(23, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(df_labels.iloc[-testX.size()[0]:][0])\nplt.plot(df_predict.iloc[-testX.size()[0]:][0])\nplt.legend(['Prediction','Time Series'],fontsize = 21)\nplt.suptitle('Time-Series Prediction Test',fontsize = 23)\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.899340Z","iopub.status.idle":"2022-01-15T20:47:22.900276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test RMSE","metadata":{}},{"cell_type":"code","source":"np.sqrt(((dataY_plot[-testX.size()[0]:] - data_predict[-testX.size()[0]:] ) ** 2).mean())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T20:47:22.901879Z","iopub.status.idle":"2022-01-15T20:47:22.902777Z"},"trusted":true},"execution_count":null,"outputs":[]}]}