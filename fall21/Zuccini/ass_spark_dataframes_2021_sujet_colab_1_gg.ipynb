{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbuZfFWZVXpn"
   },
   "source": [
    "<h1><center>Big Data Algorithms Techniques & Platforms</center></h1>\n",
    "<h2>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Spark and DataFrames</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBLI_qUSVXpq"
   },
   "source": [
    "## Objectives \n",
    "\n",
    "<strong> Dataframes: </strong>\n",
    "<ul>\n",
    "    <li>  Pyspark </li> \n",
    "    <li>  Pandas library on Spark</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42q0jiKKVXpr"
   },
   "source": [
    "# A. Context\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "For running this serie of exercises we are going to use a quite big dataset containing data on Bitcoin made available from <a href=\"https://www.kaggle.com/mczielinski/bitcoin-historical-data\">Kaggle</a>.\n",
    "\n",
    "As stated in the description of the dataset:\n",
    "\"Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary.\" \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "### The dataset\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The dataset is in a .csv file:\n",
    "\n",
    "$bitstampUSD\\_1-min\\_data\\_2012-01-01\\_to\\_2021-03-31.csv$\n",
    "\n",
    "CSV files for select bitcoin exchanges for the time period of Jan 2012 to December March 2021, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. \n",
    "\n",
    "Notice that:\n",
    "<ul>\n",
    "    <li> Timestamps are in Unix time.</li>\n",
    "<li> Timestamps without any trades or activity have their data fields filled with NaNs. </li>\n",
    "<li>  If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforeseen technical error in data reporting or gathering. </li>\n",
    "</ul>\n",
    "As stated by the authors \"all effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk\".\n",
    "</p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd5y1A2BVXpr"
   },
   "source": [
    "# B. Environment set-up\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "As first step you must include your dataset in your environment.\n",
    "\n",
    "You can folllow the procedure that includes Kaggle data into colab working folders or simply download and re-upload the file on your Colab space.\n",
    "\n",
    "\n",
    "$bitstampUSD\\_1-min\\_data\\_2012-01-01\\_to\\_2021-03-31.csv$\n",
    "    \n",
    "and upload it in the folder where your notebook is supposed to read the input.\n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "As second step you must prepare your environment running the following two cells that:\n",
    "<ul>\n",
    "    <li> Import the Pandas library.</li>\n",
    "<li> Set the Spark environment and return a SparkSession (acting as was acting the SparkContext in the previous exercises). </li>\n",
    "</ul>    \n",
    "    \n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wVZaAkIgVXps"
   },
   "outputs": [],
   "source": [
    "# import of Pandas library\n",
    "import pandas as pd\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: apt-get\n",
      "x spark-3.0.3-bin-hadoop2.7/\n",
      "x spark-3.0.3-bin-hadoop2.7/NOTICE\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/tests/\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/tests/worker_memory_check.py\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/tests/py_container_checks.py\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/tests/pyfiles.py\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
      "x spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-vector-code-gen-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-beanutils-1.9.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/httpcore-4.4.12.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/scala-library-2.12.10.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/parquet-format-2.4.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/xercesImpl-2.12.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/okio-1.15.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-compiler-3.0.16.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spire-macros_2.12-0.17.0-M1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/arrow-memory-0.15.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/JLargeArrays-1.5.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/logging-interceptor-3.12.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-cli-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/parquet-common-1.10.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.10.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/algebra_2.12-2.0.0-M2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-graphx_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-annotations-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/kubernetes-client-4.9.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spire-util_2.12-0.17.0-M1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jakarta.activation-api-1.2.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-network-shuffle_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jline-2.14.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/breeze_2.12-1.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/metrics-jvm-4.1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-launcher_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/machinist_2.12-0.6.8.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/scala-compiler-2.12.10.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/arrow-format-0.15.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-mllib_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jta-1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-yarn_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/istack-commons-runtime-3.0.8.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/metrics-graphite-4.1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-hdfs-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/audience-annotations-0.5.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jpam-1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jakarta.annotation-api-1.3.5.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/py4j-0.10.9.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spire-platform_2.12-0.17.0-M1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/avro-1.8.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-core_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/curator-client-2.7.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/antlr4-runtime-4.7.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/javax.inject-1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-kubernetes_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jersey-media-jaxb-2.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-lang3-3.9.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/javax.jdo-3.2.0-m3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-auth-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/objenesis-2.5.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/paranamer-2.8.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/cats-kernel_2.12-2.0.0-M4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/flatbuffers-java-1.9.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jersey-server-2.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/stream-2.9.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/datanucleus-api-jdo-4.2.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/xml-apis-1.4.01.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-hive_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-jdbc-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-exec-2.3.7-core.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-sketch_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jsr305-3.0.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/univocity-parsers-2.9.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/macro-compat_2.12-1.1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jersey-container-servlet-core-2.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/orc-core-1.5.10.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hk2-api-2.6.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/chill-java-0.9.5.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-tags_2.12-3.0.3-tests.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/automaton-1.11-8.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-annotations-2.10.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hk2-utils-2.6.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-network-common_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-repl_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/velocity-1.5.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jul-to-slf4j-1.7.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/JTransforms-3.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/json4s-ast_2.12-3.6.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jersey-client-2.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/httpclient-4.5.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jetty-sslengine-6.1.26.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-kvstore_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spire_2.12-0.17.0-M1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jersey-hk2-2.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/xbean-asm7-shaded-4.15.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jakarta.validation-api-2.0.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/snappy-java-1.1.8.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/slf4j-log4j12-1.7.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/zstd-jni-1.4.4-3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/slf4j-api-1.7.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-shims-0.23-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-storage-api-2.7.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/shapeless_2.12-2.3.3.jar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x spark-3.0.3-bin-hadoop2.7/jars/kubernetes-model-common-4.9.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/core-1.1.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jersey-container-servlet-2.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/datanucleus-rdbms-4.1.19.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-module-paranamer-2.10.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/osgi-resource-locator-1.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-module-scala_2.12-2.10.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-shims-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/json-1.8.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/antlr-runtime-3.5.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/threeten-extra-1.5.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-tags_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.10.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jersey-common-2.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/aircompressor-0.10.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/lz4-java-1.7.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-client-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/shims-0.7.45.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-crypto-1.1.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-mesos_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/libthrift-0.12.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/orc-mapreduce-1.5.10.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/HikariCP-2.5.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/generex-1.0.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/okhttp-3.12.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/breeze-macros_2.12-1.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jaxb-runtime-2.3.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/scala-xml_2.12-1.2.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hk2-locator-2.6.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-common-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-common-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/xz-1.5.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-shims-scheduler-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/chill_2.12-0.9.5.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-databind-2.10.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-hive-thriftserver_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/scala-reflect-2.12.10.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/joda-time-2.10.5.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jakarta.ws.rs-api-2.1.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/orc-shims-1.5.10.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/aopalliance-repackaged-2.6.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-compress-1.20.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-core-2.10.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-net-3.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/datanucleus-core-4.1.17.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/janino-3.0.16.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-shims-common-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/transaction-api-1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-streaming_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jackson-datatype-jsr310-2.10.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-mllib-local_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jakarta.inject-2.6.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/metrics-jmx-4.1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/arrow-vector-0.15.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jakarta.xml.bind-api-2.3.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-catalyst_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/curator-framework-2.7.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-metastore-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-beeline-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/metrics-core-4.1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-llap-common-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/netty-all-4.1.47.Final.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/javassist-3.25.0-GA.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/scala-collection-compat_2.12-2.1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/spark-sql_2.12-3.0.3.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/parquet-column-1.10.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/kubernetes-model-4.9.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/guice-3.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/commons-text-1.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/json4s-scalap_2.12-3.6.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/metrics-json-4.1.1.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/snakeyaml-1.24.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/json4s-jackson_2.12-3.6.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.4.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/json4s-core_2.12-3.6.6.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/pyrolite-4.30.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/hive-serde-2.3.7.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/RoaringBitmap-0.7.45.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/jars/zookeeper-3.4.14.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/data/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/iris_libsvm.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/als/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/als/test.data\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/pic_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/54893.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/not-image.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/DP802813.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/DP153539.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/license.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/multi-channel/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA.png\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/images/license.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/ridge-data/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/graphx/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/graphx/users.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/graphx/followers.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/data/streaming/\n",
      "x spark-3.0.3-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/R/\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/sparkr.zip\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/tests/\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/profile/\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/R/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/html/\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/worker/\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
      "x spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
      "x spark-3.0.3-bin-hadoop2.7/README.md\n",
      "x spark-3.0.3-bin-hadoop2.7/RELEASE\n",
      "x spark-3.0.3-bin-hadoop2.7/yarn/\n",
      "x spark-3.0.3-bin-hadoop2.7/yarn/spark-3.0.3-yarn-shuffle.jar\n",
      "x spark-3.0.3-bin-hadoop2.7/LICENSE\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/start-master.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/spark-config.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/start-history-server.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/start-slaves.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/spark-daemon.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/slaves.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/stop-history-server.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/start-slave.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/start-all.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/stop-slave.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/spark-daemons.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/stop-slaves.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/stop-all.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/sbin/stop-master.sh\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/prefixSpan.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/powerIterationClustering.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/streaming/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/people.json\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/people.csv\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/users.orc\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/dir1/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/dir1/file1.parquet\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/dir1/dir2/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/dir1/dir2/file2.parquet\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/dir1/file3.json\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scripts/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/scripts/getGpusResources.sh\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/kmeans.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/fm_regressor_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/robust_scaler_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/power_iteration_clustering_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/summarizer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/fm_classifier_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/prefixspan_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/interaction_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/als.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/wordcount.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/pagerank.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sort.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/pi.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/hive.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/arrow.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/streaming/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/basic.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/jars/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/jars/spark-examples_2.12-3.0.3.jar\r\n",
      "x spark-3.0.3-bin-hadoop2.7/examples/jars/scopt_2.12-3.7.1.jar\r\n",
      "x spark-3.0.3-bin-hadoop2.7/conf/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/conf/slaves.template\r\n",
      "x spark-3.0.3-bin-hadoop2.7/conf/metrics.properties.template\r\n",
      "x spark-3.0.3-bin-hadoop2.7/conf/fairscheduler.xml.template\r\n",
      "x spark-3.0.3-bin-hadoop2.7/conf/log4j.properties.template\r\n",
      "x spark-3.0.3-bin-hadoop2.7/conf/spark-defaults.conf.template\r\n",
      "x spark-3.0.3-bin-hadoop2.7/conf/spark-env.sh.template\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x spark-3.0.3-bin-hadoop2.7/bin/sparkR.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/sparkR\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-submit\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/pyspark2.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-class\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/pyspark.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-submit2.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/load-spark-env.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-sql\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/docker-image-tool.sh\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/find-spark-home.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/load-spark-env.sh\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/pyspark\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-shell.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-shell2.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-submit.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/beeline.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/find-spark-home\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-class.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/sparkR2.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/beeline\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-class2.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-sql.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/run-example\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-shell\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/run-example.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/bin/spark-sql2.cmd\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/.gitignore\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/run-tests-with-coverage\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pylintrc\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/MANIFEST.in\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/README.md\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_coverage/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_coverage/coverage_daemon.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_coverage/conf/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_coverage/conf/spark-defaults.conf\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_coverage/sitecustomize.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/run-tests.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/setup.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/userlibrary.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/hello/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/hello/sub_hello/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/hello/hello.txt\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/userlib-0.1.zip\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/people.json\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/people_array.json\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/people_array_utf16le.json\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/text-test.txt\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/ages.csv\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/streaming/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/test_support/sql/people1.json\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_rddbarrier.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_worker.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_serializers.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_util.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_rdd.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_broadcast.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_appsubmit.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_profiler.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_pin_thread.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_shuffle.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_join.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_taskcontext.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_context.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_readwrite.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_conf.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_daemon.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/mlutils.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/mllibutils.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/utils.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/sqlutils.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/streamingutils.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/accumulators.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/rddsampler.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_algorithms.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_evaluation.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_util.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_feature.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_pipeline.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_wrapper.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_tuning.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_persistence.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_param.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_training_summary.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_linalg.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_image.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_stat.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_base.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/functions.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tuning.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/pipeline.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/base.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/feature.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/stat.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/image.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/classification.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/recommendation.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/regression.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/param/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/param/shared.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tree.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/fpm.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/clustering.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/common.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/linalg/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/evaluation.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/util.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/find_spark_home.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/heapq3.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/serializers.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/java_gateway.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/traceback_utils.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/conf.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_algorithms.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_streaming_algorithms.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_util.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_feature.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_linalg.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_stat.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/feature.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/classification.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/regression.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tree.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/fpm.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/random.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/clustering.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/common.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/linalg/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/util.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/resultiterable.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/profiler.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/statcounter.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/join.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/daemon.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/rdd.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/context.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/cloudpickle.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/version.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/resource.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/files.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/worker.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/shell.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/test_listener.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/test_kinesis.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/test_dstream.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/test_context.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/dstream.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/listener.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/context.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/util.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/status.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_functions.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_readwriter.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_utils.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_grouped_map.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_dataframe.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_map.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_udf.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_streaming.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/__init__.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_serde.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_window.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_group.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_scalar.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_catalog.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_datasources.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_typehints.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_types.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_column.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_context.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_conf.py\r\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_arrow.py\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_session.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/functions.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/serializers.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/__init__.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/typehints.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/map_ops.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/types.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/group_ops.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/utils.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/window.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/udf.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/session.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/column.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/group.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/context.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/types.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/avro/\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/avro/functions.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/avro/__init__.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/shuffle.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/_globals.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/broadcast.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/util.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/.coveragerc\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/index.rst\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/conf.py\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.rst\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/_templates/\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/_templates/layout.html\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/_static/\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/_static/copybutton.js\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/make2.bat\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/make.bat\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/Makefile\n",
      "x spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.resource.rst\n",
      "x spark-3.0.3-bin-hadoop2.7/python/lib/\n",
      "x spark-3.0.3-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip\n",
      "x spark-3.0.3-bin-hadoop2.7/python/lib/pyspark.zip\n",
      "x spark-3.0.3-bin-hadoop2.7/python/run-tests\n",
      "x spark-3.0.3-bin-hadoop2.7/python/setup.cfg\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-respond.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-janino.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-datatables.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-pmml-model.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jakarta-ws-rs-api\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-dnsjava.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jakarta-annotation-api\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-CC0.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jodd.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-machinist.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-leveldbjni.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-join.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-arpack.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jsp-api.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-JTransforms.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-JLargeArrays.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-bootstrap.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-javassist.html\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-zstd.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-json-formatter.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-matchMedia-polyfill.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jakarta.activation-api.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-automaton.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jaxb-runtime.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-mustache.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-istack-commons-runtime.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-vis-timeline.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-re2j.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
      "x spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
    "!tar zxvf spark-3.0.3-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnIAJZrfVXps",
    "outputId": "2f348663-cec2-411d-bbfa-9e712b97a4e9"
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3c20979a8b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#import of the SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         raise Exception(\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;34m\"Unable to find py4j, your SPARK_HOME may not be configured correctly\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         )\n",
      "\u001b[0;31mException\u001b[0m: Unable to find py4j, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#import of the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#inizialization of the Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Assignment2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "nOcFr4uSV03u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdYKtVF_VXpt"
   },
   "source": [
    "## B.1  File import\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In this exercise the goal is to create a Spark DataFrame from the csv file in imput. \n",
    "\n",
    "Recall that in Spark DataFrame the type of the columns is very important for the definition of the internal data representation. \n",
    "    \n",
    "For this step you the target set of typed columns is the following one: \n",
    "<ul>\n",
    "    <li>    $Date\\_Time: Timestamp$ </li>\n",
    "     <li>   $Open: double$ </li>\n",
    "     <li>   $High: double$ </li>\n",
    "    <li>    $Low: double$ </li>\n",
    "    <li>    $Close: double$ </li>\n",
    "    <li>    $Volume\\_BTC: double$ </li>\n",
    "    <li>    $Volume\\_Currency: double$ </li>\n",
    "    <li>    $Weighted\\_Price: double$ </li>\n",
    "</ul>\n",
    "    \n",
    "We will arrive to define the schema in 3 guided steps described in the following sections.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Notice that the header of the $csv$ file contains the data description and that the simple import of the\n",
    "file treats the timestamp column as a String. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In data import you must check that:\n",
    "<ul>\n",
    "    <li>  the types of the imported data (the ones read from the file using the operation you choose) are equal to the types in the given schema</li>\n",
    "    <li>  the names of columns correspond (and make transofrmations if necessary). </li> \n",
    "</ul>\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQZN4xiqVXpt"
   },
   "source": [
    "### <strong> Exercise 1.</strong> First import (1 point)\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Import the csv file in Spark DataFrame. If you have any doubt you can always refer to the Spark 3.1.1 documentation:\n",
    "\n",
    "<a href=\"https://spark.apache.org/docs/3.1.1/\">Spark Reference Documentation</a>\n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imMeOHQmVXpu",
    "outputId": "7ec87c51-dee1-4a21-9287-5d6ac7cab82e"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark' has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b41f4e8d0a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Weighted_Price\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark' has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "# Write the command that creates (reads) a Spark DataFrame and stores the reference in the dfs variable\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "schema = StructType() \\\n",
    "      .add(\"TimeStamp\",IntegerType(),True) \\\n",
    "      .add(\"Open\",DoubleType(),True) \\\n",
    "      .add(\"High\",DoubleType(),True) \\\n",
    "      .add(\"Low\",DoubleType(),True) \\\n",
    "      .add(\"Close\",DoubleType(),True) \\\n",
    "      .add(\"Volumn_BTC\",DoubleType(),True) \\\n",
    "      .add(\"Volumn_Currency\",DoubleType(),True) \\\n",
    "      .add(\"Weighted_Price\",DoubleType(),True) \n",
    "\n",
    "dfs = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv\")\n",
    "\n",
    "#This example reads the data into DataFrame columns \"_c0\" for the first column \n",
    "#and \"_c1\" for the second and so on. \n",
    "#and by default data type for all these columns is treated as String.\n",
    "dfs.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "#show the DataFrame schema\n",
    "dfs\n",
    "\n",
    "\n",
    "#######################\n",
    "# EXPECTED OUTPUT:\n",
    "# DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, Volume_(BTC): double, Volume_(Currency): double, Weighted_Price: double]</font>\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XTbhA1Aip4mM",
    "outputId": "4d1432bf-c59d-4c26-b9f7-02e0fa5732b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TimeStamp=1325317920, Open=4.39, High=4.39, Low=4.39, Close=4.39, Volumn_BTC=0.45558087, Volumn_Currency=2.0000000193, Weighted_Price=4.39),\n",
       " Row(TimeStamp=1325317980, Open=nan, High=nan, Low=nan, Close=nan, Volumn_BTC=nan, Volumn_Currency=nan, Weighted_Price=nan),\n",
       " Row(TimeStamp=1325318040, Open=nan, High=nan, Low=nan, Close=nan, Volumn_BTC=nan, Volumn_Currency=nan, Weighted_Price=nan),\n",
       " Row(TimeStamp=1325318100, Open=nan, High=nan, Low=nan, Close=nan, Volumn_BTC=nan, Volumn_Currency=nan, Weighted_Price=nan),\n",
       " Row(TimeStamp=1325318160, Open=nan, High=nan, Low=nan, Close=nan, Volumn_BTC=nan, Volumn_Currency=nan, Weighted_Price=nan)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following command is going to show 5 rows of the DataFrame\n",
    "dfs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkQ8aYX-VXpv"
   },
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Look again at the target schema:\n",
    "    \n",
    "<ul>\n",
    "    <li>    $Date\\_Time: Timestamp$ </li>\n",
    "     <li>   $Open: double$ </li>\n",
    "     <li>   $High: double$ </li>\n",
    "    <li>    $Low: double$ </li>\n",
    "    <li>    $Close: double$ </li>\n",
    "    <li>    $Volume\\_BTC: double$ </li>\n",
    "    <li>    $Volume\\_Currency: double$ </li>\n",
    "    <li>    $Weighted\\_Price: double$ </li>\n",
    "</ul>\n",
    "    \n",
    "You notice that the import data has three problems with respect to the target schema:\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "    <li> the $Date\\_Time$ column is not present in the original file </li>\n",
    "    <li> there is an $int$ column $Timestamp$ that can be converted and transformed to a $Date$</li> \n",
    "    <li> some of the column names contain not required parentesis. </li>\n",
    "</ul>     \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUS9JWIcVXpv"
   },
   "source": [
    "### <strong> Exercise 2. </strong> Timestamp column (1 point)\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Refine the import of the csv file and convert the \"timestamp\" column in the proper $Timestamp$ type:\n",
    "    <ul>\n",
    "        <li>   Create a new column <code>Date\\_Time</code> that is the conversion of the $String$ column $Timestamp$ in $Timestamp$ type  </li>\n",
    "</ul>\n",
    "The Dataframe are immutable structure, then your procedure will use a command (discussed in the slides) that will create a new Spark $DataFrame$ from the $dfs$ $DataFrame$ having a different schema. \n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Look at the timestamp column of the csv file and from the imported DataFrame \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pj8BeALtVXpv",
    "outputId": "b5fa33a1-1b1a-4a83-eb73-1f170c401947"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dd5c19ab7e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#'''############## WRITE YOUR CODE HERE ##############'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdfsdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date_Time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimestampType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#'''############## END OF THE EXERCISE ##############'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "# write the command that creates a new Data Frame Spark with Date_Time column\n",
    "# and stores the reference in the dfsdt variable (it must be a DataFrame Spark with Date_Time column)\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "from pyspark.sql.functions import lit\n",
    "dfsdt = dfs.withColumn('Date_Time', lit(None).cast(TimestampType()))\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "#show the DataFrame schema\n",
    "dfsdt\n",
    "\n",
    "#######################\n",
    "# EXPECTED OUTPUT:\n",
    "# DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, \n",
    "#Volume_(BTC): double, Volume_(Currency): double, Weighted_Price: double, Date_Time: timestamp]\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "iBcoKytNVXpw",
    "outputId": "ffc6d018-21eb-45a2-c51a-a2e8be10107c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TimeStamp=1325317920, Open=4.39, High=4.39, Low=4.39, Close=4.39, Volumn_BTC=0.45558087, Volumn_Currency=2.0000000193, Weighted_Price=4.39, Date_Time=None),\n",
       " Row(TimeStamp=1325317980, Open=nan, High=nan, Low=nan, Close=nan, Volumn_BTC=nan, Volumn_Currency=nan, Weighted_Price=nan, Date_Time=None),\n",
       " Row(TimeStamp=1325318040, Open=nan, High=nan, Low=nan, Close=nan, Volumn_BTC=nan, Volumn_Currency=nan, Weighted_Price=nan, Date_Time=None),\n",
       " Row(TimeStamp=1325318100, Open=nan, High=nan, Low=nan, Close=nan, Volumn_BTC=nan, Volumn_Currency=nan, Weighted_Price=nan, Date_Time=None),\n",
       " Row(TimeStamp=1325318160, Open=nan, High=nan, Low=nan, Close=nan, Volumn_BTC=nan, Volumn_Currency=nan, Weighted_Price=nan, Date_Time=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show 5 rows of the DataFrame\n",
    "dfsdt.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWEJTFojVXpw"
   },
   "source": [
    "### <strong> Exercise 3.</strong> Column names (2 points)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "As you can see from the output of the previous exercise the names of the columns still present some problems since there are some parentesis that are not required.\n",
    "    <ul>\n",
    "     <li> Remove the not required parentesis from the colum names </li>\n",
    "     <li> Hint: look at the documentation of DataFrame API and check the operation for column renaming </li>\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "yRv_VbgYVXpw",
    "outputId": "29a9a7b7-334e-4129-ce96-b3b82fbf82f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[TimeStamp: int, Open: double, High: double, Low: double, Close: double, Volumn_BTC: double, Volumn_Currency: double, Weighted_Price: double, Date_Time: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write the command that creates a new Data Frame Spark with correct names for all the columns\n",
    "# and store the reference in the dfscr variable (Data Frame Spark with Correct Names)\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "import datetime\n",
    "\n",
    "# #udf to convert the ts to timestamp\n",
    "# get_timestamp = udf(lambda x : datetime.datetime.fromtimestamp(x/1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')\n",
    "\n",
    "# #apply this udf in the dataframe with your timestamp\n",
    "# dfscr = dfsdt.withColumn(\"Date_Time\", get_timestamp(dfsdt.TimeStamp))\n",
    "\n",
    "dfscr = dfsdt.withColumn(\"Date_Time\", from_unixtime('TimeStamp', format='yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# dfscr = dfsdt.withColumn('Date_Time', func.from_unixtime('TimeStamp').cast(DateType())).show()\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "#show the DataFrame schema\n",
    "dfscr\n",
    "\n",
    "#######################\n",
    "# EXPECTED OUTPUT:\n",
    "#DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, \n",
    "#          Volume_BTC: double, Volume_Currency: double, Weighted_Price: double]\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "YU2SFo0UVXpw",
    "outputId": "ce122cc4-e673-4d1e-9137-e3fa416c27bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
      "| TimeStamp|Open|High| Low|Close|Volumn_BTC|Volumn_Currency|Weighted_Price|          Date_Time|\n",
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
      "|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|\n",
      "|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|\n",
      "|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|\n",
      "|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|\n",
      "|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|\n",
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show 5 rows of the DataFrame\n",
    "dfscr.show(5)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
    "#| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
    "#|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|\n",
    "#|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|\n",
    "#|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|\n",
    "#|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|\n",
    "#|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw7ktEJKVXpx"
   },
   "source": [
    "## B.2 DataFrame columns \n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "    \n",
    "In this part of the exercise we are going continue to  modify in the Spark DataFrames.\n",
    "\n",
    "    \n",
    "Remember that using  PySpark, it's possible to access a DataFrame's columns either by attribute (<code>df.attributeName</code>) or by indexing <code>(df['attributeName'])</code>.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "    \n",
    "Loook at the list of the functions to get familiar with the documentation: some functions that can be of help to manipulate the schema:\n",
    "    \n",
    "<ul>\n",
    "     <li>    <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\">Spark Functions</a>.  </li>\n",
    "</ul>    \n",
    "    \n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBgUaY_oVXpx"
   },
   "source": [
    "### <strong> Exercise 4.</strong>  Add two new columns to the DataFrame (2 points)\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to extend the DataFrame with two other columns: given the $Date\\_Time$ column create two new columns ($Year$ and $Month$) that contain \n",
    "    <ul>\n",
    "     <li> the year </li>\n",
    "     <li> the month of the year </li>\n",
    "</ul>\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDH7YsSeVXpx"
   },
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">    \n",
    "Look at the documentation of Spark functions and find the two functions that are convenient for this use case (hint: the name of the columns can help: <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\">Spark Functions</a>)\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "mpL3519cVXpx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the functions that you will use\n",
    "\n",
    "############## WRITE YOUR CODE HERE ##############\n",
    "\n",
    "\n",
    "############## END OF THE EXERCISE ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zsehHsySVXpz",
    "outputId": "54e912fa-7c58-475a-dbaf-4a07cd3c9a39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "| TimeStamp|Open|High| Low|Close|Volumn_BTC|Volumn_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
      "|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
      "|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
      "|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
      "|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write the command that creates a new Data Frame Spark with the two additional columns\n",
    "# and store the reference in the dfsym variable (Data Frame Spark with Correct Names)\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "dfsym = dfscr.withColumn(\"Year\", year('Date_Time')) \\\n",
    "              .withColumn(\"Month\", month('Date_Time'))\n",
    "\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "\n",
    "dfsym.show(5)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
    "#|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
    "#|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
    "#|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
    "#|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDMJbhTbVXpz"
   },
   "source": [
    "###  <strong>Exercise 5.</strong>  Drop Timestamp (2 points)\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Finally we clean the schema and we can remove the the $Timestamp$ column.\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qm8I1zhzVXp0",
    "outputId": "5114553f-dad7-4fd6-d787-e1d77e67d6f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "|Open|High| Low|Close|Volumn_BTC|Volumn_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
      "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
      "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write the command that creates a new DataFrame Spark from the dfsym without the Timestamp column\n",
    "# and store the reference in the dfc variable (Data Frame Spark Clean)\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "dfsc = dfsym.drop(\"TimeStamp\")\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "\n",
    "dfsc.show(5)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
    "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
    "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dm-ovIcVXp0"
   },
   "source": [
    "#  C. Using Parquet\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In order to gain in performance in the following it is a good idea, as we have seen at lesson, to use a NoSQL structure, here Parquet, that will \n",
    "    allow \n",
    "to partition the SparkDataframe and to store it in multiple Parquet files. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WicAuWMfA9xo"
   },
   "source": [
    "## C.1 Saving data in Parquet\n",
    "    \n",
    "For this first example partition the file according to:\n",
    "    \n",
    " <ul>\n",
    "     <li> the year </li>\n",
    "             <li> the month of the year </li>\n",
    "</ul>\n",
    "The $partitionBy()$ operation can help for this step (Documentation of reference: <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\">Spark Functions</a>).\n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "84UNDFHiVXp0",
    "outputId": "689769ef-926d-4b65-805d-c34e20747326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "|Open|High| Low|Close|Volumn_BTC|Volumn_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
      "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:57:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:58:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:59:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:00:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:01:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:02:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:03:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:04:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:05:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:06:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:07:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:08:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:09:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:10:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:11:00|2011|   12|\n",
      "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "vTR8AxS2VXp0",
    "outputId": "d1a75c42-4650-4b5c-d86c-5bf37205177f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write to Parquet done\n"
     ]
    }
   ],
   "source": [
    "# here you can see and check the command that saves the dfsc DataFrame in Parquet\n",
    "\n",
    "dfsc.write.partitionBy([\"Year\", \"Month\"]).parquet(\"BTC/\",mode='overwrite')\n",
    "\n",
    "\n",
    "print(\"write to Parquet done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic-eLcr0VXp0"
   },
   "source": [
    "##  C.2 Check the folder Structure\n",
    "\n",
    " \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Look at the folder structure that has been created for the storage of the file. You see how the partitioning stategy of Parquet and the data distribution of Spark can be used, explicitely or implicitely, to improve performance.\n",
    "\n",
    "While you navigate (and the folder structure) data remember that in the data access:\n",
    "    \n",
    " <ul>\n",
    "     <li> the navigation is done using Parquet </li>\n",
    "     <li> the leaf contain the encoded Parquet files </li>\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dC1C75uIVXp1"
   },
   "outputs": [],
   "source": [
    "#BTC\n",
    "#        ├── Year=2011\n",
    "#        │   ├── ...\n",
    "#        │   │\n",
    "#        │   ├── month=12\n",
    "#        ├── Year=2012\n",
    "#        │   ├── month=1\n",
    "#        │   ├── ...\n",
    "#        │   │\n",
    "#       ...\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JeyD03ipig_"
   },
   "source": [
    "This folder structure correspond to a phisical and logical data partition and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMCbaNcPVXp1"
   },
   "source": [
    "# D. Pandas\n",
    "\n",
    " \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "This data organization opens the opportunity to read data also using Pandas and not using Parquet.\n",
    "    \n",
    "Look at the documentation and check how you can read a Parquet structure and store it in a Pandas DataFrame:\n",
    "<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html\">Pandas and Parquet</a>\n",
    "\n",
    "Notice how at the data-exchange base there is the presence of Arrow (thanks to $pyarrow$).\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Write the command that using Pandas read the data for the year 2011.\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "id": "B11RsK9YVXp1"
   },
   "outputs": [],
   "source": [
    "#import of pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "collapsed": true,
    "id": "jlRYuW0hVXp1",
    "outputId": "aa1ec91b-219e-41c2-cebd-d067fd708b57",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volumn_BTC</th>\n",
       "      <th>Volumn_Currency</th>\n",
       "      <th>Weighted_Price</th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.39</td>\n",
       "      <td>2011-12-31 07:52:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 07:53:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 07:54:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 07:55:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 07:56:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 23:55:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 23:56:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 23:57:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 23:58:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 23:59:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>968 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open  High   Low  ...  Weighted_Price            Date_Time  Month\n",
       "0    4.39  4.39  4.39  ...            4.39  2011-12-31 07:52:00     12\n",
       "1     NaN   NaN   NaN  ...             NaN  2011-12-31 07:53:00     12\n",
       "2     NaN   NaN   NaN  ...             NaN  2011-12-31 07:54:00     12\n",
       "3     NaN   NaN   NaN  ...             NaN  2011-12-31 07:55:00     12\n",
       "4     NaN   NaN   NaN  ...             NaN  2011-12-31 07:56:00     12\n",
       "..    ...   ...   ...  ...             ...                  ...    ...\n",
       "963   NaN   NaN   NaN  ...             NaN  2011-12-31 23:55:00     12\n",
       "964   NaN   NaN   NaN  ...             NaN  2011-12-31 23:56:00     12\n",
       "965   NaN   NaN   NaN  ...             NaN  2011-12-31 23:57:00     12\n",
       "966   NaN   NaN   NaN  ...             NaN  2011-12-31 23:58:00     12\n",
       "967   NaN   NaN   NaN  ...             NaN  2011-12-31 23:59:00     12\n",
       "\n",
       "[968 rows x 9 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we show you how we can create DataFrame using Pandas functions and reading from Parquet the data only for the year 2011/\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"BTC/Year=2011\")\n",
    "\n",
    "df\n",
    "#######################\n",
    "# Check the expected output:\n",
    "#Open\tHigh\tLow\tClose\tVolume_BTC\tVolume_Currency\tWeighted_Price\tDate_Time\tMonth\n",
    "#0\t4.39\t4.39\t4.39\t4.39\t0.455581\t2.0\t4.39\t2011-12-31 07:52:00\t12\n",
    "#1\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:53:00\t12\n",
    "#2\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:54:00\t12\n",
    "#3\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:55:00\t12\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAYamKaEVXp1"
   },
   "source": [
    "###  D.1 Read Parquet file\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Here you can see now the the Spark DataFrame is created from Parquet data.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6C0iA8SqVXp2",
    "outputId": "f64611eb-a111-4aa8-f72d-b386237d0cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read done\n"
     ]
    }
   ],
   "source": [
    "# And here how we can create a DataFrame using Spark and reading the whole data/\n",
    "\n",
    "dfs = spark.read.parquet(\"BTC/\")\n",
    "\n",
    "print(\"read done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjH0cqC5VXp2"
   },
   "source": [
    "## <strong>Exercise 6</strong>. Verify number of column and count the number of rows (2 points)\n",
    "    \n",
    "<p align=\"justify\">\n",
    "Maybe you have not noticed that the volume of data we are treating is not so small as it seems. \n",
    "Count how many rows we are manipulating in the dataframe <code>dfs</code>\n",
    "<font size=\"3\">\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nYqK9VD8VXp2",
    "outputId": "62c7ce97-a57e-4bea-d27d-720080a9aa7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1380877\n"
     ]
    }
   ],
   "source": [
    "# Write the command that returns the number of rows of the DataFrame\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "count = dfs.count()\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "print(count)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "# 4857377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZJVbEunCVXp2",
    "outputId": "9bece875-d272-4c77-dbe3-c545fc65ffaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volumn_BTC: double (nullable = true)\n",
      " |-- Volumn_Currency: double (nullable = true)\n",
      " |-- Weighted_Price: double (nullable = true)\n",
      " |-- Date_Time: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We can also check and verify the schema of the DataFrame\n",
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoAfjdAuVXp3"
   },
   "source": [
    "# E. Statistics\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to calculate the statistics of the bitcoin by month for all the years.\n",
    "\n",
    "The computed statistics will be stored in a DataFrame having this schema\n",
    "<ul>\n",
    "     <li>   Mean_Vol  : double </li>\n",
    "     <li>   Std_Vol   : double </li>\n",
    "     <li>   Min_Vol   : double </li>\n",
    "     <li>   Max_vol   : double </li>\n",
    "     <li>   Year      : int </li>\n",
    "     <li>   Month     : int </li>\n",
    "  \n",
    "</ul>\n",
    "\n",
    "In this exercise you will have two develop different methodologies to compute the statistics:\n",
    "<ul>\n",
    "    <li>   using the <code>applyInPandas()</code> Pyspark function and the Pandas functions </li>\n",
    "     <li>  only using the Pyspark functionnalities </li>\n",
    "</ul>\n",
    "The statistics computed should be stored in a Pandas DataFrame with both the two approaches.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gts_hy5HVXp4"
   },
   "source": [
    "## E.1. Spark applyinPandas\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The solution with $applyinPandas$ \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytXT9ViT7ucL",
    "outputId": "6649a5ae-33f3-46db-920e-446c38e3b5eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     4.000000\n",
       "mean     23.829470\n",
       "std      22.711133\n",
       "min       0.455581\n",
       "25%       6.863895\n",
       "50%      23.431149\n",
       "75%      40.396723\n",
       "max      48.000000\n",
       "Name: Volumn_BTC, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Volumn_BTC\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "id": "icjKlPHcVXp4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# the Python function that must be used. \n",
    "\n",
    "def compute_stats(key,df):\n",
    "    res = df[\"Volumn_BTC\"].describe()\n",
    "\n",
    "    res_dict = {}\n",
    "    for index, value in res.items():\n",
    "        if index == \"mean\":\n",
    "            res_dict[\"Mean_Vol\"] = value\n",
    "        elif index == \"std\":\n",
    "            res_dict[\"Std_Vol\"] = value\n",
    "        elif index == \"min\":\n",
    "            res_dict[\"Min_Vol\"] = value\n",
    "        elif index == \"max\":\n",
    "            res_dict[\"Max_Vol\"] = value\n",
    "\n",
    "    final =  pd.DataFrame([res_dict])\n",
    "    final[\"Year\"]  = key[0]\n",
    "    final[\"Month\"] = key[1]\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsqKcxxNVXp4"
   },
   "source": [
    "### <strong>Exercise 7</strong>. The two parameters of the Python function (2 points)\n",
    "The two parameters of the Python <code>applyinPandas(funct,schema)</code> function (2 points)\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "    Look at the documentation of the <code>applyinPandas(funct,schema)</code> (<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.applyInPandas.html\">click here to go to the documentation of <code>applyinpandas</code></a>) and describe how it works in detail from the DataFrame point of view in our example (what the $key$ and the $df$ will contain in our example).\n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99_WnUHTVXp5"
   },
   "source": [
    "#### apply in pandas takes a pandas.DataFrame and return another pandas.DataFrame. For each group, year and month, all columns are passed together as a pandas.DataFrame to the compute stats function and the returned pandas.DataFrame are combined as a DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shVh5lBcVXp5"
   },
   "source": [
    "### <strong>Exercise 8</strong>. The two parameters in action (1 point)\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Compute the statistics using then the $applyInPandas$ and the provided functions. \n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IebDul7NVXp6",
    "outputId": "fe2d7c76-e8f8-4231-c457-efb979aaba6e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fb1f62b291ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#'''############## WRITE YOUR ANSWER HERE ##############'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m statsdf = dfs.groupby('Year','Month').applyInPandas(compute_stats, \\\n\u001b[0m\u001b[1;32m      8\u001b[0m             schema = schema) \n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "schema = \"Mean_Vol double, Std_Vol double, Min_Vol double, Max_Vol double,Year int, Month int\"\n",
    "\n",
    "# Write the command that will store in the variable statsdf the DataFrame \n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "statsdf = dfs.groupby('Year','Month').applyInPandas(compute_stats, \\\n",
    "            schema = schema) \n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "statsdf.show(5)\n",
    "\n",
    "# statsdf\n",
    "\n",
    "\n",
    "####### EXPECTED OUTPUT\n",
    "#+------------------+------------------+----------+------------+----+-----+\n",
    "#|          Mean_Vol|           Std_Vol|   Min_Vol|     Max_Vol|Year|Month|\n",
    "#+------------------+------------------+----------+------------+----+-----+\n",
    "#| 20.39613620802532| 54.24699556644988|    9.4E-5|2258.8231405|2012|   10|\n",
    "#|12.095179597807542|44.149334198665166| 2.0452E-4|2037.2239038|2015|    2|\n",
    "#| 6.147061206279663|17.745599117954125|0.00127783|564.21436237|2019|   10|\n",
    "#| 8.468866447160776|  28.9837002907642|    1.0E-8|1616.0600006|2017|    3|\n",
    "#| 8.684880075589284| 17.69646210434965|       0.0|533.10078293|2017|    8|\n",
    "#+------------------+------------------+----------+------------+----+-----+#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqmV2sIqU4Gh",
    "outputId": "4af5ebaf-99aa-46d5-acc2-6cc40a72f565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6Pdf1N-VXp6"
   },
   "source": [
    "### <strong>Exercise 9</strong>. The statsdf DataFrame (1 point)\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Which kind of DataFrame is statsdf?\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbXKRuBEVXp6"
   },
   "source": [
    "#### WRITE YOUR ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z6Y-NBM8_I4B",
    "outputId": "a29fad9e-e088-4951-9e4f-67ae5e23252e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mean_Vol', 'double'),\n",
       " ('Std_Vol', 'double'),\n",
       " ('Min_Vol', 'double'),\n",
       " ('Max_Vol', 'double'),\n",
       " ('Year', 'int'),\n",
       " ('Month', 'int')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statsdf.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgGmWPhSVXp7"
   },
   "source": [
    "\n",
    "### <strong>Exercise 10</strong>. DataFrame in Pandas (2 points)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Since we computed a stat by month the results will be small (we will have only one row by month)\n",
    "we can get and handle all the results in memory in Pandas.  \n",
    "    \n",
    "Notice that Spark is lazy so the $toPandas$ action will trigger the computation.\n",
    "    \n",
    "Write the command that will do the operation.\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Wupi4CTgVXp7",
    "outputId": "d94236a1-0df4-4710-df56-166d0b91843f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 ms, sys: 2.66 ms, total: 15.7 ms\n",
      "Wall time: 61 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Write the command that will store in the variable stats_dfp the outoput DataFrame \n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "def compute_stats_1(key,df):\n",
    "    res = df[\"Volumn_BTC\"].describe()\n",
    "\n",
    "    res_dict = {}\n",
    "    for index, value in res.items():\n",
    "        if index == \"mean\":\n",
    "            res_dict[\"Mean_Vol\"] = value\n",
    "        elif index == \"std\":\n",
    "            res_dict[\"Std_Vol\"] = value\n",
    "        elif index == \"min\":\n",
    "            res_dict[\"Min_Vol\"] = value\n",
    "        elif index == \"max\":\n",
    "            res_dict[\"Max_Vol\"] = value\n",
    "\n",
    "    final =  pd.DataFrame([res_dict])\n",
    "    final['Year'] = df['Year']\n",
    "    final[\"Month\"]  = key\n",
    "    return final\n",
    "\n",
    "stats_dfp = dfs.groupby('Month').applyInPandas(compute_stats_1, \\\n",
    "            schema = \"Mean_Vol double, Std_Vol double, \\\n",
    "            Min_Vol double, Max_Vol double, Year int, Month int\")\n",
    "type(stats_dfp)\n",
    "#'''############## END OF THE EXERCISE ##############'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "collapsed": true,
    "id": "Ve6u_nzSVXp8",
    "outputId": "d2f49c9a-111b-4b55-be81-6799616b4d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean_Vol</th>\n",
       "      <th>Std_Vol</th>\n",
       "      <th>Min_Vol</th>\n",
       "      <th>Max_Vol</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.062208</td>\n",
       "      <td>56.942152</td>\n",
       "      <td>4.442000e-05</td>\n",
       "      <td>2196.866405</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.905119</td>\n",
       "      <td>36.108533</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1409.877007</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.007266</td>\n",
       "      <td>37.463433</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2285.000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.763788</td>\n",
       "      <td>46.147974</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1453.822424</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.014784</td>\n",
       "      <td>29.732479</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1415.686400</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.329530</td>\n",
       "      <td>39.341147</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1138.049014</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.966906</td>\n",
       "      <td>46.206634</td>\n",
       "      <td>9.900000e-07</td>\n",
       "      <td>4111.876106</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.437348</td>\n",
       "      <td>44.757863</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2644.436194</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.916682</td>\n",
       "      <td>40.716253</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18.655983</td>\n",
       "      <td>58.118384</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2258.823141</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean_Vol    Std_Vol       Min_Vol      Max_Vol  Year  Month\n",
       "0  23.062208  56.942152  4.442000e-05  2196.866405  2013     12\n",
       "1  12.905119  36.108533  0.000000e+00  1409.877007  2014      1\n",
       "2  10.007266  37.463433  0.000000e+00  2285.000000  2014      6\n",
       "3  13.763788  46.147974  0.000000e+00  1453.822424  2014      3\n",
       "4   9.014784  29.732479  0.000000e+00  1415.686400  2014      5\n",
       "5  12.329530  39.341147  1.000000e-08  1138.049014  2013      9\n",
       "6  14.966906  46.206634  9.900000e-07  4111.876106  2014      4\n",
       "7  12.437348  44.757863  0.000000e+00  2644.436194  2013      8\n",
       "8  10.916682  40.716253  0.000000e+00  1503.000000  2014      7\n",
       "9  18.655983  58.118384  0.000000e+00  2258.823141  2013     10"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results\n",
    "print(type(stats_dfp))\n",
    "stats_dfp = stats_dfp.toPandas()\n",
    "stats_dfp.head(10)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#\tMean_Vol\tStd_Vol\tMin_Vol\tMax_Vol\tYear\tMonth\n",
    "#0\t20.396136\t54.246996\t9.400000e-05\t2258.823141\t2012\t10\n",
    "#1\t12.095180\t44.149334\t2.045200e-04\t2037.223904\t2015\t2\n",
    "#2\t6.147061\t17.745599\t1.277830e-03\t564.214362\t2019\t10\n",
    "#3\t8.468866\t28.983700\t1.000000e-08\t1616.060001\t2017\t3\n",
    "#4\t8.684880\t17.696462\t0.000000e+00\t533.100783\t2017\t8\n",
    "#5\t16.040933\t57.641501\t2.044000e-05\t4111.876106\t2014\t4\n",
    "#6\t4.984386\t18.903445\t1.054000e-05\t822.866974\t2020\t6\n",
    "#7\t8.331579\t18.350084\t5.758000e-04\t806.636224\t2019\t5\n",
    "#8\t8.621910\t18.820399\t4.047000e-04\t602.282607\t2017\t10\n",
    "#9\t3.106413\t10.738051\t3.300000e-06\t582.564185\t2018\t10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LX4X2ioqVXp8"
   },
   "source": [
    "###  <strong>Exercise 11</strong>. Show the stats of the stats (1 point)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to calculate the statistics of the bitcoin by month for all the years.\n",
    "\n",
    "The computed statistics will be stored in a DataFrame having this schema\n",
    "<ul>\n",
    "     <li>   the min of the set min values </li>\n",
    "     <li>   the mean of the set of mean values </li>\n",
    "     <li>   ... </li> \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "collapsed": true,
    "id": "4FFQDxOyVXp8",
    "outputId": "8b8adc80-156d-41aa-efee-9479ca104e78"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean_Vol</th>\n",
       "      <th>Std_Vol</th>\n",
       "      <th>Min_Vol</th>\n",
       "      <th>Max_Vol</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.535749</td>\n",
       "      <td>43.578531</td>\n",
       "      <td>0.014834</td>\n",
       "      <td>1324.013668</td>\n",
       "      <td>2011.676471</td>\n",
       "      <td>6.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.692091</td>\n",
       "      <td>19.655978</td>\n",
       "      <td>0.079201</td>\n",
       "      <td>962.223241</td>\n",
       "      <td>6.709200</td>\n",
       "      <td>3.424250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.031777</td>\n",
       "      <td>6.740555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.312196</td>\n",
       "      <td>1974.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31.504424</td>\n",
       "      <td>106.976067</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>4111.876106</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Mean_Vol     Std_Vol    Min_Vol      Max_Vol         Year      Month\n",
       "count  33.000000   33.000000  33.000000    33.000000    34.000000  34.000000\n",
       "mean   16.535749   43.578531   0.014834  1324.013668  2011.676471   6.176471\n",
       "std     6.692091   19.655978   0.079201   962.223241     6.709200   3.424250\n",
       "min     4.031777    6.740555   0.000000    43.312196  1974.000000   1.000000\n",
       "max    31.504424  106.976067   0.455581  4111.876106  2014.000000  12.000000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the command that will show and compute the stats on the numerical columns of the statsdf DataFrame\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "#statsdf = statsdf.toPandas()\n",
    "stats_stats = statsdf.describe()\n",
    "stats_stats.iloc[[0,1,2,3,7],:]\n",
    "\n",
    "# stats_stats.loc[['count', 'means', 'std', 'min', 'max']]\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
    "#|summary|          Mean_Vol|           Std_Vol|             Min_Vol|           Max_Vol|              Year|             Month|\n",
    "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
    "#|  count|               112|               112|                 112|               112|               112|               112|\n",
    "#|   mean|10.782191354847754|28.871463944232485|0.004551177678571...|1067.2847720235718|2016.0892857142858| 6.428571428571429|\n",
    "#| stddev| 6.488551661205522| 18.11344145463867|0.043048607639448476| 895.8083462469303|2.7164947320662614|3.5252353718985097|\n",
    "#|    min| 2.929999689326444| 6.490701567379118|                 0.0|       43.31219578|              2011|                 1|\n",
    "#|    max|31.504423573146152|106.97606692383131|          0.45558087|      5853.8521659|              2021|                12|\n",
    "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wtYR3CaVXp8"
   },
   "source": [
    "# F. Plotting and equivalence \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to plot the resutls of the statistics by year and month (that will be in the $x$ orizontal axis of the plot). \n",
    "\n",
    "$Plotly$ will be used for the plotting\n",
    "    \n",
    "\n",
    "This provided version of the code is fully working in Python.\n",
    "    \n",
    "\n",
    "A Python routine converts the two columns $Year$ and $Month$ into a $DateTime$ column 'Date' (in order to plot the data in relation with the date).\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "collapsed": true,
    "id": "5E1eQmpJVXp9",
    "outputId": "ce0a2dc2-d24e-4cbf-d5f8-0b5a81223ba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#install plotly and import the libraries\n",
    "\n",
    "!pip install plotly\n",
    "\n",
    "from plotly.offline import iplot,init_notebook_mode\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "id": "wlyn4HLZVXp9"
   },
   "outputs": [],
   "source": [
    "#Helper function that converts the Year Month of our data into Date type\n",
    "\n",
    "def get_date_from_year_month(df):\n",
    "    df[\"Date\"] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%m')\n",
    "    return df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "mS9hH6hHVXp9",
    "outputId": "2adcf297-4533-4dbb-94d7-867fce064a70"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean_Vol</th>\n",
       "      <th>Std_Vol</th>\n",
       "      <th>Min_Vol</th>\n",
       "      <th>Max_Vol</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.437348</td>\n",
       "      <td>44.757863</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2644.436194</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>2013-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.329530</td>\n",
       "      <td>39.341147</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1138.049014</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>2013-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18.655983</td>\n",
       "      <td>58.118384</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2258.823141</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>2013-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.415995</td>\n",
       "      <td>57.998610</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>2958.477574</td>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>2013-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.062208</td>\n",
       "      <td>56.942152</td>\n",
       "      <td>4.442000e-05</td>\n",
       "      <td>2196.866405</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>2013-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.905119</td>\n",
       "      <td>36.108533</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1409.877007</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.523314</td>\n",
       "      <td>75.294714</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>3379.750829</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.763788</td>\n",
       "      <td>46.147974</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1453.822424</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.966906</td>\n",
       "      <td>46.206634</td>\n",
       "      <td>9.900000e-07</td>\n",
       "      <td>4111.876106</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.014784</td>\n",
       "      <td>29.732479</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1415.686400</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.007266</td>\n",
       "      <td>37.463433</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2285.000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>2014-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.916682</td>\n",
       "      <td>40.716253</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>2014-07-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Mean_Vol    Std_Vol       Min_Vol      Max_Vol  Year  Month       Date\n",
       "7   12.437348  44.757863  0.000000e+00  2644.436194  2013      8 2013-08-01\n",
       "5   12.329530  39.341147  1.000000e-08  1138.049014  2013      9 2013-09-01\n",
       "9   18.655983  58.118384  0.000000e+00  2258.823141  2013     10 2013-10-01\n",
       "10  23.415995  57.998610  1.000000e-08  2958.477574  2013     11 2013-11-01\n",
       "0   23.062208  56.942152  4.442000e-05  2196.866405  2013     12 2013-12-01\n",
       "1   12.905119  36.108533  0.000000e+00  1409.877007  2014      1 2014-01-01\n",
       "11  22.523314  75.294714  1.000000e-08  3379.750829  2014      2 2014-02-01\n",
       "3   13.763788  46.147974  0.000000e+00  1453.822424  2014      3 2014-03-01\n",
       "6   14.966906  46.206634  9.900000e-07  4111.876106  2014      4 2014-04-01\n",
       "4    9.014784  29.732479  0.000000e+00  1415.686400  2014      5 2014-05-01\n",
       "2   10.007266  37.463433  0.000000e+00  2285.000000  2014      6 2014-06-01\n",
       "8   10.916682  40.716253  0.000000e+00  1503.000000  2014      7 2014-07-01"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this phase we need to sort by the date to allow parallelisation of shuffled the results\n",
    "\n",
    "stats_dfp = get_date_from_year_month(stats_dfp)    \n",
    "stats_dfp.sort_values(by = 'Date',inplace = True)\n",
    "stats_dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UunBZ10kVXp-",
    "outputId": "6400292a-460f-486d-db06-5fb94972f7d0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
       "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
       "            <div id=\"9b1f0729-bce9-4b46-80cc-f653e15daa86\" class=\"plotly-graph-div\" style=\"height:1000px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                \n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"9b1f0729-bce9-4b46-80cc-f653e15daa86\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '9b1f0729-bce9-4b46-80cc-f653e15daa86',\n",
       "                        [{\"type\": \"scatter\", \"x\": [\"2013-08-01T00:00:00\", \"2013-09-01T00:00:00\", \"2013-10-01T00:00:00\", \"2013-11-01T00:00:00\", \"2013-12-01T00:00:00\", \"2014-01-01T00:00:00\", \"2014-02-01T00:00:00\", \"2014-03-01T00:00:00\", \"2014-04-01T00:00:00\", \"2014-05-01T00:00:00\", \"2014-06-01T00:00:00\", \"2014-07-01T00:00:00\"], \"y\": [12.437347808829253, 12.329530266987739, 18.65598265106828, 23.415995382718275, 23.062208254295115, 12.905119092518701, 22.523314234906593, 13.76378815606584, 14.966905762942186, 9.014784462475216, 10.007266200041803, 10.91668243471151]}],\n",
       "                        {\"height\": 1000, \"showlegend\": true, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Average Volume by Month of BTC\"}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('9b1f0729-bce9-4b46-80cc-f653e15daa86');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                \n",
       "            </script>\n",
       "        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTTING OF THE MEAN VOLUME BY MONTH\n",
    "mean_vol_trac = {\n",
    "    \"x\": stats_dfp.Date,\n",
    "    \"y\": stats_dfp[\"Mean_Vol\"],\n",
    "}\n",
    "\n",
    "layout = {\n",
    "  \"height\":1000,\n",
    "  \"showlegend\": True, \n",
    "  \"title\": \"Average Volume by Month of BTC\",\n",
    "}\n",
    "\n",
    "fig = go.Figure(data=[mean_vol_trac], layout=layout)\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1XHlwrvVXp-"
   },
   "source": [
    "###  <strong> Exercise 12 </strong> - Compute the statistics using Pyspark (1 point)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to calculate the statistics of the bitcoin as we did before but using Pandas.\n",
    "\n",
    "The steps will be:\n",
    "<ul>\n",
    "     <li>   import data from Parquet in a Spark DataFrame </li>\n",
    "     <li>   remove null values </li>\n",
    "     <li>   perform the aggregation of the results </li> \n",
    "     <li>   convert the results to Pandas </li> \n",
    " \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcS4_jawVXp_",
    "outputId": "d19ae6ed-929b-43a4-d7ce-8e349c313e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.5 ms, sys: 2.83 ms, total: 44.4 ms\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# solution to compute the statistics using pyspark function \n",
    "\n",
    "from pyspark.sql.functions import min, max, mean, stddev\n",
    "\n",
    "\n",
    "# full spark dataframe (recall exercise 8a)\n",
    "df_spark = spark.read.parquet(\"BTC/\") \n",
    "\n",
    "# the na drop is important to be able to compute properly the stats\n",
    "# look at the documentation of the na.drop function\n",
    "\n",
    "group_ym = df_spark.na.drop().select([\"Volumn_BTC\",\"Year\",\"Month\"]).groupBy([\"Year\",\"Month\"])\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''         \n",
    "    \n",
    "# aggregation \n",
    "# notice that the argument of the agg function is strictly related to min Vol, max Vol, mean, and stddev.\n",
    "\n",
    "res_df = group_ym.agg(F.count(\"Volumn_BTC\").alias('count'), \\\n",
    "             F.mean(\"Volumn_BTC\").alias('mean'), \\\n",
    "             F.stddev(\"Volumn_BTC\").alias('std'), \\\n",
    "             F.min(\"Volumn_BTC\").alias('min'), \\\n",
    "             F.max(\"Volumn_BTC\").alias('max'))\n",
    "\n",
    "# # #conversion the results to pandas\n",
    "stats_dfs = res_df.toPandas()\n",
    "\n",
    "# #'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "stats_dfs = get_date_from_year_month(stats_dfs)    \n",
    "stats_dfs.sort_values(by = 'Date',inplace = True)\n",
    "stats_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAzvUAQJVXp_"
   },
   "source": [
    "### Extra. Equivalence of results\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Now that you have seen the two procedures to get the results you must compare the outputs:\n",
    "<ul>\n",
    "     <li>   verify if the pandas dataframe from applyInPandas and PySpark functions are equivalents (look at the documentation to find the function that asserts if two DataFrames are equals) </li> \n",
    "         <li> compare the processing time between applyInPandas and PySpark routine with functions (that we have visualised with the %%time function) and comment them.</li> \n",
    " \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "4lngxwISVXp_"
   },
   "outputs": [],
   "source": [
    "# verify if the pandas dataframe from applyInPands and PySpark functions are equivalents \n",
    "# compare the processing time between applyInPandas and PySpark function\n",
    "cols = ['Mean_Vol', 'Std_Vol', 'Min_Vol', 'Max_Vol', 'Year', 'Month', 'Date']\n",
    " \n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "# equivalence verification: look at the pandas API and check if there is any test/assertion operation of help\n",
    "%%time\n",
    "\n",
    "# Write the command that will store in the variable stats_dfp the outoput DataFrame \n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "def compute_stats_1(key,df):\n",
    "    res = df[\"Volumn_BTC\"].describe()\n",
    "\n",
    "    res_dict = {}\n",
    "    for index, value in res.items():\n",
    "        if index == \"mean\":\n",
    "            res_dict[\"Mean_Vol\"] = value\n",
    "        elif index == \"std\":\n",
    "            res_dict[\"Std_Vol\"] = value\n",
    "        elif index == \"min\":\n",
    "            res_dict[\"Min_Vol\"] = value\n",
    "        elif index == \"max\":\n",
    "            res_dict[\"Max_Vol\"] = value\n",
    "\n",
    "    final =  pd.DataFrame([res_dict])\n",
    "    final['Year'] = df['Year']\n",
    "    final[\"Month\"]  = key\n",
    "    return final\n",
    "\n",
    "stats_dfp = dfs.groupby('Month').applyInPandas(compute_stats_1, \\\n",
    "            schema = \"Mean_Vol double, Std_Vol double, \\\n",
    "            Min_Vol double, Max_Vol double, Year int, Month int\")\n",
    "type(stats_dfp)\n",
    "\n",
    "\n",
    "# comment about time execution and draw your considerations\n",
    "#'''############## END OF THE EXERCISE ##############'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSw_cXcmVXqA"
   },
   "source": [
    "### Extra - Plotting the financial data (1 point)\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Now that you have seen some examples you can draw your graphs:\n",
    "<ul>\n",
    "     <li>   filter the global data frame fron Parquet and take only the first day of the year 2021 </li> \n",
    "         <li> convert it to a pandas dataframe </li> \n",
    "         <li>    display the data using the $plot_candlestick$ routine </li> \n",
    " \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "id": "R05figbfVXqA"
   },
   "outputs": [],
   "source": [
    "#this function helps you to display the candlestick ( representation of financial data) of the pandas dataframe\n",
    "\n",
    "def plot_candlestick(df):\n",
    "    trace = {\n",
    "      \"x\": df.Date_Time,\n",
    "      \"close\": dfp[\"Open\"],\n",
    "      \"decreasing\": {\"line\": {\"color\": \"#008000\"}}, \n",
    "      \"high\":df[\"High\"] ,\n",
    "      \"increasing\": {\"line\": {\"color\": \"#db4052\"}}, \n",
    "      \"low\": df[\"Low\"],\n",
    "      \"name\": \"BTC\", \n",
    "      \"open\": df[\"Close\"],\n",
    "      \"type\": \"candlestick\"\n",
    "    }\n",
    "\n",
    "    layout = {\n",
    "      \"height\":1000,\n",
    "      \"showlegend\": True, \n",
    "      \"title\": \"Technical Analysis\",\n",
    "    }\n",
    "    \n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "hu81JTxBVXqA",
    "outputId": "1f890e5a-38aa-4e8f-ef99-40129fbe1fe5"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-114-5a0045d8db59>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    df_spark =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Exercise filter the spark dataframe by date \n",
    "import datetime as dt\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "#read the global dataframe (as usual)\n",
    "df_spark = spark.read.parquet(\"BTC/\") \n",
    "\n",
    "#create the beginning end date \n",
    "beg = \n",
    "end = \n",
    "\n",
    "\n",
    "#create the filter\n",
    "df_spark_filtered = \n",
    "\n",
    "#apply and convert it to pandas\n",
    "dfp = \n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5VQE8mhPVXqB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_candlestick(dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxGOfykpVXqB"
   },
   "source": [
    "####  Extra - Propose your analysis\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Think about a new analysis on this set of data to run on your data and run it showing a graph\n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "id": "ewmVzLeuVXqB"
   },
   "outputs": [],
   "source": [
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "524h_jgeVXqB"
   },
   "source": [
    "### Conclusion \n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "$ApplyinPandas$ can be very powerful when you need to apply advanced Python code or Python libraries (i.e. <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a>  otherwise you can use Pyspark routines relying on most powerful storage techniques for example using Parquet.\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "eG3YxgAzVXqB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "99_WnUHTVXp5"
   ],
   "name": "ass_spark_dataframes_2021_sujet_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
