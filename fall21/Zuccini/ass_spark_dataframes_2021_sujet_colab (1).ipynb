{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ass_spark_dataframes_2021_sujet_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbuZfFWZVXpn"
      },
      "source": [
        "<h1><center>Big Data Algorithms Techniques & Platforms</center></h1>\n",
        "<h2>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Spark and DataFrames</center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBLI_qUSVXpq"
      },
      "source": [
        "## Objectives \n",
        "\n",
        "<strong> Dataframes: </strong>\n",
        "<ul>\n",
        "    <li>  Pyspark </li> \n",
        "    <li>  Pandas library on Spark</li> \n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42q0jiKKVXpr"
      },
      "source": [
        "# A. Context\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "For running this serie of exercises we are going to use a quite big dataset containing data on Bitcoin made available from <a href=\"https://www.kaggle.com/mczielinski/bitcoin-historical-data\">Kaggle</a>.\n",
        "\n",
        "As stated in the description of the dataset:\n",
        "\"Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary.\" \n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "### The dataset\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The dataset is in a .csv file:\n",
        "\n",
        "$bitstampUSD\\_1-min\\_data\\_2012-01-01\\_to\\_2021-03-31.csv$\n",
        "\n",
        "CSV files for select bitcoin exchanges for the time period of Jan 2012 to December March 2021, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. \n",
        "\n",
        "Notice that:\n",
        "<ul>\n",
        "    <li> Timestamps are in Unix time.</li>\n",
        "<li> Timestamps without any trades or activity have their data fields filled with NaNs. </li>\n",
        "<li>  If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforeseen technical error in data reporting or gathering. </li>\n",
        "</ul>\n",
        "As stated by the authors \"all effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk\".\n",
        "</p>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd5y1A2BVXpr"
      },
      "source": [
        "# B. Environment set-up\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "As first step you must include your dataset in your environment.\n",
        "\n",
        "You can folllow the procedure that includes Kaggle data into colab working folders or simply download and re-upload the file on your Colab space.\n",
        "\n",
        "\n",
        "$bitstampUSD\\_1-min\\_data\\_2012-01-01\\_to\\_2021-03-31.csv$\n",
        "    \n",
        "and upload it in the folder where your notebook is supposed to read the input.\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "As second step you must prepare your environment running the following two cells that:\n",
        "<ul>\n",
        "    <li> Import the Pandas library.</li>\n",
        "<li> Set the Spark environment and return a SparkSession (acting as was acting the SparkContext in the previous exercises). </li>\n",
        "</ul>    \n",
        "    \n",
        "\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVZaAkIgVXps"
      },
      "source": [
        "# import of Pandas library\n",
        "import pandas as pa\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTm3dI99NgTW",
        "outputId": "a1f72fbe-531d-4980-d932-cbb4eea35023"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!tar zxvf spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [73.0 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [808 kB]\n",
            "Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [666 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [699 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,812 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.8 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,439 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,430 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,867 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,219 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]\n",
            "Get:27 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [928 kB]\n",
            "Fetched 14.3 MB in 6s (2,367 kB/s)\n",
            "Reading package lists... Done\n",
            "spark-3.0.3-bin-hadoop2.7/\n",
            "spark-3.0.3-bin-hadoop2.7/NOTICE\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/tests/\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/tests/py_container_checks.py\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/tests/pyfiles.py\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.0.3-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-3.0.3-bin-hadoop2.7/jars/\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-vector-code-gen-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-beanutils-1.9.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/httpcore-4.4.12.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/scala-library-2.12.10.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/parquet-format-2.4.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/xercesImpl-2.12.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/okio-1.15.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-compiler-3.0.16.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spire-macros_2.12-0.17.0-M1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/arrow-memory-0.15.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/JLargeArrays-1.5.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/logging-interceptor-3.12.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-cli-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/parquet-common-1.10.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.10.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/algebra_2.12-2.0.0-M2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-graphx_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-annotations-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/kubernetes-client-4.9.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spire-util_2.12-0.17.0-M1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jakarta.activation-api-1.2.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-network-shuffle_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jline-2.14.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/breeze_2.12-1.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/metrics-jvm-4.1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-launcher_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/machinist_2.12-0.6.8.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/scala-compiler-2.12.10.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/arrow-format-0.15.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-mllib_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-yarn_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/metrics-graphite-4.1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-hdfs-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/py4j-0.10.9.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spire-platform_2.12-0.17.0-M1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/avro-1.8.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-core_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/curator-client-2.7.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/antlr4-runtime-4.7.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-kubernetes_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jersey-media-jaxb-2.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-lang3-3.9.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-auth-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/objenesis-2.5.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/paranamer-2.8.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/cats-kernel_2.12-2.0.0-M4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/flatbuffers-java-1.9.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jersey-server-2.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/stream-2.9.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/xml-apis-1.4.01.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-hive_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-jdbc-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-exec-2.3.7-core.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-sketch_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jsr305-3.0.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/univocity-parsers-2.9.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/macro-compat_2.12-1.1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jersey-container-servlet-core-2.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/orc-core-1.5.10.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hk2-api-2.6.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/chill-java-0.9.5.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-tags_2.12-3.0.3-tests.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/automaton-1.11-8.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-annotations-2.10.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-network-common_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-repl_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/velocity-1.5.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jul-to-slf4j-1.7.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/JTransforms-3.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/json4s-ast_2.12-3.6.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jersey-client-2.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/httpclient-4.5.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jetty-sslengine-6.1.26.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-kvstore_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spire_2.12-0.17.0-M1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jersey-hk2-2.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/xbean-asm7-shaded-4.15.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/snappy-java-1.1.8.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/slf4j-log4j12-1.7.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/zstd-jni-1.4.4-3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/slf4j-api-1.7.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-shims-0.23-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-storage-api-2.7.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/shapeless_2.12-2.3.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/kubernetes-model-common-4.9.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jersey-container-servlet-2.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-module-paranamer-2.10.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-module-scala_2.12-2.10.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-shims-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/json-1.8.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/threeten-extra-1.5.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-tags_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.10.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jersey-common-2.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/aircompressor-0.10.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/lz4-java-1.7.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-client-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/shims-0.7.45.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-crypto-1.1.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-mesos_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/libthrift-0.12.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/orc-mapreduce-1.5.10.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/HikariCP-2.5.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/generex-1.0.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/okhttp-3.12.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/breeze-macros_2.12-1.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/scala-xml_2.12-1.2.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-common-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-common-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/xz-1.5.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-shims-scheduler-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/chill_2.12-0.9.5.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-databind-2.10.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-hive-thriftserver_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/scala-reflect-2.12.10.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/joda-time-2.10.5.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/orc-shims-1.5.10.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-compress-1.20.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-core-2.10.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-net-3.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/janino-3.0.16.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-shims-common-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/transaction-api-1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-streaming_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jackson-datatype-jsr310-2.10.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-mllib-local_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/metrics-jmx-4.1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/arrow-vector-0.15.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-catalyst_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/curator-framework-2.7.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-metastore-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-beeline-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/metrics-core-4.1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-llap-common-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/netty-all-4.1.47.Final.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/javassist-3.25.0-GA.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/scala-collection-compat_2.12-2.1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/spark-sql_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/parquet-column-1.10.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/kubernetes-model-4.9.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/commons-text-1.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/json4s-scalap_2.12-3.6.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/metrics-json-4.1.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/snakeyaml-1.24.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/json4s-jackson_2.12-3.6.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.4.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/json4s-core_2.12-3.6.6.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/pyrolite-4.30.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/hive-serde-2.3.7.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/RoaringBitmap-0.7.45.jar\n",
            "spark-3.0.3-bin-hadoop2.7/jars/zookeeper-3.4.14.jar\n",
            "spark-3.0.3-bin-hadoop2.7/data/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/iris_libsvm.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/als/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/license.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/multi-channel/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/images/license.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/graphx/\n",
            "spark-3.0.3-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-3.0.3-bin-hadoop2.7/data/streaming/\n",
            "spark-3.0.3-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-3.0.3-bin-hadoop2.7/R/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-3.0.3-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.0.3-bin-hadoop2.7/README.md\n",
            "spark-3.0.3-bin-hadoop2.7/RELEASE\n",
            "spark-3.0.3-bin-hadoop2.7/yarn/\n",
            "spark-3.0.3-bin-hadoop2.7/yarn/spark-3.0.3-yarn-shuffle.jar\n",
            "spark-3.0.3-bin-hadoop2.7/LICENSE\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.0.3-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-3.0.3-bin-hadoop2.7/examples/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/people.csv\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/users.orc\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/dir1/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/dir1/file1.parquet\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/dir1/dir2/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/dir1/dir2/file2.parquet\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/dir1/file3.json\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scripts/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/fm_regressor_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/fm_classifier_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/arrow.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.0.3-bin-hadoop2.7/examples/jars/\n",
            "spark-3.0.3-bin-hadoop2.7/examples/jars/spark-examples_2.12-3.0.3.jar\n",
            "spark-3.0.3-bin-hadoop2.7/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.0.3-bin-hadoop2.7/conf/\n",
            "spark-3.0.3-bin-hadoop2.7/conf/slaves.template\n",
            "spark-3.0.3-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-3.0.3-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-3.0.3-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-3.0.3-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-3.0.3-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-3.0.3-bin-hadoop2.7/bin/\n",
            "spark-3.0.3-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/sparkR\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-submit\n",
            "spark-3.0.3-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-class\n",
            "spark-3.0.3-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-sql\n",
            "spark-3.0.3-bin-hadoop2.7/bin/docker-image-tool.sh\n",
            "spark-3.0.3-bin-hadoop2.7/bin/find-spark-home.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-3.0.3-bin-hadoop2.7/bin/pyspark\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/beeline\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-sql.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/run-example\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-shell\n",
            "spark-3.0.3-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/bin/spark-sql2.cmd\n",
            "spark-3.0.3-bin-hadoop2.7/python/\n",
            "spark-3.0.3-bin-hadoop2.7/python/.gitignore\n",
            "spark-3.0.3-bin-hadoop2.7/python/run-tests-with-coverage\n",
            "spark-3.0.3-bin-hadoop2.7/python/pylintrc\n",
            "spark-3.0.3-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-3.0.3-bin-hadoop2.7/python/README.md\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_coverage/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_coverage/coverage_daemon.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_coverage/conf/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_coverage/sitecustomize.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/run-tests.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/setup.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.0.3-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_worker.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_serializers.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_util.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_rdd.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_profiler.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_join.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_context.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_conf.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/tests/test_daemon.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/mlutils.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/mllibutils.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/utils.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/sqlutils.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/testing/streamingutils.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_util.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/functions.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/image.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/tree.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/heapq3.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/resource.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_grouped_map.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_map.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_streaming.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_window.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_typehints.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/functions.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/serializers.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/typehints.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/map_ops.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/types.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/group_ops.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/utils.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/udf.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/avro/\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/avro/functions.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/_globals.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/.coveragerc\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/index.rst\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/conf.py\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.rst\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/_templates/\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/_templates/layout.html\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/_static/\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/_static/copybutton.js\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-3.0.3-bin-hadoop2.7/python/docs/pyspark.resource.rst\n",
            "spark-3.0.3-bin-hadoop2.7/python/lib/\n",
            "spark-3.0.3-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip\n",
            "spark-3.0.3-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-3.0.3-bin-hadoop2.7/python/run-tests\n",
            "spark-3.0.3-bin-hadoop2.7/python/setup.cfg\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-respond.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-janino.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-datatables.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-CC0.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jodd.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-machinist.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-join.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-arpack.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-javassist.html\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-zstd.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-automaton.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-mustache.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-vis-timeline.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-re2j.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-3.0.3-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnIAJZrfVXps"
      },
      "source": [
        "import os\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-1.8.0-openjdk-amd64/\"\n",
        "#os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home\"\n",
        "\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
        "\n",
        "#os.environ[\"SPARK_HOME\"] = \"/Users/viceroy/Downloads/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/Users/viceroy/Downloads/spark-3.0.3-bin-hadoop2.7\" \n",
        "#os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.8/site-packages/pyspark/\"\n",
        "\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "#import of the SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#inizialization of the Spark Session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Assignment2\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdYKtVF_VXpt"
      },
      "source": [
        "## B.1  File import\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In this exercise the goal is to create a Spark DataFrame from the csv file in imput. \n",
        "\n",
        "Recall that in Spark DataFrame the type of the columns is very important for the definition of the internal data representation. \n",
        "    \n",
        "For this step you the target set of typed columns is the following one: \n",
        "<ul>\n",
        "    <li>    $Date\\_Time: Timestamp$ </li>\n",
        "     <li>   $Open: double$ </li>\n",
        "     <li>   $High: double$ </li>\n",
        "    <li>    $Low: double$ </li>\n",
        "    <li>    $Close: double$ </li>\n",
        "    <li>    $Volume\\_BTC: double$ </li>\n",
        "    <li>    $Volume\\_Currency: double$ </li>\n",
        "    <li>    $Weighted\\_Price: double$ </li>\n",
        "</ul>\n",
        "    \n",
        "We will arrive to define the schema in 3 guided steps described in the following sections.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Notice that the header of the $csv$ file contains the data description and that the simple import of the\n",
        "file treats the timestamp column as a String. \n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In data import you must check that:\n",
        "<ul>\n",
        "    <li>  the types of the imported data (the ones read from the file using the operation you choose) are equal to the types in the given schema</li>\n",
        "    <li>  the names of columns correspond (and make transofrmations if necessary). </li> \n",
        "</ul>\n",
        "    \n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQZN4xiqVXpt"
      },
      "source": [
        "### <strong> Exercise 1.</strong> First import (1 point)\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Import the csv file in Spark DataFrame. If you have any doubt you can always refer to the Spark 3.1.1 documentation:\n",
        "\n",
        "<a href=\"https://spark.apache.org/docs/3.1.1/\">Spark Reference Documentation</a>\n",
        "\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imMeOHQmVXpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89208d9f-a31b-46eb-fc3a-08db68598d13"
      },
      "source": [
        "# Write the command that creates (reads) a Spark DataFrame and stores the reference in the dfs variable\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
        "#'''############## WRITE YOUR CODE HERE ##############'''\n",
        "schema = StructType([\n",
        "    StructField(\"Timestamp\",IntegerType(), True),\n",
        "    StructField(\"Open\",DoubleType(), True),\n",
        "    StructField(\"High\",DoubleType(), True),\n",
        "    StructField(\"Low\",DoubleType(), True),\n",
        "    StructField(\"Close\",DoubleType(), True),\n",
        "    StructField(\"Volume_BTC\",DoubleType(), True),\n",
        "    StructField(\"Volume_Currency\",DoubleType(), True),\n",
        "    StructField(\"Weighted_Price\",DoubleType(), True)\n",
        "])\n",
        "\n",
        "dfs = spark.read.csv(\"bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv\", schema=schema, header=True)\n",
        "dfs.printSchema()\n",
        "\n",
        "#'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "#show the DataFrame schema\n",
        "dfs\n",
        "#######################\n",
        "# EXPECTED OUTPUT:\n",
        "# DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, Volume_(BTC): double, Volume_(Currency): double, Weighted_Price: double]</font>\n",
        "#\n",
        "# Notice that if you have something like:\n",
        "# DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]\n",
        "# you forgot a step: you did not include the schema of the columns\n",
        "#\n",
        "# Notice also that if you have:\n",
        "# DataFrame[Timestamp: string, Open: string, High: string, Low: string, Close: string, Volume_(BTC): string, Volume_(Currency): string, Weighted_Price: string]\n",
        "# you also forgot a step: the type of the Timestamp must be a String\n",
        "###########################"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Timestamp: integer (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume_BTC: double (nullable = true)\n",
            " |-- Volume_Currency: double (nullable = true)\n",
            " |-- Weighted_Price: double (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, Volume_BTC: double, Volume_Currency: double, Weighted_Price: double]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTbhA1Aip4mM",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fabf19-028a-4f86-998d-97c45caf0577"
      },
      "source": [
        "# the following command is going to show 5 rows of the DataFrame\n",
        "dfs.take(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Timestamp=1325317920, Open=4.39, High=4.39, Low=4.39, Close=4.39, Volume_BTC=0.45558087, Volume_Currency=2.0000000193, Weighted_Price=4.39),\n",
              " Row(Timestamp=1325317980, Open=nan, High=nan, Low=nan, Close=nan, Volume_BTC=nan, Volume_Currency=nan, Weighted_Price=nan),\n",
              " Row(Timestamp=1325318040, Open=nan, High=nan, Low=nan, Close=nan, Volume_BTC=nan, Volume_Currency=nan, Weighted_Price=nan),\n",
              " Row(Timestamp=1325318100, Open=nan, High=nan, Low=nan, Close=nan, Volume_BTC=nan, Volume_Currency=nan, Weighted_Price=nan),\n",
              " Row(Timestamp=1325318160, Open=nan, High=nan, Low=nan, Close=nan, Volume_BTC=nan, Volume_Currency=nan, Weighted_Price=nan)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1_anEQzNUFG",
        "outputId": "bf1c3baa-7496-4c86-c0a8-fafe7d7b8273"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "165"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkQ8aYX-VXpv"
      },
      "source": [
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Look again at the target schema:\n",
        "    \n",
        "<ul>\n",
        "    <li>    $Date\\_Time: Timestamp$ </li>\n",
        "     <li>   $Open: double$ </li>\n",
        "     <li>   $High: double$ </li>\n",
        "    <li>    $Low: double$ </li>\n",
        "    <li>    $Close: double$ </li>\n",
        "    <li>    $Volume\\_BTC: double$ </li>\n",
        "    <li>    $Volume\\_Currency: double$ </li>\n",
        "    <li>    $Weighted\\_Price: double$ </li>\n",
        "</ul>\n",
        "    \n",
        "You notice that the import data has three problems with respect to the target schema:\n",
        "    \n",
        "    \n",
        "<ul>\n",
        "    <li> the $Date\\_Time$ column is not present in the original file </li>\n",
        "    <li> there is an $int$ column $Timestamp$ that can be converted and transformed to a $Date$</li> \n",
        "    <li> some of the column names contain not required parentesis. </li>\n",
        "</ul>     \n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUS9JWIcVXpv"
      },
      "source": [
        "### <strong> Exercise 2. </strong> Timestamp column (1 point)\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Refine the import of the csv file and convert the \"timestamp\" column in the proper $Timestamp$ type:\n",
        "    <ul>\n",
        "        <li>   Create a new column <code>Date\\_Time</code> that is the conversion of the $String$ column $Timestamp$ in $Timestamp$ type  </li>\n",
        "</ul>\n",
        "The Dataframe are immutable structure, then your procedure will use a command (discussed in the slides) that will create a new Spark $DataFrame$ from the $dfs$ $DataFrame$ having a different schema. \n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Look at the timestamp column of the csv file and from the imported DataFrame \n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj8BeALtVXpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec104048-65b8-45cc-a482-406fd1ab6962"
      },
      "source": [
        "# write the command that creates a new Data Frame Spark with Date_Time column\n",
        "# and stores the reference in the dfsdt variable (it must be a DataFrame Spark with Date_Time column)\n",
        "\n",
        "#'''############## WRITE YOUR CODE HERE ##############'''\n",
        "from pyspark.sql.types import TimestampType\n",
        "\n",
        "dfsdt = dfs.withColumn(\"Date_Time\", dfs[\"Timestamp\"].cast(TimestampType()))\n",
        "\n",
        "#'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "#show the DataFrame schema\n",
        "dfsdt\n",
        "\n",
        "#######################\n",
        "# EXPECTED OUTPUT:\n",
        "# DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, \n",
        "#Volume_(BTC): double, Volume_(Currency): double, Weighted_Price: double, Date_Time: timestamp]\n",
        "#######################"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, Volume_BTC: double, Volume_Currency: double, Weighted_Price: double, Date_Time: timestamp]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBcoKytNVXpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2025fb91-7b90-41dc-9f3d-8b5e3e2edbd0"
      },
      "source": [
        "#show 5 rows of the DataFrame\n",
        "dfsdt.take(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Timestamp=1325317920, Open=4.39, High=4.39, Low=4.39, Close=4.39, Volume_BTC=0.45558087, Volume_Currency=2.0000000193, Weighted_Price=4.39, Date_Time=datetime.datetime(2011, 12, 31, 7, 52)),\n",
              " Row(Timestamp=1325317980, Open=nan, High=nan, Low=nan, Close=nan, Volume_BTC=nan, Volume_Currency=nan, Weighted_Price=nan, Date_Time=datetime.datetime(2011, 12, 31, 7, 53)),\n",
              " Row(Timestamp=1325318040, Open=nan, High=nan, Low=nan, Close=nan, Volume_BTC=nan, Volume_Currency=nan, Weighted_Price=nan, Date_Time=datetime.datetime(2011, 12, 31, 7, 54)),\n",
              " Row(Timestamp=1325318100, Open=nan, High=nan, Low=nan, Close=nan, Volume_BTC=nan, Volume_Currency=nan, Weighted_Price=nan, Date_Time=datetime.datetime(2011, 12, 31, 7, 55)),\n",
              " Row(Timestamp=1325318160, Open=nan, High=nan, Low=nan, Close=nan, Volume_BTC=nan, Volume_Currency=nan, Weighted_Price=nan, Date_Time=datetime.datetime(2011, 12, 31, 7, 56))]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhArt3zLNUFI",
        "outputId": "d6d94794-d519-4348-f1eb-e412df83b5de"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWEJTFojVXpw"
      },
      "source": [
        "### <strong> Exercise 3.</strong> Column names (2 points)\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "As you can see from the output of the previous exercise the names of the columns still present some problems since there are some parentesis that are not required.\n",
        "    <ul>\n",
        "     <li> Remove the not required parentesis from the colum names </li>\n",
        "     <li> Hint: look at the documentation of DataFrame API and check the operation for column renaming </li>\n",
        "</ul>\n",
        "</font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRv_VbgYVXpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc131c7c-de58-4cda-b889-821840f08bf8"
      },
      "source": [
        "# write the command that creates a new Data Frame Spark with correct names for all the columns\n",
        "# and store the reference in the dfscr variable (Data Frame Spark with Correct Names)\n",
        "\n",
        "#'''############## WRITE YOUR CODE HERE ##############'''\n",
        "from pyspark.sql.functions import from_unixtime\n",
        "dfscr = dfsdt.withColumn(\"Date_Time\", \n",
        "                         from_unixtime('TimeStamp',  format='yyyy-MM-dd HH:mm:ss'))\n",
        "\n",
        "#'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "#show the DataFrame schema\n",
        "dfscr\n",
        "\n",
        "#######################\n",
        "# EXPECTED OUTPUT:\n",
        "#DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, \n",
        "#          Volume_BTC: double, Volume_Currency: double, Weighted_Price: double]\n",
        "#######################"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, Volume_BTC: double, Volume_Currency: double, Weighted_Price: double, Date_Time: string]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU2SFo0UVXpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4efebdc9-8127-4165-9987-04fd3d34193c"
      },
      "source": [
        "#show 5 rows of the DataFrame\n",
        "dfscr.show(5)\n",
        "\n",
        "#######################\n",
        "# Expected output:\n",
        "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
        "#| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|\n",
        "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
        "#|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|\n",
        "#|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|\n",
        "#|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|\n",
        "#|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|\n",
        "#|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|\n",
        "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
            "| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|\n",
            "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
            "|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|\n",
            "|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|\n",
            "|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|\n",
            "|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|\n",
            "|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|\n",
            "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw7ktEJKVXpx"
      },
      "source": [
        "## B.2 DataFrame columns \n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "    \n",
        "In this part of the exercise we are going continue to  modify in the Spark DataFrames.\n",
        "\n",
        "    \n",
        "Remember that using  PySpark, it's possible to access a DataFrame's columns either by attribute (<code>df.attributeName</code>) or by indexing <code>(df['attributeName'])</code>.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "    \n",
        "Loook at the list of the functions to get familiar with the documentation: some functions that can be of help to manipulate the schema:\n",
        "    \n",
        "<ul>\n",
        "     <li>    <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\">Spark Functions</a>.  </li>\n",
        "</ul>    \n",
        "    \n",
        "    \n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBgUaY_oVXpx"
      },
      "source": [
        "### <strong> Exercise 4.</strong>  Add two new columns to the DataFrame (2 points)\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to extend the DataFrame with two other columns: given the $Date\\_Time$ column create two new columns ($Year$ and $Month$) that contain \n",
        "    <ul>\n",
        "     <li> the year </li>\n",
        "     <li> the month of the year </li>\n",
        "</ul>\n",
        "    \n",
        "</font>\n",
        "</p>\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDH7YsSeVXpx"
      },
      "source": [
        "<p align=\"justify\">\n",
        "<font size=\"3\">    \n",
        "Look at the documentation of Spark functions and find the two functions that are convenient for this use case (hint: the name of the columns can help: <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\">Spark Functions</a>)\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpL3519cVXpx",
        "scrolled": true
      },
      "source": [
        "#import the functions that you will use\n",
        "\n",
        "############## WRITE YOUR CODE HERE ##############\n",
        "############## END OF THE EXERCISE ##############"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsehHsySVXpz",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dfd2523-e3a9-4a49-aecc-ca4e1fd45472"
      },
      "source": [
        "# write the command that creates a new Data Frame Spark with the two additional columns\n",
        "# and store the reference in the dfsym variable (Data Frame Spark with Correct Names)\n",
        "\n",
        "#'''############## WRITE YOUR CODE HERE ##############'''\n",
        "from pyspark.sql.functions import year, month\n",
        "dfsym = dfscr.withColumn(\"Year\", year('Date_Time')) \n",
        "dfsym = dfsym.withColumn(\"Month\", month('Date_Time'))\n",
        "\n",
        "#'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "\n",
        "dfsym.show(5)\n",
        "\n",
        "#######################\n",
        "# Expected output:\n",
        "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
        "#| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
        "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
        "#|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
        "#|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
        "#|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
        "#|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
        "#|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
        "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
            "| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
            "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
            "|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
            "|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
            "|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
            "|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
            "|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
            "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhYd9R6lNUFL",
        "outputId": "74b05038-415e-4fe6-ad80-e4fec52d8ea1"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "314"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDMJbhTbVXpz"
      },
      "source": [
        "###  <strong>Exercise 5.</strong>  Drop Timestamp (2 points)\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Finally we clean the schema and we can remove the the $Timestamp$ column.\n",
        "</font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm8I1zhzVXp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80bd245-8d43-4cb7-e9e0-75ba1b17d47b"
      },
      "source": [
        "# write the command that creates a new DataFrame Spark from the dfsym without the Timestamp column\n",
        "# and store the reference in the dfc variable (Data Frame Spark Clean)\n",
        "#'''############## WRITE YOUR CODE HERE ##############'''\n",
        "dfsc = dfsym.drop(\"TimeStamp\")\n",
        "#'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "\n",
        "dfsc.show(5)\n",
        "\n",
        "#######################\n",
        "# Expected output:\n",
        "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
        "#|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
        "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
        "#|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
        "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
        "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
        "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
        "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
        "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
            "|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
            "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
            "|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
            "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
            "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
            "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
            "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
            "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dm-ovIcVXp0"
      },
      "source": [
        "#  C. Using Parquet\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In order to gain in performance in the following it is a good idea, as we have seen at lesson, to use a NoSQL structure, here Parquet, that will \n",
        "    allow \n",
        "to partition the SparkDataframe and to store it in multiple Parquet files. \n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WicAuWMfA9xo"
      },
      "source": [
        "## C.1 Saving data in Parquet\n",
        "    \n",
        "For this first example partition the file according to:\n",
        "    \n",
        " <ul>\n",
        "     <li> the year </li>\n",
        "             <li> the month of the year </li>\n",
        "</ul>\n",
        "The $partitionBy()$ operation can help for this step (Documentation of reference: <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\">Spark Functions</a>).\n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84UNDFHiVXp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1831e1-9e08-4552-9269-497fa46989f4"
      },
      "source": [
        "dfs"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, Volume_BTC: double, Volume_Currency: double, Weighted_Price: double]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTR8AxS2VXp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6a5388-3e0a-49a4-8809-672e6e898760"
      },
      "source": [
        "# here you can see and check the command that saves the dfsc DataFrame in Parquet\n",
        "\n",
        "dfsc.write.partitionBy([\"Year\", \"Month\"]).parquet(\"BTC/\",mode='overwrite')\n",
        "\n",
        "\n",
        "print(\"write to Parquet done\")\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "write to Parquet done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic-eLcr0VXp0"
      },
      "source": [
        "##  C.2 Check the folder Structure\n",
        "\n",
        " \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Look at the folder structure that has been created for the storage of the file. You see how the partitioning stategy of Parquet and the data distribution of Spark can be used, explicitely or implicitely, to improve performance.\n",
        "\n",
        "While you navigate (and the folder structure) data remember that in the data access:\n",
        "    \n",
        " <ul>\n",
        "     <li> the navigation is done using Parquet </li>\n",
        "     <li> the leaf contain the encoded Parquet files </li>\n",
        "</ul>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC1C75uIVXp1"
      },
      "source": [
        "#BTC\n",
        "#         Year=2011\n",
        "#            ...\n",
        "#           \n",
        "#            month=12\n",
        "#         Year=2012\n",
        "#            month=1\n",
        "#            ...\n",
        "#           \n",
        "#       ...\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JeyD03ipig_"
      },
      "source": [
        "This folder structure correspond to a phisical and logical data partition and "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMCbaNcPVXp1"
      },
      "source": [
        "# D. Pandas\n",
        "\n",
        " \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "This data organization opens the opportunity to read data also using Pandas and not using Parquet.\n",
        "    \n",
        "Look at the documentation and check how you can read a Parquet structure and store it in a Pandas DataFrame:\n",
        "<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html\">Pandas and Parquet</a>\n",
        "\n",
        "Notice how at the data-exchange base there is the presence of Arrow (thanks to $pyarrow$).\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Write the command that using Pandas read the data for the year 2011.\n",
        "    \n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B11RsK9YVXp1"
      },
      "source": [
        "#import of pandas\n",
        "import pandas as pa"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlRYuW0hVXp1",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "551a91f5-a915-44b9-ba24-fff0b54c8173"
      },
      "source": [
        "# Here we show you how we can create DataFrame using Pandas functions and reading from Parquet the data only for the year 2011/\n",
        "\n",
        "\n",
        "df = pa.read_parquet(\"BTC/Year=2011\")\n",
        "\n",
        "df\n",
        "#######################\n",
        "# Check the expected output:\n",
        "#Open\tHigh\tLow\tClose\tVolume_BTC\tVolume_Currency\tWeighted_Price\tDate_Time\tMonth\n",
        "#0\t4.39\t4.39\t4.39\t4.39\t0.455581\t2.0\t4.39\t2011-12-31 07:52:00\t12\n",
        "#1\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:53:00\t12\n",
        "#2\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:54:00\t12\n",
        "#3\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:55:00\t12\n",
        "#..."
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume_BTC</th>\n",
              "      <th>Volume_Currency</th>\n",
              "      <th>Weighted_Price</th>\n",
              "      <th>Date_Time</th>\n",
              "      <th>Month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0.455581</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.39</td>\n",
              "      <td>2011-12-31 07:52:00</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011-12-31 07:53:00</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011-12-31 07:54:00</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011-12-31 07:55:00</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011-12-31 07:56:00</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011-12-31 23:55:00</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>964</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011-12-31 23:56:00</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>965</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011-12-31 23:57:00</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>966</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011-12-31 23:58:00</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>967</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011-12-31 23:59:00</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>968 rows  9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Open  High   Low  ...  Weighted_Price            Date_Time  Month\n",
              "0    4.39  4.39  4.39  ...            4.39  2011-12-31 07:52:00     12\n",
              "1     NaN   NaN   NaN  ...             NaN  2011-12-31 07:53:00     12\n",
              "2     NaN   NaN   NaN  ...             NaN  2011-12-31 07:54:00     12\n",
              "3     NaN   NaN   NaN  ...             NaN  2011-12-31 07:55:00     12\n",
              "4     NaN   NaN   NaN  ...             NaN  2011-12-31 07:56:00     12\n",
              "..    ...   ...   ...  ...             ...                  ...    ...\n",
              "963   NaN   NaN   NaN  ...             NaN  2011-12-31 23:55:00     12\n",
              "964   NaN   NaN   NaN  ...             NaN  2011-12-31 23:56:00     12\n",
              "965   NaN   NaN   NaN  ...             NaN  2011-12-31 23:57:00     12\n",
              "966   NaN   NaN   NaN  ...             NaN  2011-12-31 23:58:00     12\n",
              "967   NaN   NaN   NaN  ...             NaN  2011-12-31 23:59:00     12\n",
              "\n",
              "[968 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAYamKaEVXp1"
      },
      "source": [
        "###  D.1 Read Parquet file\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Here you can see now the the Spark DataFrame is created from Parquet data.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C0iA8SqVXp2",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3382800b-a9ff-4865-d70b-7d6cbb2459cc"
      },
      "source": [
        "# And here how we can create a DataFrame using Spark and reading the whole data/\n",
        "\n",
        "dfs = spark.read.parquet(\"BTC/\")\n",
        "\n",
        "print(\"read done\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XMqN7rcNUFN",
        "outputId": "dffac07c-4aa5-45a8-ba0f-71e6ca1ebfce"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjH0cqC5VXp2"
      },
      "source": [
        "## <strong>Exercise 6</strong>. Verify number of column and count the number of rows (2 points)\n",
        "    \n",
        "<p align=\"justify\">\n",
        "Maybe you have not noticed that the volume of data we are treating is not so small as it seems. \n",
        "Count how many rows we are manipulating in the dataframe <code>dfs</code>\n",
        "<font size=\"3\">\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYqK9VD8VXp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "539b8832-aa97-48da-d230-5fe285b6e0b1"
      },
      "source": [
        "# Write the command that returns the number of rows of the DataFrame\n",
        "\n",
        "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
        "count = dfs.count()\n",
        "#'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "print(count)\n",
        "\n",
        "#######################\n",
        "# Expected output:\n",
        "# 4857377"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "187124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJVbEunCVXp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d1baa7-be4a-4398-f708-e5bda6e306d8"
      },
      "source": [
        "#We can also check and verify the schema of the DataFrame\n",
        "dfs.printSchema()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume_BTC: double (nullable = true)\n",
            " |-- Volume_Currency: double (nullable = true)\n",
            " |-- Weighted_Price: double (nullable = true)\n",
            " |-- Date_Time: string (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Month: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoAfjdAuVXp3"
      },
      "source": [
        "# E. Statistics\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to calculate the statistics of the bitcoin by month for all the years.\n",
        "\n",
        "The computed statistics will be stored in a DataFrame having this schema\n",
        "<ul>\n",
        "     <li>   Mean_Vol  : double </li>\n",
        "     <li>   Std_Vol   : double </li>\n",
        "     <li>   Min_Vol   : double </li>\n",
        "     <li>   Max_vol   : double </li>\n",
        "     <li>   Year      : int </li>\n",
        "     <li>   Month     : int </li>\n",
        "  \n",
        "</ul>\n",
        "\n",
        "In this exercise you will have two develop different methodologies to compute the statistics:\n",
        "<ul>\n",
        "    <li>   using the <code>applyInPandas()</code> Pyspark function and the Pandas functions </li>\n",
        "     <li>  only using the Pyspark functionnalities </li>\n",
        "</ul>\n",
        "The statistics computed should be stored in a Pandas DataFrame with both the two approaches.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gts_hy5HVXp4"
      },
      "source": [
        "## E.1. Spark applyinPandas\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The solution with $applyinPandas$ \n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icjKlPHcVXp4"
      },
      "source": [
        "# the Python function that must be used. \n",
        "def compute_stats(key,df):\n",
        "    res = df[\"Volume_BTC\"].describe()\n",
        "\n",
        "    res_dict = {}\n",
        "    for index, value in res.items():\n",
        "\n",
        "        if index == \"mean\":\n",
        "            res_dict[\"Mean_Vol\"] = value\n",
        "        elif index == \"std\":\n",
        "            res_dict[\"Std_Vol\"] = value\n",
        "        elif index == \"min\":\n",
        "            res_dict[\"Min_Vol\"] = value\n",
        "        elif index == \"max\":\n",
        "            res_dict[\"Max_Vol\"] = value\n",
        "\n",
        "    final =  pa.DataFrame([res_dict])\n",
        "    final[\"Year\"]  = key[0]\n",
        "    final[\"Month\"] = key[1]\n",
        "    \n",
        "    return final"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsqKcxxNVXp4"
      },
      "source": [
        "### <strong>Exercise 7</strong>. The two parameters of the Python function (2 points)\n",
        "The two parameters of the Python <code>applyinPandas(funct,schema)</code> function (2 points)\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "    Look at the documentation of the <code>applyinPandas(funct,schema)</code> (<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.applyInPandas.html\">click here to go to the documentation of <code>applyinpandas</code></a>) and describe how it works in detail from the DataFrame point of view in our example (what the $key$ and the $df$ will contain in our example).\n",
        "\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99_WnUHTVXp5"
      },
      "source": [
        "#### apply in pandas takes a pandas.DataFrame and return another pandas.DataFrame. For each group, year and month, all columns are passed together as a pandas.DataFrame to the compute stats function and the returned pandas.DataFrame are combined as a DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shVh5lBcVXp5"
      },
      "source": [
        "### <strong>Exercise 8</strong>. The two parameters in action (1 point)\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Compute the statistics using then the $applyInPandas$ and the provided functions. \n",
        "\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IebDul7NVXp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abdbc63-ced5-4c09-f39b-2f764d4c8a05"
      },
      "source": [
        "schema = \"Mean_Vol double, Std_Vol double, Min_Vol double, Max_Vol double, Year int, Month int\"\n",
        "\n",
        "# Write the command that will store in the variable statsdf the DataFrame \n",
        "#from pyspark.sql import applyInPandas\n",
        "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
        "statsdf = dfs.groupby('Year','Month').applyInPandas(compute_stats, schema = schema) \n",
        "\n",
        "#'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "statsdf.show(5)\n",
        "\n",
        "\n",
        "####### EXPECTED OUTPUT\n",
        "#+------------------+------------------+----------+------------+----+-----+\n",
        "#|          Mean_Vol|           Std_Vol|   Min_Vol|     Max_Vol|Year|Month|\n",
        "#+------------------+------------------+----------+------------+----+-----+\n",
        "#| 20.39613620802532| 54.24699556644988|    9.4E-5|2258.8231405|2012|   10|\n",
        "#|12.095179597807542|44.149334198665166| 2.0452E-4|2037.2239038|2015|    2|\n",
        "#| 6.147061206279663|17.745599117954125|0.00127783|564.21436237|2019|   10|\n",
        "#| 8.468866447160776|  28.9837002907642|    1.0E-8|1616.0600006|2017|    3|\n",
        "#| 8.684880075589284| 17.69646210434965|       0.0|533.10078293|2017|    8|\n",
        "#+------------------+------------------+----------+------------+----+-----+#"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+------------------+----------+------------+----+-----+\n",
            "|         Mean_Vol|           Std_Vol|   Min_Vol|     Max_Vol|Year|Month|\n",
            "+-----------------+------------------+----------+------------+----+-----+\n",
            "|21.68391320646051| 36.39353845336437|0.00127551|300.51609759|2012|    4|\n",
            "|     23.829469525|22.711132615895384|0.45558087|        48.0|2011|   12|\n",
            "| 4.03177731714571| 6.740555326924376|      0.02| 43.31219578|2012|    1|\n",
            "|8.313992791715775|11.924511042377938|0.00313838| 92.65487394|2012|    2|\n",
            "|15.19779132870153|26.505989960881884|0.00209644|247.56012381|2012|    3|\n",
            "+-----------------+------------------+----------+------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlwS5VdINUFO",
        "outputId": "3a5b60b1-bcd5-4fd5-cbea-6e70d24b26a0"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Pdf1N-VXp6"
      },
      "source": [
        "### <strong>Exercise 9</strong>. The statsdf DataFrame (1 point)\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Which kind of DataFrame is statsdf?\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rSwtoDjNUFO",
        "outputId": "c25d4b40-3177-47d1-803c-b2dff67a5fd9"
      },
      "source": [
        "type(statsdf)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbXKRuBEVXp6"
      },
      "source": [
        "#### pyspark.sql.dataframe.DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgGmWPhSVXp7"
      },
      "source": [
        "\n",
        "### <strong>Exercise 10</strong>. DataFrame in Pandas (2 points)\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Since we computed a stat by month the results will be small (we will have only one row by month)\n",
        "we can get and handle all the results in memory in Pandas.  \n",
        "    \n",
        "Notice that Spark is lazy so the $toPandas$ action will trigger the computation.\n",
        "    \n",
        "Write the command that will do the operation.\n",
        "    \n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wupi4CTgVXp7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc8668d-3f9f-49de-f657-7e68a5eac614"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Write the command that will store in the variable stats_dfp the outoput DataFrame \n",
        "\n",
        "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
        "stats_dfp = statsdf.toPandas()\n",
        "# stats_dfp = dfs.groupby('Month').applyInPandas(compute_stats_1, \n",
        "#             schema = schema).toPandas()\n",
        "#'''############## END OF THE EXERCISE ##############'''"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 40.6 ms, sys: 7.65 ms, total: 48.3 ms\n",
            "Wall time: 1.75 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve6u_nzSVXp8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "885f35f8-2560-467a-f721-43478153bd9f"
      },
      "source": [
        "#results\n",
        "stats_dfp.head(10)\n",
        "\n",
        "#######################\n",
        "# Expected output:\n",
        "#\tMean_Vol\tStd_Vol\tMin_Vol\tMax_Vol\tYear\tMonth\n",
        "#0\t20.396136\t54.246996\t9.400000e-05\t2258.823141\t2012\t10\n",
        "#1\t12.095180\t44.149334\t2.045200e-04\t2037.223904\t2015\t2\n",
        "#2\t6.147061\t17.745599\t1.277830e-03\t564.214362\t2019\t10\n",
        "#3\t8.468866\t28.983700\t1.000000e-08\t1616.060001\t2017\t3\n",
        "#4\t8.684880\t17.696462\t0.000000e+00\t533.100783\t2017\t8\n",
        "#5\t16.040933\t57.641501\t2.044000e-05\t4111.876106\t2014\t4\n",
        "#6\t4.984386\t18.903445\t1.054000e-05\t822.866974\t2020\t6\n",
        "#7\t8.331579\t18.350084\t5.758000e-04\t806.636224\t2019\t5\n",
        "#8\t8.621910\t18.820399\t4.047000e-04\t602.282607\t2017\t10\n",
        "#9\t3.106413\t10.738051\t3.300000e-06\t582.564185\t2018\t10"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean_Vol</th>\n",
              "      <th>Std_Vol</th>\n",
              "      <th>Min_Vol</th>\n",
              "      <th>Max_Vol</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>21.683913</td>\n",
              "      <td>36.393538</td>\n",
              "      <td>0.001276</td>\n",
              "      <td>300.516098</td>\n",
              "      <td>2012</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23.829470</td>\n",
              "      <td>22.711133</td>\n",
              "      <td>0.455581</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>2011</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.031777</td>\n",
              "      <td>6.740555</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>43.312196</td>\n",
              "      <td>2012</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.313993</td>\n",
              "      <td>11.924511</td>\n",
              "      <td>0.003138</td>\n",
              "      <td>92.654874</td>\n",
              "      <td>2012</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15.197791</td>\n",
              "      <td>26.505990</td>\n",
              "      <td>0.002096</td>\n",
              "      <td>247.560124</td>\n",
              "      <td>2012</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21.867528</td>\n",
              "      <td>38.525832</td>\n",
              "      <td>0.007482</td>\n",
              "      <td>384.988000</td>\n",
              "      <td>2012</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Mean_Vol    Std_Vol   Min_Vol     Max_Vol  Year  Month\n",
              "0  21.683913  36.393538  0.001276  300.516098  2012      4\n",
              "1  23.829470  22.711133  0.455581   48.000000  2011     12\n",
              "2   4.031777   6.740555  0.020000   43.312196  2012      1\n",
              "3   8.313993  11.924511  0.003138   92.654874  2012      2\n",
              "4  15.197791  26.505990  0.002096  247.560124  2012      3\n",
              "5  21.867528  38.525832  0.007482  384.988000  2012      5"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ffppQnaNUFP",
        "outputId": "5a6f619f-91e9-496e-e1aa-b66438fce565"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX4X2ioqVXp8"
      },
      "source": [
        "###  <strong>Exercise 11</strong>. Show the stats of the stats (1 point)\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to calculate the statistics of the bitcoin by month for all the years.\n",
        "\n",
        "The computed statistics will be stored in a DataFrame having this schema\n",
        "<ul>\n",
        "     <li>   the min of the set min values </li>\n",
        "     <li>   the mean of the set of mean values </li>\n",
        "     <li>   ... </li> \n",
        "</ul>\n",
        "\n",
        "\n",
        "    \n",
        "</font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FFQDxOyVXp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745dccea-2bd4-4ddd-882a-9fa3a8baa0b2"
      },
      "source": [
        "# Write the command that will show and compute the stats on the numerical columns of the statsdf DataFrame\n",
        "\n",
        "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
        "\n",
        "statsdf.describe()\n",
        "#'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "\n",
        "\n",
        "#######################\n",
        "# Expected output:\n",
        "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
        "#|summary|          Mean_Vol|           Std_Vol|             Min_Vol|           Max_Vol|              Year|             Month|\n",
        "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
        "#|  count|               112|               112|                 112|               112|               112|               112|\n",
        "#|   mean|10.782191354847754|28.871463944232485|0.004551177678571...|1067.2847720235718|2016.0892857142858| 6.428571428571429|\n",
        "#| stddev| 6.488551661205522| 18.11344145463867|0.043048607639448476| 895.8083462469303|2.7164947320662614|3.5252353718985097|\n",
        "#|    min| 2.929999689326444| 6.490701567379118|                 0.0|       43.31219578|              2011|                 1|\n",
        "#|    max|31.504423573146152|106.97606692383131|          0.45558087|      5853.8521659|              2021|                12|\n",
        "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, Mean_Vol: string, Std_Vol: string, Min_Vol: string, Max_Vol: string, Year: string, Month: string]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wtYR3CaVXp8"
      },
      "source": [
        "# F. Plotting and equivalence \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to plot the resutls of the statistics by year and month (that will be in the $x$ orizontal axis of the plot). \n",
        "\n",
        "$Plotly$ will be used for the plotting\n",
        "    \n",
        "\n",
        "This provided version of the code is fully working in Python.\n",
        "    \n",
        "\n",
        "A Python routine converts the two columns $Year$ and $Month$ into a $DateTime$ column 'Date' (in order to plot the data in relation with the date).\n",
        "    \n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E1eQmpJVXp9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5cc4d39e-03f7-4252-e4d1-990684befcf1"
      },
      "source": [
        "#install plotly and import the libraries\n",
        "\n",
        "!pip install plotly\n",
        "\n",
        "from plotly.offline import iplot,init_notebook_mode\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "init_notebook_mode(connected=True)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly) (1.3.3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlyn4HLZVXp9"
      },
      "source": [
        "#Helper function that converts the Year Month of our data into Date type\n",
        "\n",
        "def get_date_from_year_month(df):\n",
        "    df[\"Date\"] = pa.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%m')\n",
        "    return df\n",
        "     "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS9hH6hHVXp9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "1d306965-d61f-45ec-f6a1-4f3c3259ce4d"
      },
      "source": [
        "# In this phase we need to sort by the date to allow parallelisation of shuffled the results\n",
        "\n",
        "stats_dfp = get_date_from_year_month(stats_dfp)    \n",
        "stats_dfp.sort_values(by = 'Date',inplace = True)\n",
        "stats_dfp"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean_Vol</th>\n",
              "      <th>Std_Vol</th>\n",
              "      <th>Min_Vol</th>\n",
              "      <th>Max_Vol</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23.829470</td>\n",
              "      <td>22.711133</td>\n",
              "      <td>0.455581</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>2011</td>\n",
              "      <td>12</td>\n",
              "      <td>2011-12-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.031777</td>\n",
              "      <td>6.740555</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>43.312196</td>\n",
              "      <td>2012</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.313993</td>\n",
              "      <td>11.924511</td>\n",
              "      <td>0.003138</td>\n",
              "      <td>92.654874</td>\n",
              "      <td>2012</td>\n",
              "      <td>2</td>\n",
              "      <td>2012-02-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15.197791</td>\n",
              "      <td>26.505990</td>\n",
              "      <td>0.002096</td>\n",
              "      <td>247.560124</td>\n",
              "      <td>2012</td>\n",
              "      <td>3</td>\n",
              "      <td>2012-03-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>21.683913</td>\n",
              "      <td>36.393538</td>\n",
              "      <td>0.001276</td>\n",
              "      <td>300.516098</td>\n",
              "      <td>2012</td>\n",
              "      <td>4</td>\n",
              "      <td>2012-04-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21.867528</td>\n",
              "      <td>38.525832</td>\n",
              "      <td>0.007482</td>\n",
              "      <td>384.988000</td>\n",
              "      <td>2012</td>\n",
              "      <td>5</td>\n",
              "      <td>2012-05-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Mean_Vol    Std_Vol   Min_Vol     Max_Vol  Year  Month       Date\n",
              "1  23.829470  22.711133  0.455581   48.000000  2011     12 2011-12-01\n",
              "2   4.031777   6.740555  0.020000   43.312196  2012      1 2012-01-01\n",
              "3   8.313993  11.924511  0.003138   92.654874  2012      2 2012-02-01\n",
              "4  15.197791  26.505990  0.002096  247.560124  2012      3 2012-03-01\n",
              "0  21.683913  36.393538  0.001276  300.516098  2012      4 2012-04-01\n",
              "5  21.867528  38.525832  0.007482  384.988000  2012      5 2012-05-01"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UunBZ10kVXp-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e782ec9-71f8-4af0-940a-bb1f9ebedaa3"
      },
      "source": [
        "# PLOTTING OF THE MEAN VOLUME BY MONTH\n",
        "mean_vol_trac = {\n",
        "    \"x\": stats_dfp.Date,\n",
        "    \"y\": stats_dfp[\"Mean_Vol\"],\n",
        "}\n",
        "\n",
        "layout = {\n",
        "  \"height\":1000,\n",
        "  \"showlegend\": True, \n",
        "  \"title\": \"Average Volume by Month of BTC\",\n",
        "}\n",
        "\n",
        "fig = go.Figure(data=[mean_vol_trac], layout=layout)\n",
        "fig.show(renderer=\"colab\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"2159198c-9c88-4824-9c8c-11287ebd2d80\" class=\"plotly-graph-div\" style=\"height:1000px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"2159198c-9c88-4824-9c8c-11287ebd2d80\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '2159198c-9c88-4824-9c8c-11287ebd2d80',\n",
              "                        [{\"type\": \"scatter\", \"x\": [\"2011-12-01T00:00:00\", \"2012-01-01T00:00:00\", \"2012-02-01T00:00:00\", \"2012-03-01T00:00:00\", \"2012-04-01T00:00:00\", \"2012-05-01T00:00:00\"], \"y\": [23.829469525, 4.03177731714571, 8.313992791715775, 15.19779132870153, 21.68391320646051, 21.867528252181803]}],\n",
              "                        {\"height\": 1000, \"showlegend\": true, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Average Volume by Month of BTC\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2159198c-9c88-4824-9c8c-11287ebd2d80');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1XHlwrvVXp-"
      },
      "source": [
        "###  <strong> Exercise 12 </strong> - Compute the statistics using Pyspark (1 point)\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to calculate the statistics of the bitcoin as we did before but using Pandas.\n",
        "\n",
        "The steps will be:\n",
        "<ul>\n",
        "     <li>   import data from Parquet in a Spark DataFrame </li>\n",
        "     <li>   remove null values </li>\n",
        "     <li>   perform the aggregation of the results </li> \n",
        "     <li>   convert the results to Pandas </li> \n",
        " \n",
        "</ul>\n",
        "\n",
        "\n",
        "    \n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcS4_jawVXp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b63ce52-d8af-439a-910b-df37b9ed97d7"
      },
      "source": [
        "%%time\n",
        "# solution to compute the statistics using pyspark function \n",
        "\n",
        "from pyspark.sql.functions import min, max, mean, stddev\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "\n",
        "# full spark dataframe (recall exercise 8a)\n",
        "df_spark = spark.read.parquet(\"BTC/\") \n",
        "\n",
        "# the na drop is important to be able to compute properly the stats\n",
        "# look at the documentation of the na.drop function\n",
        "group_ym = df_spark.na.drop().select([\"Volume_BTC\",\"Year\",\"Month\"]).groupBy([\"Year\",\"Month\"])\n",
        "\n",
        "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
        "           \n",
        "    \n",
        "# aggregation \n",
        "# notice that the argument of the agg function is strictly related to min Vol, max Vol, mean, and stddev.\n",
        "res_df = group_ym.agg(F.count(\"Volume_BTC\").alias('count'), \n",
        "             F.mean(\"Volume_BTC\").alias('mean'), \n",
        "             F.stddev(\"Volume_BTC\").alias('std'), \n",
        "             F.min(\"Volume_BTC\").alias('min'), \n",
        "             F.max(\"Volume_BTC\").alias('max'))\n",
        "\n",
        "# # #conversion the results to pandas\n",
        "stats_dfs = res_df.toPandas()\n",
        "\n",
        "# #'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "stats_dfs = get_date_from_year_month(stats_dfs)    \n",
        "stats_dfs.sort_values(by = 'Date',inplace = True)\n",
        "stats_dfs"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 42.3 ms, sys: 5.99 ms, total: 48.3 ms\n",
            "Wall time: 2.25 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAzvUAQJVXp_"
      },
      "source": [
        "### Extra. Equivalence of results\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Now that you have seen the two procedures to get the results you must compare the outputs:\n",
        "<ul>\n",
        "     <li>   verify if the pandas dataframe from applyInPandas and PySpark functions are equivalents (look at the documentation to find the function that asserts if two DataFrames are equals) </li> \n",
        "         <li> compare the processing time between applyInPandas and PySpark routine with functions (that we have visualised with the %%time function) and comment them.</li> \n",
        " \n",
        "</ul>\n",
        "\n",
        "\n",
        "    \n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lngxwISVXp_"
      },
      "source": [
        "# verify if the pandas dataframe from applyInPands and PySpark functions are equivalents \n",
        "# compare the processing time between applyInPandas and PySpark function\n",
        "cols = ['Mean_Vol', 'Std_Vol', 'Min_Vol', 'Max_Vol', 'Year', 'Month', 'Date']\n",
        " \n",
        "\n",
        "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
        "\n",
        "# equivalence verification: look at the pandas API and check if there is any test/assertion operation of help\n",
        "\n",
        "\n",
        "# comment about time execution and draw your considerations\n",
        "#'''############## END OF THE EXERCISE ##############'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSw_cXcmVXqA"
      },
      "source": [
        "### Extra - Plotting the financial data (1 point)\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Now that you have seen some examples you can draw your graphs:\n",
        "<ul>\n",
        "     <li>   filter the global data frame fron Parquet and take only the first day of the year 2021 </li> \n",
        "         <li> convert it to a pandas dataframe </li> \n",
        "         <li>    display the data using the $plot_candlestick$ routine </li> \n",
        " \n",
        "</ul>\n",
        "\n",
        "\n",
        "    \n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R05figbfVXqA"
      },
      "source": [
        "#this function helps you to display the candlestick ( representation of financial data) of the pandas dataframe\n",
        "\n",
        "def plot_candlestick(df):\n",
        "    trace = {\n",
        "      \"x\": df.Date_Time,\n",
        "      \"close\": dfp[\"Open\"],\n",
        "      \"decreasing\": {\"line\": {\"color\": \"#008000\"}}, \n",
        "      \"high\":df[\"High\"] ,\n",
        "      \"increasing\": {\"line\": {\"color\": \"#db4052\"}}, \n",
        "      \"low\": df[\"Low\"],\n",
        "      \"name\": \"BTC\", \n",
        "      \"open\": df[\"Close\"],\n",
        "      \"type\": \"candlestick\"\n",
        "    }\n",
        "\n",
        "    layout = {\n",
        "      \"height\":1000,\n",
        "      \"showlegend\": True, \n",
        "      \"title\": \"Technical Analysis\",\n",
        "    }\n",
        "    \n",
        "    fig = go.Figure(data=[trace], layout=layout)\n",
        "    fig.show(renderer=\"colab\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu81JTxBVXqA"
      },
      "source": [
        "# Exercise filter the spark dataframe by date \n",
        "import datetime as dt\n",
        "\n",
        "\n",
        "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
        "#read the global dataframe (as usual)\n",
        "df_spark = \n",
        "\n",
        "#create the beginning end date \n",
        "beg = \n",
        "end = \n",
        "\n",
        "\n",
        "#create the filter\n",
        "df_spark_filtered = \n",
        "\n",
        "#apply and convert it to pandas\n",
        "dfp = \n",
        "#'''############## END OF THE EXERCISE ##############'''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VQE8mhPVXqB",
        "scrolled": false
      },
      "source": [
        "plot_candlestick(dfp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxGOfykpVXqB"
      },
      "source": [
        "####  Extra - Propose your analysis\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Think about a new analysis on this set of data to run on your data and run it showing a graph\n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewmVzLeuVXqB"
      },
      "source": [
        "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
        "\n",
        "#'''############## END OF THE EXERCISE ##############'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "524h_jgeVXqB"
      },
      "source": [
        "###Conclusion \n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "$ApplyinPandas$ can be very powerful when you need to apply advanced Python code or Python libraries (i.e. <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a>  otherwise you can use Pyspark routines relying on most powerful storage techniques for example using Parquet.\n",
        "\n",
        "    \n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG3YxgAzVXqB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}