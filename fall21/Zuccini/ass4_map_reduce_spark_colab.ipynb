{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfstpKhUM8x7"
      },
      "source": [
        "<h1><center>Big Data Algorithms Techniques & Platforms</center></h1>\n",
        "<h2>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Assignment 4 - MapReduce and Spark</center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWwndZSiM8x8"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In this exercise you is asked to use Spark for implementing an algorithm that applies computations on documents and dataframes.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Execute the following cell in order to initialize Spark**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjL7k73lEmgb"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!tar zxvf spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3shtJGpOM8x8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/Users/viceroy/Downloads/content/spark-3.0.3-bin-hadoop2.7\"\n",
        "# os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home\"\n",
        "\n",
        "#os.environ[\"SPARK_HOME\"] = \"/Users/viceroy/Downloads/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/Users/viceroy/Downloads/spark-3.0.3-bin-hadoop2.7\" \n",
        "#os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.8/site-packages/pyspark/\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"/content/spark-3.0.3-bin-hadoop2.7\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"hw4\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgBYksx_Emgd"
      },
      "outputs": [],
      "source": [
        "!apt install unzip\n",
        "!unzip lyrics_files_idioms.zip -d lyrics_files_idioms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2weEbWrAEmgd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK49MFw9M8yA"
      },
      "source": [
        "# Analysing documents\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We have already seen that MapReduce procedures are good in analyzing text-files.\n",
        "    \n",
        "The provided data comes from a scraping operation on the website https://www.vagalume.com.br/ and is available on kaggle:\n",
        "    \n",
        "https://www.kaggle.com/neisse\n",
        "    \n",
        "\n",
        "    \n",
        "The assignment is divided in 2 parts:\n",
        "    \n",
        "* Part 1 is focused on MapReduce \n",
        "    \n",
        "* Part 2  is focuses on dataframes\n",
        "    </font>\n",
        "    </p>\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>Notice that  dataset is noisy and shows all the typical issues related with data coming from this procedure (duplicated entries, etc).</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvwJGzaHEmge"
      },
      "source": [
        "# Part 1 -  MapReduce\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In the provided folder you can find a set of documents/files containing  descriptions of songs (lyrics and additional informations). Specifically in each file:\n",
        "\n",
        "- the first line is the idiom/language\n",
        "- the second line is the relative url of the song of the original website\n",
        "- the third line is the title of a song \n",
        "- from fourth line on the text you find the lyrics of the song.\n",
        "    </font>\n",
        "    </p>\n",
        "\n",
        "## Exercise 1 - (2 points) - Song's lyrics \n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Provide a Spark MapReduce procedure that reads the documents and checks how many song's lyrics appear at least two times.\n",
        "\n",
        "In the data-interpretation of this exercise you can consider that two files represent the same lyric if the url (3rd line of each file) is the same.\n",
        "\n",
        " </font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>Notice that  you can reuse any code that was made available for the previous labs/assignments or that you already developed in these contexts.</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Q2PCHcomEmgf"
      },
      "outputs": [],
      "source": [
        "### Write here your code\n",
        "import re\n",
        "\n",
        "#split by content in item[1]\n",
        "def find_url(item): \n",
        "    description = item[1]\n",
        "    lst = re.compile(r\"\\n\").split(description)\n",
        "    url = lst[1] #the second line is the relative url of the song of the original website\n",
        "    return url, 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ncioSH4YEmgg"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"hw4\")\n",
        "sc = SparkContext.getOrCreate(conf = conf)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#note: took my computer 33 min to run\n",
        "input_file = sc.wholeTextFiles(\"lyrics_files_idioms/*\")\n",
        "urls = input_file.map(find_url)\n",
        "url_counts = urls.reduceByKey(lambda x, y: x + y)\n",
        "duplicate_songs = url_counts.filter(lambda x: x[1] > 1)"
      ],
      "metadata": {
        "id": "OGYZw49JVwSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total Duplicates count: {duplicate_songs.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99TM1uFTWFLw",
        "outputId": "c0629f04-c37e-481c-87bc-6eccf2144f73"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Duplicate count: 38096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNBVGHDkEmgg"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "### 2.1 - (1 point) - Distinct songs\n",
        "Provide a Spark MapReduce procedure that provides how many distinct song's lyrics are present.\n",
        "\n",
        "Also in this case consider the uri as key: two files represent the same lyric if the url is equal.\n",
        "\n",
        "### 2.2 - (1 point) - Chaining\n",
        "According to your implementation of Exercise 1, can you chain MapReduce additional MapReduce steps for solving Exercise 2.2? \n",
        "\n",
        "Provide the code for 2.1 and anwer for 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "E_kbQX-9Emgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c25997e-c85c-4a82-fc0a-005f1d8ead63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Distinct count: 167499\n"
          ]
        }
      ],
      "source": [
        "### Write here your code followed by the answer to question 2.2\n",
        "\n",
        "print(f\"Total Distinct count: {url_counts.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyEhyV6WEmgg"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "### 3.1 - (3 points) - Most common word for language\n",
        "\n",
        "Now that you discovered the duplicated documents consider just one occurence of each song's lyric and define a MapReduce procedure that finds the most common word for each language (of course you must remove stop words).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = input_file.toDF()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOE98G1JZD-s",
        "outputId": "8031c554-0dc0-46a2-b304-74a7c46fa660"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|                  _1|                  _2|\n",
            "+--------------------+--------------------+\n",
            "|file:/content/lyr...|ENGLISH\n",
            "/pac-div/...|\n",
            "|file:/content/lyr...|ENGLISH\n",
            "/johnny-m...|\n",
            "|file:/content/lyr...|ENGLISH\n",
            "/ian-dury...|\n",
            "|file:/content/lyr...|ENGLISH\n",
            "/flo-rida...|\n",
            "|file:/content/lyr...|PORTUGUESE\n",
            "/davi-...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "N0PxUbwcEmgh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "3b60759b-1fdb-49e1-f287-b3fee53ebb1b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-1dd0e5489ab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#new_df = [remover.transform(i) for txt in df]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mremover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStopWordsRemover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopWords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"_3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mremover\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Input type must be array<string> but got string."
          ]
        }
      ],
      "source": [
        "### Write here your code\n",
        "\n",
        "from pyspark.ml.feature import StopWordsRemover \n",
        "stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
        "#new_df = [remover.transform(i) for txt in df]\n",
        "remover = StopWordsRemover(stopWords=stopwords, inputCol=\"_1\", outputCol=\"_3\")\n",
        "#remover.transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZfguAifEmgh"
      },
      "source": [
        "### 3.2 - (3 points) - Most common end/start words\n",
        "\n",
        "Finally discover, for each language, the most common ending and starting word (of course, also in this case) you must remove stop words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMCu3TZSEmgh"
      },
      "outputs": [],
      "source": [
        "### Write here your code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-QgxsF1M8yB"
      },
      "source": [
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**DataFrames**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBkYMzzIM8yB",
        "outputId": "2f6a99b7-c42d-4401-9a7b-96628f3a0472"
      },
      "source": [
        "# Part 2 - Dataframes\n",
        "\n",
        "In this part you can use Pandas Dataframes or  Spark Dataframes.  I suggest to use a Spark Dataframe\n",
        "end exploit the Pandas functionalities as we have seen in the 2nd assignment. Download the two available datasets at the link:\n",
        "\n",
        "https://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres\n",
        "\n",
        "You can find two .cvs files: \n",
        "\n",
        "* artists-data.csv\n",
        "\n",
        "* lyrics-data.csv\n",
        "\n",
        "\n",
        "# Import artist data.\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The artist data in the .csv file can be stored in a dataframe. \n",
        "    \n",
        "Each row of the .csv file describes an artist and the columns represent the following data:\n",
        "    \n",
        "* Artist - The artist's name\n",
        "* Popularity - Popularity score at the date of scrapping\n",
        "* ALink - The link to the artist's page\n",
        "* AGenre - Primary musical genre of the artist\n",
        "* AGenres - A list (pay attention to the format) of genres the artist fits in\n",
        "    \n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "# Import song's lyrics data.\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "    \n",
        "Each row of the .csv file describes a lyric and the columns represent the following data:\n",
        "    \n",
        "* ALink - The link to the webpage of the artist\n",
        "* SLink - The link to the webpage of the song\n",
        "* Idiom - The idiom of the lyric\n",
        "* Lyric - The lyrics\n",
        "* SName - The name of the song\n",
        "\n",
        "    \n",
        "\n",
        "</font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQn1a3PqM80l"
      },
      "source": [
        "#  Exercise 4 - (3 points) - Artist's genre\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Provide a program that finds the artists for which the genre is not specified.\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "0liD2NOVM8yE"
      },
      "outputs": [],
      "source": [
        "### Write here your code\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"HW4\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "artists_df = spark.read.option(\"header\", \"true\")\\\n",
        "    .option(\"inferSchema\", \"true\")\\\n",
        "    .csv(\"artists-data.csv\")\n",
        "\n",
        "artists_df.show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSHYCNPBR77S",
        "outputId": "d6e9574d-7482-45c8-f604-20e600692883"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+----------+-------------------+-----+--------------------+\n",
            "|           Artist|Songs|Popularity|               Link|Genre|              Genres|\n",
            "+-----------------+-----+----------+-------------------+-----+--------------------+\n",
            "|    10000 Maniacs|  110|       0.3|    /10000-maniacs/| Rock|Rock; Pop; Electr...|\n",
            "|        12 Stones|   75|       0.3|        /12-stones/| Rock|Rock; Gospel/Reli...|\n",
            "|              311|  196|       0.5|              /311/| Rock|Rock; Surf Music;...|\n",
            "|    4 Non Blondes|   15|       7.5|    /4-non-blondes/| Rock|Rock; Pop/Rock; R...|\n",
            "|A Cruz Está Vazia|   13|       0.0|/a-cruz-esta-vazia/| Rock|                Rock|\n",
            "|  Aborto Elétrico|   36|       0.1|  /aborto-eletrico/| Rock|Rock; Punk Rock; ...|\n",
            "|            Abril|   36|       0.1|            /abril/| Rock|Rock; Emocore; Ha...|\n",
            "|            Abuse|   13|       0.0|            /abuse/| Rock|      Rock; Hardcore|\n",
            "|            AC/DC|  192|      10.8|            /ac-dc/| Rock|Rock; Heavy Metal...|\n",
            "|            ACEIA|    0|       0.0|            /aceia/| Rock|                Rock|\n",
            "+-----------------+-----+----------+-------------------+-----+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can see there is nothing == the data is very clean\n",
        "artists_df.filter(\"Genre IS NULL\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIU_OlqMSSY1",
        "outputId": "a44b42cb-341d-4e36-fb5f-5842febca11d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+----------+----+-----+------+\n",
            "|Artist|Songs|Popularity|Link|Genre|Genres|\n",
            "+------+-----+----------+----+-----+------+\n",
            "+------+-----+----------+----+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf_WOVxOEmgi"
      },
      "source": [
        "#  Exercise 5 - (3 points) - Duplicates\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Provide a program that removes the duplicates in the artists (also in this case the URL is the key).\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Ivji6KGREmgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c5761b-87f1-4389-cfc2-c8b3cb9b1741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3242, 6)\n"
          ]
        }
      ],
      "source": [
        "### Write here your code\n",
        "print((artists_df.count(), len(artists_df.columns))) #df.shape format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "artists_new = artists_df.dropDuplicates([\"Link\"]) #url==Link is key\n",
        "print((artists_new.count(), len(artists_new.columns)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-xkZRJVScrK",
        "outputId": "3ced40c4-20d4-4be8-e18d-a13fe5413eea"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2940, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7L_8xp4Emgi"
      },
      "source": [
        "#  Exercise 6 - (4 points)\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Provide a program that using dataframe return the 100 most popular artists and the lyrics of their songs.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
        "    .csv(\"lyrics-data.csv\")\n",
        "\n",
        "lyrics_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlkgynRsS7fw",
        "outputId": "3fac861c-6cfa-4abb-8cee-a9aabe8a86d2"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|          ALink|               SName|               SLink|               Lyric|               Idiom|\n",
            "+---------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|/10000-maniacs/|      More Than This|/10000-maniacs/mo...|I could feel at t...|             ENGLISH|\n",
            "|/10000-maniacs/|   Because The Night|/10000-maniacs/be...|Take me now, baby...|             ENGLISH|\n",
            "|/10000-maniacs/|      These Are Days|/10000-maniacs/th...|These are. These ...|             ENGLISH|\n",
            "|/10000-maniacs/|     A Campfire Song|/10000-maniacs/a-...|\"A lie to say, \"\"...| \"\"O my. river wh...|\n",
            "|/10000-maniacs/|Everyday Is Like ...|/10000-maniacs/ev...|Trudging slowly o...|             ENGLISH|\n",
            "|/10000-maniacs/|          Don't Talk|/10000-maniacs/do...|Don't talk, I wil...|             ENGLISH|\n",
            "|/10000-maniacs/|   Across The Fields|/10000-maniacs/ac...|Well they left th...|             ENGLISH|\n",
            "|/10000-maniacs/|Planned Obsolescence|/10000-maniacs/pl...|[ music: Dennis D...|             ENGLISH|\n",
            "|/10000-maniacs/|           Rainy Day|/10000-maniacs/ra...|On bended kneeI'v...|             ENGLISH|\n",
            "|/10000-maniacs/|Anthem For Doomed...|/10000-maniacs/an...|For whom do the b...|             ENGLISH|\n",
            "+---------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "31TJWgi2Emgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29421c4e-9188-4a05-c5ff-c26cdf6c9f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------+\n",
            "|         Artist|             Link|\n",
            "+---------------+-----------------+\n",
            "|  Ariana Grande|  /ariana-grande/|\n",
            "|      Lady Gaga|      /lady-gaga/|\n",
            "|       Maroon 5|       /maroon-5/|\n",
            "|         Eminem|         /eminem/|\n",
            "|          Queen|          /queen/|\n",
            "|Imagine Dragons|/imagine-dragons/|\n",
            "|          Adele|          /adele/|\n",
            "|    The Beatles|    /the-beatles/|\n",
            "|        Beyoncé|        /beyonce/|\n",
            "|         Anitta|         /anitta/|\n",
            "+---------------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Write here your code\n",
        "\n",
        "sorted_artists = artists_new.sort(artists_new.Popularity.desc())\\\n",
        "    .limit(100)\\\n",
        "    .select(\"Artist\", \"Link\")\n",
        "\n",
        "sorted_artists.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined = sorted_artists.join(lyrics_df, \n",
        "                               sorted_artists.Link == lyrics_df.ALink, \n",
        "                               \"inner\")\\\n",
        "    .select(\"Artist\", \"Sname\", \"Lyric\")\\\n",
        "    \n",
        "combined.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQjUIliiTCRw",
        "outputId": "a1c74867-6289-4df6-8238-7367d5a025cf"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+--------------------+\n",
            "|Artist|               Sname|               Lyric|\n",
            "+------+--------------------+--------------------+\n",
            "| AC/DC|       Back In Black|Back in black, I ...|\n",
            "| AC/DC|     Highway To Hell|Living easy, livi...|\n",
            "| AC/DC|       Thunderstruck|Ahh-ahh-ahh-ahh-a...|\n",
            "| AC/DC|              T.N.T.|Aye, aye, aye, ay...|\n",
            "| AC/DC|You Shook Me All ...|She was a fast ma...|\n",
            "| AC/DC| Rock 'n' Roll Train|One hot angel. On...|\n",
            "| AC/DC|     Shoot To Thrill|All you women who...|\n",
            "| AC/DC|         Hells Bells|I'm a rolling thu...|\n",
            "| AC/DC|It's A Long Way T...|Ridin' down the h...|\n",
            "| AC/DC|Dirty Deeds, Done...|If you're havin' ...|\n",
            "+------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DatBVcKUM8yn"
      },
      "source": [
        "# 2 - Bonus \n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Using the approach you prefer (just Dataframes, hybrid approach)  :\n",
        "    \n",
        "* the 10 most common words in the lyrics of each artist\n",
        "* the 10 most common words for each genre. For this question we can use the primary genre of the artist.\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3EIzkQFM8yo"
      },
      "outputs": [],
      "source": [
        "# Write here your code and the detailed description of the MapReduce algorithm.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "ass4_map_reduce_spark_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "name": "BE4-Spark.ipynb"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}