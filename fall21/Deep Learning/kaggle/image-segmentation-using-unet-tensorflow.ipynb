{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Segmentation using Tensorflow","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/2592/1*rZ1vDrOBWqISFiNL5OMEbg.jpeg)","metadata":{}},{"cell_type":"markdown","source":"# First question arise what is Image Segmentation?","metadata":{}},{"cell_type":"markdown","source":"## So far you have seen image classification, where the task of the network is to assign a label or class to an input image. However, suppose you want to know where an object is located in the image, the shape of that object, which pixel belongs to which object, etc. In this case you will want to segment the image, i.e., each pixel of the image is given a label. Thus, the task of image segmentation is to train a neural network to output a pixel-wise mask of the image. This helps in understanding the image at a much lower level, i.e., the pixel level. Image segmentation has many applications in medical imaging, self-driving cars and satellite imaging to name a few.","metadata":{}},{"cell_type":"markdown","source":"![](https://www.tensorflow.org/tutorials/images/segmentation_files/output_a6u_Rblkteqb_0.png)","metadata":{}},{"cell_type":"markdown","source":"## We are Using Oxford IIIT Pet Dataset The dataset consists of images, their corresponding labels, and pixel-wise masks. The masks are basically labels for each pixel. Each pixel is given one of three categories \n\n*    Class 1 : Pixel belonging to the pet.\n*     Class 2 : Pixel bordering the pet.\n*     Class 3 : None of the above/ Surrounding pixel.","metadata":{}},{"cell_type":"markdown","source":"## Let's Begin","metadata":{}},{"cell_type":"code","source":"pip install -q git+https://github.com/tensorflow/examples.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-11T20:34:50.177763Z","iopub.execute_input":"2022-01-11T20:34:50.178114Z","iopub.status.idle":"2022-01-11T20:35:04.235378Z","shell.execute_reply.started":"2022-01-11T20:34:50.178067Z","shell.execute_reply":"2022-01-11T20:35:04.234440Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_datasets","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:35:07.720880Z","iopub.execute_input":"2022-01-11T20:35:07.721250Z","iopub.status.idle":"2022-01-11T20:35:30.653072Z","shell.execute_reply.started":"2022-01-11T20:35:07.721198Z","shell.execute_reply":"2022-01-11T20:35:30.652067Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow as tf\n\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\n\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-01-11T20:35:30.656394Z","iopub.execute_input":"2022-01-11T20:35:30.656809Z","iopub.status.idle":"2022-01-11T20:35:31.221671Z","shell.execute_reply.started":"2022-01-11T20:35:30.656759Z","shell.execute_reply":"2022-01-11T20:35:31.220512Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:35:31.226540Z","iopub.execute_input":"2022-01-11T20:35:31.228933Z","iopub.status.idle":"2022-01-11T20:36:22.832312Z","shell.execute_reply.started":"2022-01-11T20:35:31.228881Z","shell.execute_reply":"2022-01-11T20:36:22.831309Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## The following code performs a simple augmentation of flipping an image. In addition, image is normalized to [0,1]. Finally, as mentioned above the pixels in the segmentation mask are labeled either {1, 2, 3}. For the sake of convenience, let's subtract 1 from the segmentation mask, resulting in labels that are : {0, 1, 2}.","metadata":{}},{"cell_type":"code","source":"def normalize(input_image, input_mask):\n  input_image = tf.cast(input_image, tf.float32) / 255.0\n  input_mask -= 1\n  return input_image, input_mask","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:36:52.042512Z","iopub.execute_input":"2022-01-11T20:36:52.042896Z","iopub.status.idle":"2022-01-11T20:36:52.048282Z","shell.execute_reply.started":"2022-01-11T20:36:52.042846Z","shell.execute_reply":"2022-01-11T20:36:52.047337Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef load_image_train(datapoint):\n  input_image = tf.image.resize(datapoint['image'], (128, 128))\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n\n  if tf.random.uniform(()) > 0.5:\n    input_image = tf.image.flip_left_right(input_image)\n    input_mask = tf.image.flip_left_right(input_mask)\n\n  input_image, input_mask = normalize(input_image, input_mask)\n  return input_image, input_mask","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:36:56.520012Z","iopub.execute_input":"2022-01-11T20:36:56.520372Z","iopub.status.idle":"2022-01-11T20:36:56.527783Z","shell.execute_reply.started":"2022-01-11T20:36:56.520312Z","shell.execute_reply":"2022-01-11T20:36:56.526895Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def load_image_test(datapoint):\n  input_image = tf.image.resize(datapoint['image'], (128, 128))\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n  input_image, input_mask = normalize(input_image, input_mask)\n  return input_image, input_mask","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:36:57.134058Z","iopub.execute_input":"2022-01-11T20:36:57.134425Z","iopub.status.idle":"2022-01-11T20:36:57.140227Z","shell.execute_reply.started":"2022-01-11T20:36:57.134372Z","shell.execute_reply":"2022-01-11T20:36:57.139399Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## The dataset already contains the required splits of test and train and so let's continue to use the same split.","metadata":{}},{"cell_type":"code","source":"TRAIN_LENGTH = info.splits['train'].num_examples\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nSTEPS_PER_EPOCH = TRAIN_LENGTH ","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:36:58.105858Z","iopub.execute_input":"2022-01-11T20:36:58.106216Z","iopub.status.idle":"2022-01-11T20:36:58.113617Z","shell.execute_reply.started":"2022-01-11T20:36:58.106166Z","shell.execute_reply":"2022-01-11T20:36:58.112731Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntest = dataset['test'].map(load_image_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:36:58.561784Z","iopub.execute_input":"2022-01-11T20:36:58.562137Z","iopub.status.idle":"2022-01-11T20:36:58.942168Z","shell.execute_reply.started":"2022-01-11T20:36:58.562088Z","shell.execute_reply":"2022-01-11T20:36:58.941398Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntest_dataset = test.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:36:59.232375Z","iopub.execute_input":"2022-01-11T20:36:59.232857Z","iopub.status.idle":"2022-01-11T20:36:59.247941Z","shell.execute_reply.started":"2022-01-11T20:36:59.232793Z","shell.execute_reply":"2022-01-11T20:36:59.246931Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def display(display_list):\n  plt.figure(figsize=(15, 15))\n\n  title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(title[i])\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n    plt.axis('off')\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:36:59.742313Z","iopub.execute_input":"2022-01-11T20:36:59.742690Z","iopub.status.idle":"2022-01-11T20:36:59.749058Z","shell.execute_reply.started":"2022-01-11T20:36:59.742634Z","shell.execute_reply":"2022-01-11T20:36:59.748165Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"for image, mask in train.take(1):\n  sample_image, sample_mask = image, mask\ndisplay([sample_image, sample_mask])","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:37:00.383963Z","iopub.execute_input":"2022-01-11T20:37:00.384339Z","iopub.status.idle":"2022-01-11T20:37:00.896145Z","shell.execute_reply.started":"2022-01-11T20:37:00.384284Z","shell.execute_reply":"2022-01-11T20:37:00.895056Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Now it's time to the define the model","metadata":{}},{"cell_type":"markdown","source":"## The model being used here is a modified U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, a pretrained model can be used as the encoder. Thus, the encoder for this task will be a pretrained MobileNetV2 model,","metadata":{}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:37:03.806145Z","iopub.execute_input":"2022-01-11T20:37:03.806521Z","iopub.status.idle":"2022-01-11T20:37:03.810640Z","shell.execute_reply.started":"2022-01-11T20:37:03.806469Z","shell.execute_reply":"2022-01-11T20:37:03.809701Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## The reason to output three channels is because there are three possible labels for each pixel. Think of this as multi-classification where each pixel is being classified into three classes.","metadata":{}},{"cell_type":"code","source":"base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n# Use the activations of these layers\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nlayers = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\ndown_stack.trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:37:06.224103Z","iopub.execute_input":"2022-01-11T20:37:06.224464Z","iopub.status.idle":"2022-01-11T20:37:09.629453Z","shell.execute_reply.started":"2022-01-11T20:37:06.224409Z","shell.execute_reply":"2022-01-11T20:37:09.628557Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples.","metadata":{}},{"cell_type":"code","source":"up_stack = [\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:37:35.233943Z","iopub.execute_input":"2022-01-11T20:37:35.234318Z","iopub.status.idle":"2022-01-11T20:37:35.252379Z","shell.execute_reply.started":"2022-01-11T20:37:35.234267Z","shell.execute_reply":"2022-01-11T20:37:35.251409Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Defining the Unet","metadata":{}},{"cell_type":"code","source":"def unet_model(output_channels):\n  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n  x = inputs\n\n  # Downsampling through the model\n  skips = down_stack(x)\n  x = skips[-1]\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    concat = tf.keras.layers.Concatenate()\n    x = concat([x, skip])\n\n  # This is the last layer of the model\n  last = tf.keras.layers.Conv2DTranspose(\n      output_channels, 3, strides=2,\n      padding='same')  #64x64 -> 128x128\n\n  x = last(x)\n  return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:37:36.908246Z","iopub.execute_input":"2022-01-11T20:37:36.908638Z","iopub.status.idle":"2022-01-11T20:37:36.917018Z","shell.execute_reply.started":"2022-01-11T20:37:36.908581Z","shell.execute_reply":"2022-01-11T20:37:36.915664Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Let's Train the model","metadata":{}},{"cell_type":"code","source":"model = unet_model(OUTPUT_CHANNELS)\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:37:44.138598Z","iopub.execute_input":"2022-01-11T20:37:44.138977Z","iopub.status.idle":"2022-01-11T20:37:46.088951Z","shell.execute_reply.started":"2022-01-11T20:37:44.138926Z","shell.execute_reply":"2022-01-11T20:37:46.088155Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Have a quick look at the resulting model architecture","metadata":{}},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T16:53:46.674756Z","iopub.execute_input":"2022-01-11T16:53:46.675385Z","iopub.status.idle":"2022-01-11T16:53:46.830799Z","shell.execute_reply.started":"2022-01-11T16:53:46.675333Z","shell.execute_reply":"2022-01-11T16:53:46.829859Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Let's try out the model to see what it predicts before training.","metadata":{}},{"cell_type":"code","source":"def create_mask(pred_mask):\n  pred_mask = tf.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0]","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:37:51.064216Z","iopub.execute_input":"2022-01-11T20:37:51.064761Z","iopub.status.idle":"2022-01-11T20:37:51.070490Z","shell.execute_reply.started":"2022-01-11T20:37:51.064528Z","shell.execute_reply":"2022-01-11T20:37:51.069307Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def show_predictions(dataset=None, num=1):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:37:54.032798Z","iopub.execute_input":"2022-01-11T20:37:54.035822Z","iopub.status.idle":"2022-01-11T20:37:54.047088Z","shell.execute_reply.started":"2022-01-11T20:37:54.033164Z","shell.execute_reply":"2022-01-11T20:37:54.045326Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"show_predictions()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T21:03:07.577730Z","iopub.execute_input":"2022-01-11T21:03:07.578174Z","iopub.status.idle":"2022-01-11T21:03:08.014612Z","shell.execute_reply.started":"2022-01-11T21:03:07.578111Z","shell.execute_reply":"2022-01-11T21:03:08.013645Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Let's observe how the model improves while it is training. To accomplish this task, a callback function is defined below. ","metadata":{}},{"cell_type":"code","source":"class DisplayCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    clear_output(wait=True)\n    show_predictions()\n    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:38:45.300860Z","iopub.execute_input":"2022-01-11T20:38:45.301225Z","iopub.status.idle":"2022-01-11T20:38:45.306948Z","shell.execute_reply.started":"2022-01-11T20:38:45.301175Z","shell.execute_reply":"2022-01-11T20:38:45.305882Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\nVAL_SUBSPLITS = 5\nVALIDATION_STEPS = info.splits['test'].num_examples\n\nmodel_history = model.fit(train_dataset, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=test_dataset,\n                          callbacks=[DisplayCallback()])","metadata":{"execution":{"iopub.status.busy":"2022-01-11T21:03:00.736394Z","iopub.execute_input":"2022-01-11T21:03:00.736778Z","iopub.status.idle":"2022-01-11T21:03:02.369510Z","shell.execute_reply.started":"2022-01-11T21:03:00.736726Z","shell.execute_reply":"2022-01-11T21:03:02.367037Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"loss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\nepochs = range(EPOCHS)\n\nplt.figure(figsize = (10, 8)) \nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'bo', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.ylim([0, 1])\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:38:51.492573Z","iopub.execute_input":"2022-01-11T20:38:51.492923Z","iopub.status.idle":"2022-01-11T20:38:51.576787Z","shell.execute_reply.started":"2022-01-11T20:38:51.492876Z","shell.execute_reply":"2022-01-11T20:38:51.575122Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"show_predictions(test_dataset, 7)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T21:03:22.422591Z","iopub.execute_input":"2022-01-11T21:03:22.422957Z","iopub.status.idle":"2022-01-11T21:03:26.758132Z","shell.execute_reply.started":"2022-01-11T21:03:22.422908Z","shell.execute_reply":"2022-01-11T21:03:26.757213Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# Using FDL Dataset","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport glob         # file operations\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image # Image I/O (e.g. Image.open)\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:39:02.673991Z","iopub.execute_input":"2022-01-11T20:39:02.674339Z","iopub.status.idle":"2022-01-11T20:39:02.679755Z","shell.execute_reply.started":"2022-01-11T20:39:02.674290Z","shell.execute_reply":"2022-01-11T20:39:02.678638Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\ntrain_images = '../input/fdl2022/train_images/train_images/'\ntrain_masks = '../input/fdl2022/train_masks/train_masks/'\ntest_image = '../input/fdl2022/test_images/test_images/'","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:39:02.964291Z","iopub.execute_input":"2022-01-11T20:39:02.964676Z","iopub.status.idle":"2022-01-11T20:39:02.969662Z","shell.execute_reply.started":"2022-01-11T20:39:02.964618Z","shell.execute_reply":"2022-01-11T20:39:02.968650Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Get the directories of all images and masks\ntrain_x = sorted(glob.glob(train_images+'*.jpg'))\ntrain_y = sorted(glob.glob(train_masks+'*.png'))\ntest_x = glob.glob(test_image+'*.jpg')\n\nprint('Train Images:', len(train_x))\nprint('Test Images:', len(test_x))","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:39:03.839392Z","iopub.execute_input":"2022-01-11T20:39:03.839775Z","iopub.status.idle":"2022-01-11T20:39:04.059890Z","shell.execute_reply.started":"2022-01-11T20:39:03.839722Z","shell.execute_reply":"2022-01-11T20:39:04.057780Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Randomly select and image/mask pair from the training set\nidx = np.random.choice(range(len(train_x)))\nimg = Image.open(train_x[idx])\nmsk = Image.open(train_y[idx])\n\nprint('Image Size:',img.size)\nprint('Mask Size:',msk.size)\n\n# Show the image\nplt.imshow(img.resize((512,512)))\nplt.imshow(msk.resize((512,512)), alpha=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:39:04.645894Z","iopub.execute_input":"2022-01-11T20:39:04.646236Z","iopub.status.idle":"2022-01-11T20:39:05.488015Z","shell.execute_reply.started":"2022-01-11T20:39:04.646187Z","shell.execute_reply":"2022-01-11T20:39:05.486975Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"builder = tfds.ImageFolder('../input/fdl2022/')\nprint(builder.info)  # num examples, labels... are automatically calculated\ntrain_images = builder.as_dataset(split='train_images', shuffle_files=False)\ntfds.show_examples(train_images, builder.info)\n\ntrain_masks = builder.as_dataset(split='train_masks', shuffle_files=False)\ntfds.show_examples(train_masks, builder.info)\n\ntest_images = builder.as_dataset(split='test_images', shuffle_files=False)\ntfds.show_examples(test_images, builder.info)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:39:08.836190Z","iopub.execute_input":"2022-01-11T20:39:08.836541Z","iopub.status.idle":"2022-01-11T20:39:31.288222Z","shell.execute_reply.started":"2022-01-11T20:39:08.836490Z","shell.execute_reply":"2022-01-11T20:39:31.287373Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_masks","metadata":{"execution":{"iopub.status.busy":"2022-01-11T21:11:30.685452Z","iopub.execute_input":"2022-01-11T21:11:30.685843Z","iopub.status.idle":"2022-01-11T21:11:30.692307Z","shell.execute_reply.started":"2022-01-11T21:11:30.685791Z","shell.execute_reply":"2022-01-11T21:11:30.691460Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\nVAL_SUBSPLITS = 5\n#VALIDATION_STEPS = info.splits['test'].num_examples\n\nmodel_history = model.fit(train_masks, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          #validation_steps=VALIDATION_STEPS,\n                          validation_data=test_images,\n                          callbacks=[DisplayCallback()])","metadata":{"execution":{"iopub.status.busy":"2022-01-11T17:09:38.984175Z","iopub.execute_input":"2022-01-11T17:09:38.984561Z","iopub.status.idle":"2022-01-11T17:09:39.038650Z","shell.execute_reply.started":"2022-01-11T17:09:38.984504Z","shell.execute_reply":"2022-01-11T17:09:39.036067Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}