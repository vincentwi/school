{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzT-fZzFRKly"
   },
   "source": [
    "## FDL- DSBA Assignment 2021-2022\n",
    "\n",
    "### Please fill the blanks in the code and answer to the questions that are asked in the Jupyter Notebook (\"Markdown\" cell). \n",
    "\n",
    "### Instructions: Rename the jupyter adding your name at the end of the title FDL_Assignment-<YOUR NAME\\>.ipynb\n",
    "\n",
    "### Send your solution to fdl.dsba@gmail.com by 20 / 12 / 2021, as subject for the mail please put FDL_Assignment-<YOUR NAME\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPUOV6dFRKl7"
   },
   "source": [
    "### Question 1 -  TRAIN ON CIFAR DATASET\n",
    "In this exercise you are asked to train a Convolutional Neural Network (CNN) on the CIFAR10 dataset and visualize its feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCX0xLejRKl9"
   },
   "source": [
    "1a) Download the CIFAR10 dataset using the already provided PyTorch dataloaders. \n",
    "*   Read and understand the following code\n",
    "*   Feel free to add additional transformations for data augmentation. Explain if so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5KyVunArRKl-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## Download the CIFAR10 dataset using the PyTorch dataloaders\n",
    "import json \n",
    "from pprint import pprint\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# *****START CODE \n",
    "## Data\n",
    "##Here you are free to add further transform functions if you wish\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size= , shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "val_dataloader = torch.utils.data.DataLoader(testset, batch_size= , shuffle=False)\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3S-ITJo9RKmA"
   },
   "source": [
    "1b) Create your convolutional neural network.\n",
    "*   Go to https://dljudge.io/generate + /<YOUR NAME\\> or use curl below\n",
    "*   Save the page as json \n",
    "*   Build your CNN architecture based on those modules and hyperparameters\n",
    "*   Use the right value for 'COMPUTE' \n",
    "*   See the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://dljudge.io/generate/johnsmith -o network1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_network(model, input_dict):\n",
    "    \"\"\"\n",
    "    Validate if your network definition is same as provided archtiecture\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    i = 1\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            output[f'Layer_{str(i).zfill(3)}'] = {'type': 'Conv2d',\n",
    "                                                  'kernel_size': layer.kernel_size[0],\n",
    "                                                   'input': layer.in_channels,\n",
    "                                                   'output': layer.out_channels,\n",
    "                                                   'padding': layer.padding[0]}\n",
    "\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            output[f'Layer_{str(i).zfill(3)}'] = {'type': 'ReLU'}\n",
    "\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            output[f'Layer_{str(i).zfill(3)}'] = {'type': 'MaxPool2d',\n",
    "                                                  'kernel_size': layer.kernel_size,\n",
    "                                                  'stride': layer.stride}\n",
    "\n",
    "        if isinstance(layer, nn.AdaptiveAvgPool2d):\n",
    "            output[f'Layer_{str(i).zfill(3)}'] = {'type': 'AdaptiveAvgPool2d',\n",
    "                                                  'output': layer.output_size}\n",
    "\n",
    "        if isinstance(layer, nn.BatchNorm2d):\n",
    "            output[f'Layer_{str(i).zfill(3)}'] = {'type': 'BatchNorm2d',\n",
    "                                                  'input': layer.num_features}\n",
    "\n",
    "        if isinstance(layer, nn.Dropout):\n",
    "            output[f'Layer_{str(i).zfill(3)}'] = {'type': 'Dropout',\n",
    "                                                  'p': layer.p}\n",
    "\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            output[f'Layer_{str(i).zfill(3)}'] = {'type': 'Linear',\n",
    "                                                  'input': layer.in_features,\n",
    "                                                  'output': layer.out_features}\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    correct = True\n",
    "    for l in output.keys():\n",
    "        if l in input_dict:\n",
    "            inp_kvs = input_dict[l]\n",
    "            out_kvs = output[l]\n",
    "            for k in out_kvs:\n",
    "                if inp_kvs[k] != 'COMPUTE':\n",
    "                    if out_kvs[k] != inp_kvs[k]:\n",
    "                        print (f'Error in {l}, {k}!')\n",
    "                        correct = False\n",
    "\n",
    "    if correct:\n",
    "        print ('OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****START CODE \n",
    "# Import your individual architecture dictionnary here\n",
    "fin = open('network1.json', 'r')\n",
    "my_architecture_dict = json.load(fin)\n",
    "fin.close()\n",
    "\n",
    "pprint(my_architecture_dict)\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "9hb1skoXRKmA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# *****START CODE\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \"\"\"\n",
    "        Number of layers should be exactly same as in the provided JSON. \n",
    "        Do not use any grouping function like Sequential \n",
    "        \"\"\"\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        return x\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oCW4FGORKmC"
   },
   "source": [
    "1c) Create the training scheme\n",
    "*    Initialize the model\n",
    "*    Validate the model\n",
    "*    Specify the training hyperparameters like type of optimizer, criterion and learning rate\n",
    "*    Specify number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emDRadjhRKmD"
   },
   "outputs": [],
   "source": [
    "# *****START CODE\n",
    "lr = \n",
    "model = \n",
    "validate_network(model, my_architecture_dict)\n",
    "optimizer = \n",
    "criterion = \n",
    "epochs = \n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIAwvLE7RKmD"
   },
   "source": [
    "1d) Plot the train and validation loss curves for the entire training process\n",
    "*   Validate the model after each epoch\n",
    "*   Plot both training and validation loss curves\n",
    "*   Write a small description discussing about the curves. What is the behaviour of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UITTFW0RKmE"
   },
   "outputs": [],
   "source": [
    "## Train the model and validate it after each epoch.\n",
    "## Provide the train-val loss graph.\n",
    "\n",
    "# *****START CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Kp8nRiGRKmF"
   },
   "source": [
    "1e) Get an intermediate layer from your convolutional neural network and visualize what patterns the network has learned\n",
    "*   Complete the following code that visualizes the patterns of the network\n",
    "*   Write a small description commenting on the visualized maps. What do you observe in the different visualizations of the feature maps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intermediate layer you should visualize:\n",
    "print('My intermediate layer to visualize is: %s'%(my_architecture_dict['visualize']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xk4FadJSRKmG"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from skimage import io\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "##function for printing the loss during optimization\n",
    "def write_flush(text, stream=sys.stdout):\n",
    "    stream.write(text)\n",
    "    stream.flush()\n",
    "    return\n",
    "\n",
    "## Number of feature maps in the intermediate layer that you have chosen. \n",
    "# *****START CODE\n",
    "n_conv =   # e.g 64\n",
    "# *****END CODE\n",
    "\n",
    "## Size of visualised filter.\n",
    "img_size = 32\n",
    "\n",
    "##load your optimal model\n",
    "# *****START CODE\n",
    "model = ConvNet()\n",
    "model.load_state_dict(torch.load('drive/..../model.pt')) \n",
    "# *****END CODE\n",
    "\n",
    "## Create a submodel, until the intermediate layer of your choice.\n",
    "## Hint: Use model.#name# to create the succession of layers, where #name#\n",
    "## stands for the layer names that you defined in the initialization function \n",
    "## of your model.\n",
    "# *****START CODE\n",
    "submodel = nn.Sequential(\n",
    "\n",
    "    \n",
    ")\n",
    "# *****END CODE\n",
    "\n",
    "## Put submodel in eval mode.\n",
    "submodel.eval()\n",
    "\n",
    "## Tensor to visualised filters. \n",
    "img_stack = torch.zeros((n_conv, 3, img_size, img_size))\n",
    "\n",
    "## Number of epochs to run for every filter. \n",
    "# *****START CODE\n",
    "n_epochs_per_filt =   #e.g 30\n",
    "# *****END CODE\n",
    "\n",
    "## Visualise every convolution. \n",
    "for c in range(n_conv):\n",
    "    ## Initialise with random image. \n",
    "    img = torch.rand(1, 3, img_size, img_size).float()\n",
    "\n",
    "    ## Turn on gradient calculation on the image\n",
    "    # *****START CODE\n",
    "\n",
    "    # *****END CODE\n",
    "\n",
    "    ## Define optimizer.\n",
    "    # *****START CODE\n",
    "    \n",
    "    # *****END CODE\n",
    "    \n",
    "    for f in range(n_epochs_per_filt):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## Feedforward propagation\n",
    "        ## Hint: In order to find the loss, compupte the negative of the activation of the hidden layer. \n",
    "        ## The objective is to produce an input image which maximizes the activation \n",
    "        ## of neurons in a particular hidden layer. \n",
    "        # *****START CODE\n",
    "        \n",
    "        \n",
    "        \n",
    "        # *****END CODE\n",
    "        write_flush('\\rFilter %d. Epoch %d. Loss = %.4f'%(c, f+1, loss.item()))\n",
    "\n",
    "    write_flush('\\n')\n",
    "    img_stack[c, :, :, :] = img[0].detach()\n",
    "\n",
    "## Make grid out of visualized filters. \n",
    "##Here you may have to adjust the properties of vutils.make_grid, depending on your needs. \n",
    "##For example, you may need to change the number of rows.\n",
    "G = vutils.make_grid(img_stack, nrow=8, normalize=True, padding=1).permute(1,2,0).numpy()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(G)\n",
    "plt.axis('equal')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1f) Use GradCAM algorithm to visualize the saliency maps of your trained model at the same intermediate layer\n",
    "*   Install pytorch grad cam package if needed (https://github.com/jacobgil/pytorch-grad-cam)\n",
    "*   Complete the following code that visualizes GradCAM heatmaps on an input image from your model\n",
    "*   Try on several input images / classes. \n",
    "*   Write a small description commenting on the visualized heatmaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch grad cam package\n",
    "'''UNCOMMENT IF NEEDED (using google colab for example)\n",
    "! pip install grad-cam\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "use_cuda = True\n",
    "\n",
    "# *****START CODE\n",
    "model = ConvNet()\n",
    "model.load_state_dict(torch.load('drive/..../model.pt'))\n",
    "\n",
    "# Get your intermediate layer\n",
    "target_layers = [model. ...]\n",
    "\n",
    "rgb_img = \n",
    "input_tensor = # Create an input tensor from your image for your model..\n",
    "# Note: input_tensor can be a batch tensor with several images!\n",
    "target_category = \n",
    "# *****END CODE\n",
    "\n",
    "# Construct the CAM object once, and then re-use it on many images:\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=use_cuda)\n",
    "\n",
    "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "grayscale_cam = cam(input_tensor=input_tensor, target_category=target_category)\n",
    "\n",
    "# In this example grayscale_cam has only one image in the batch:\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "# Plot figure\n",
    "plt.figure()\n",
    "plt.imshow(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3QK7dhJRKmH"
   },
   "source": [
    "### Question 2 - Train on geometrical shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yudhcXrRKmH"
   },
   "source": [
    "Function 'generate_a_triangle' produces images depicting random triangles along with the (x,y) coordinates of the vertices. Create a convolutional neural network that receives as input the triangle image and predicts the corresponding (x,y) coordinates of the triangle's vertices. \n",
    "*  Read and understand the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aptobJOZbRZd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# On some implementations of matplotlib, you may need to change this value\n",
    "IMAGE_SIZE = 72\n",
    "\n",
    "def generate_a_drawing(figsize, U, V):\n",
    "    fig = plt.figure(figsize=(figsize,figsize))\n",
    "    ax = plt.subplot(111)\n",
    "    plt.axis('Off')\n",
    "    ax.set_xlim(0,figsize)\n",
    "    ax.set_ylim(0,figsize)\n",
    "    ax.fill(U, V, \"k\")\n",
    "    fig.canvas.draw()\n",
    "    imdata = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)[::3].astype(np.float32)\n",
    "    imdata = imdata + np.random.random(imdata.size)\n",
    "    plt.close(fig)\n",
    "    return imdata\n",
    "\n",
    "def generate_a_triangle():\n",
    "    figsize = 1.0\n",
    "    U = np.random.random(3)\n",
    "    V = np.random.random(3)\n",
    "    imdata = generate_a_drawing(figsize, U, V)\n",
    "    return [imdata, [U[0], V[0], U[1], V[1], U[2], V[2]]]\n",
    "\n",
    "[im, v] = generate_a_triangle()\n",
    "plt.imshow(im.reshape(IMAGE_SIZE,IMAGE_SIZE), cmap='gray')\n",
    "\n",
    "def generate_dataset_regression(nb_samples):\n",
    "    # Getting im_size:\n",
    "    im_size = generate_a_triangle()[0].shape[0]\n",
    "    X = np.zeros([nb_samples,im_size])\n",
    "    Y = np.zeros([nb_samples, 6])\n",
    "    print('Creating data:')\n",
    "    for i in range(nb_samples):\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        [X[i], Y[i]] = generate_a_triangle()\n",
    "    X = X / 255\n",
    "    return [X, Y]\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_prediction(x, y):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    I = x.reshape((IMAGE_SIZE,IMAGE_SIZE))\n",
    "    ax.imshow(I, extent=[-0.15,1.15,-0.15,1.15],cmap='gray')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "\n",
    "    xy = y.reshape(3,2)\n",
    "    tri = patches.Polygon(xy, closed=True, fill = False, edgecolor = 'r', linewidth = 5, alpha = 0.5)\n",
    "    ax.add_patch(tri)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def generate_test_set_regression():\n",
    "    np.random.seed(42)\n",
    "    [X_test, Y_test] = generate_dataset_regression(300)\n",
    "    return [X_test, Y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clB8r3jMRKmI"
   },
   "source": [
    "2a) Use function 'generate_dataset_regression' to create the dataset. Split the dataset to training and validation parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Y7mW83fRKmJ"
   },
   "outputs": [],
   "source": [
    "##generate dataset\n",
    "# *****START CODE\n",
    "\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIwVanB4RKmK"
   },
   "outputs": [],
   "source": [
    "##split the dataset to training and validation parts\n",
    "# *****START CODE\n",
    "\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEoZyhgZRKmK"
   },
   "source": [
    "2b) Use function 'generate_test_set' to create the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PE6e3nszRKmK"
   },
   "outputs": [],
   "source": [
    "##generate test dataset\n",
    "# *****START CODE\n",
    "\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcsDj8eQRKmK"
   },
   "source": [
    "2c) Create your own convolutional neural network.\n",
    "* Begin with the previous exercise model architecture\n",
    "* Optimize the architecture to perform well on predicting the different coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQYXQV5LRKmL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# *****START CODE\n",
    "class ConvNetR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetR, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6PxmyfqRKmM"
   },
   "source": [
    "2d) Define learning rate, model, optimizer, criterion and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vS9LIDmlRKmM"
   },
   "outputs": [],
   "source": [
    "# *****START CODE\n",
    "lr = \n",
    "model = \n",
    "optimizer = \n",
    "criterion = \n",
    "epochs = \n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNtKYgzTRKmM"
   },
   "source": [
    "2e) What criterion did you choose and why?\n",
    "* Write a small description for the loss function that you want to use for this specific problem.\n",
    "* What was your intuition for using this loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmjN0W-KRKmN"
   },
   "source": [
    "2f) Train your model and validate it at the end of each epoch.\n",
    "* Similarly to the previous question train and validate your network for each epoch\n",
    "* Write a small description on how you decide which is the optimal epoch\n",
    "* Use this epoch and evaluate your model on the test set\n",
    "* Visualise some predictions using the function 'visualize_prediction'\n",
    "* What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZfRftoFRKmN"
   },
   "outputs": [],
   "source": [
    "# *****START CODE\n",
    "\n",
    "\n",
    "\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTCMv88pRKmN"
   },
   "source": [
    "2g) Think and implement a preprocessing step that can boost the accuracy of your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTqLTm8LRKmN"
   },
   "outputs": [],
   "source": [
    "# *****START CODE\n",
    "\n",
    "\n",
    "\n",
    "# *****END CODE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FDL -- Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
