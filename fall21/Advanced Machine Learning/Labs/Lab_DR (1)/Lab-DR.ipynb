{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Dimensionnality reduction\n",
    "\n",
    "The goal of this lab session is to study different dimensionnality reduction methods. You will send only one notebook for both parts.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Problem \n",
    "Consider a random variable X with p features, $X = (X^1,X^2,...X^p)^T$  \n",
    "\n",
    "The goal of the PCA is to create $p$ new variables (principal components) which summarizes the best the variance of the previous $p$ variables such as if we take a subset of these new features the amount of variance provided is quite similar to the sum of variances of the whole original features. \n",
    "\n",
    "We search new random variables, named principal components $Z^i$, as a linear combination of the original features $X^i$ i.e projecting X in a new basis.\n",
    "\n",
    "\\begin{align} Z^1 &= \\alpha_{11}X^1 +\\alpha_{12}X^2 + ... \\alpha_{1p}X^p &= A_1^TX \\\\\n",
    " Z^2 &= \\alpha_{21}X^1 +\\alpha_{22}X^2 + ... \\alpha_{2p}X^p &= A_2^TX  \\\\\n",
    " &\\vdots \\\\\n",
    " Z^p &= \\alpha_{p1}X^1 +\\alpha_{p2}X^2 + ... \\alpha_{pp}X^p &= A_p^TX \n",
    "\\end{align}\n",
    "\n",
    "Eventually PCA looks for $A_1,...,A_p$ with the following conditions : \n",
    "- $\\forall i,  A^i= \\underset{A}{\\arg \\max}\\; Var(Z^i)$\n",
    "- $Var(Z^1)\\ge Var(Z^2)\\ge...\\ge Var(Z^p)$\n",
    "- $\\forall  i,j \\; / \\; i\\neq j, cov(Z^i,Z^j) = 0 $ \n",
    "- $\\forall i, Var(X^i) = 1$\n",
    "\n",
    "The first principal components will contain most of the variance of X, we need to extract these first principal components for reducing the dimensionnality of our dataset.\n",
    "\n",
    "Geometrically it means we seach for an orthogonal basis such as the inertia of the data points around the new axis is maximum.\n",
    "\n",
    "## Theoretical Solution\n",
    "\n",
    "If the random variable $X = (X^1,X^2,...X^p)^T $ is reduced & centered, i.e $ \\forall i, E(X^i) =0 $ and $ Var(X^i) =1 $. \n",
    "\n",
    "We call $\\Sigma$ the covariance matrix of X\n",
    "$$ \\Sigma = P^TDP $$\n",
    "with $D = diag(\\lambda_1,\\lambda_2,...,\\lambda_p)$ such that $ \\lambda_1 \\ge\\lambda_2 \\ge ... \\ge\\lambda_p $\n",
    "\n",
    "$\\boxed{A_i =\\text{ is the eigenvector of $\\Sigma$ related to the eigenvalue $\\lambda_i =$ the ith column of $P$ }}$\n",
    "\n",
    "And $Var(Z^i) = \\lambda_i$\n",
    "\n",
    "## Practical Solution\n",
    "\n",
    "Now imagine you have a dataset $M  \\in \\mathbb{R}^{n\\times p}$ which could be interpreted as n realizations of the random variable X. \n",
    "\n",
    "You need to create a centered matrix $\\bar M=\\begin{bmatrix} M_{1,1}-\\bar M_1 & \\cdots & M_{1,p}-\\bar M_p \\\\ \\vdots & \\ddots & \\vdots \\\\ M_{n,1}-\\bar M_1 & \\cdots & M_{n,p}-\\bar M_p\\end{bmatrix} $\n",
    "\n",
    "With $\\bar M_j = \\frac{1}{n}\\sum_{i=1}^n M_i^j$\n",
    "\n",
    "We call $\\bar \\Sigma = \\frac{1}{n-1}\\bar M \\bar M^T $the scatter matrix, which is an estimation of the covariance matrix.\n",
    "\n",
    "#### Point on reduction\n",
    "We can make the choice to reduce $\\bar M$ to give a variance of one to each feature.\n",
    "\n",
    "- If we do so : variables related to noise will have the same weight after PCA than a relevant variable.\n",
    "- If we don't reduce our dataset : high variance features will totally dominate the PCA.\n",
    "\n",
    "However if the features aren't with the same units, reduction is mandatory.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Considering  $ \\bar \\Sigma = P^T D P $, how do we project the vector $X$ on the k first principal components.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : How do we compute the percentage of orignal variance retained by the k first principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks :  Compute the PCA transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_pca:\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self,reduce=False):\n",
    "        '''\n",
    "        Attributes:\n",
    "        \n",
    "        sigma : np.array\n",
    "            the scatter matrix\n",
    "        eigenvectors : np.array\n",
    "            the eigenvectors in a matrix : P \n",
    "        reduce : boolean \n",
    "            Reduce the scatter matrix or not i.e give a variance of 1 for each feature\n",
    "        X : np.array\n",
    "             The dataset we want to project in the new basis\n",
    "        '''        \n",
    "        \n",
    "        self.sigma = None\n",
    "        self.eigenvectors = None \n",
    "        self.reduce = reduce \n",
    "        \n",
    "        \n",
    "    def fit(self,X):\n",
    "        \"\"\" From X, compute the scatter matrix (sigma) and diagonalize sigma \n",
    "        to extract the eigenvectors and the eigenvalues\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            Data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        Update self.eigenvalues, self.eigenvectors\n",
    "        \"\"\"       \n",
    "        \n",
    "        #TODO\n",
    "    \n",
    "    def projection(self,X,no_dims):\n",
    "        \"\"\" Project X on the no_dims first principal components \n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        no_dims: integer\n",
    "            The number of dimension of our projected dataset\n",
    "        X : np.array\n",
    "            Dataset\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        The projection of X on the no_dims principal components\n",
    "            np.array of size (n,no_dims)\n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "            \n",
    "    def variance(self,no_dims):\n",
    "        \"\"\" Returns the percentage of the total variance preserved by the projected dataset\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        no_dims: integer\n",
    "            The number of dimension of our projected dataset\n",
    "         \"\"\" \n",
    "        \n",
    "        #TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application : Biostatistics \n",
    "\n",
    "We are going to apply PCA to a medical dataset, in order to do data analysis and to finally  tell which medical features could allow doctors to diagnose breast cancer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = load_breast_cancer()\n",
    "X = load_breast_cancer().data\n",
    "y = load_breast_cancer().target\n",
    "feature_names = load_breast_cancer().feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that units of the features can be different, therefore we had better reduce our dataset in the PCA. \n",
    "### Task : Apply PCA to the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the number of principal components for our projection \n",
    "\n",
    "As usual with unsupervised method there are many solutions to answer this question. Generally people either use elbow technique or keep a number of principal components such as it provides 80% or 90% or 95% of the total variance.\n",
    "\n",
    "### Task : Print the percentage of variance retained versus the number of principal components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : If we want to keep 90% of the variance how many principal components must we keep ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : Plot the datapoints with their label along the 2 first principal components. \n",
    "Use plt.scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that we can simply assign a label to datapoints regarding their position on the first principal components. \n",
    "So the first axis is sufficient to do a quite good classification of malignant/benin cases. \n",
    "\n",
    "For our diagnostic, we must know which feature affect the most this first principal components. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Which features influence the most 2 the first principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of PCA. \n",
    "\n",
    "By its linear nature, PCA suffers from its inability to extract complicated structure. \n",
    "\n",
    "Let's see with an exemple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('mnist2500_X.txt')\n",
    "labels = np.loadtxt('mnist2500_labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks : \n",
    "- Apply PCA to the dataset \n",
    "- Project in 2D and plot the dataset with their label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite impossible to distinguish most of the clusters. Right ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non linear embedding : t-sne\n",
    "\n",
    "In order to visualize dataset accurately, we are going to project our data using non linear methods in order to decompose faithfully the dataset in order to extract the intrisec structure of the data.\n",
    "\n",
    "Sklearn definition of tsne : t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks : \n",
    "- Apply tsne to the dataset \n",
    "- Plot the dataset with their label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
